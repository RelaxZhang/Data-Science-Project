{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import IPython\n",
    "import IPython.display\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import keras_tuner as kt\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (8, 6)\n",
    "mpl.rcParams['axes.grid'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new\n",
    "\n",
    "def split_ts_data(data, val_start, test_start):\n",
    "    year_min = min(data['Year'])\n",
    "    year_max = max(data['Year'])\n",
    "    year_range = year_max-year_min\n",
    "    \n",
    "    assert (val_start >= year_min) & (test_start >= year_min) & (val_start <= year_max) & (test_start <= year_max), \"Parameter out of bounds\"\n",
    "    assert (val_start > year_min) & (test_start > year_min), \"Training set is empty.\"\n",
    "    assert val_start < test_start, \"Validation set is empty.\"\n",
    "    assert year_range > 0, \"Data contains less than 2 years.\"\n",
    "    \n",
    "    \n",
    "    train_data = data[(data['Year']<val_start) & (data['Year']<test_start)]\n",
    "    val_data = data[(data['Year']>=val_start) & (data['Year']<test_start)]\n",
    "    test_data = data[data['Year']>=test_start]\n",
    "    \n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(df, input_width, label_width, shift):\n",
    "    def create_window(tensor):\n",
    "        #input -> length of time series used for training\n",
    "        #shift -> how far off prediction is from last input\n",
    "        #label -> points to predict\n",
    "        total_window_size = input_width + shift\n",
    "        label_start = total_window_size - label_width\n",
    "\n",
    "        input_bounds = slice(0, input_width)\n",
    "        label_bounds = slice(label_start, None)\n",
    "\n",
    "        inputs = tensor[:,input_bounds,:]\n",
    "        labels = tensor[:,label_bounds,:]\n",
    "\n",
    "        inputs.set_shape([None, input_width, None])\n",
    "        labels.set_shape([None, label_width, None])\n",
    "\n",
    "        return inputs, labels\n",
    "    \n",
    "    total_window_size = input_width + shift\n",
    "    \n",
    "    arr = np.array(df, dtype=np.float32)\n",
    "    ds = tf.keras.utils.timeseries_dataset_from_array(\n",
    "      data=arr,\n",
    "      targets=None,\n",
    "      sequence_length=total_window_size,\n",
    "      sequence_stride=1,\n",
    "      shuffle=False,\n",
    "      batch_size=32,)\n",
    "    \n",
    "    ds = ds.map(create_window)\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_and_fit(model, num_epochs, input_optimizer='adam', input_loss='mse'):\n",
    "    model.compile(optimizer=input_optimizer, loss=input_loss)\n",
    "    history = model.fit(x=train_inputs,y=train_labels, batch_size = 32, epochs=num_epochs, validation_data=val_ds, shuffle=False)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def col_dict(np_df):\n",
    "    return_dict = {col:index for index, col in enumerate(np_df.columns)}\n",
    "    \n",
    "    return return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(df, ds, input_width, label_width, shift, model=None, plot_col='10101 m0.4', max_subplots=3):\n",
    "    #ensure that df and ds match e.g. train_df must be accompanied by train_ds\n",
    "    col_indices = col_dict(df)\n",
    "    \n",
    "    total_window_size = label_width + shift\n",
    "    input_slice = slice(0,input_width)\n",
    "    input_indices = np.arange(total_window_size)[input_slice]\n",
    "    label_start = total_window_size - label_width\n",
    "    labels_slice = slice(label_start, None)\n",
    "    label_indices = np.arange(total_window_size)[labels_slice]\n",
    "    \n",
    "    inputs = next(iter(ds))[0]\n",
    "    labels = next(iter(ds))[1]\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plot_col_index = col_indices[plot_col] \n",
    "    max_n = min(max_subplots, len(inputs))\n",
    "    \n",
    "    for n in range(max_n):\n",
    "        plt.subplot(max_n, 1, n+1)\n",
    "        plt.ylabel(plot_col)\n",
    "        plt.plot(input_indices, inputs[n, :, plot_col_index],\n",
    "             label='Inputs', marker='.', zorder=-10)\n",
    "        \n",
    "        plt.scatter(label_indices, labels[n, :, plot_col_index],\n",
    "                edgecolors='k', label='Labels', c='#2ca02c', s=64)\n",
    "        \n",
    "        if model is not None:\n",
    "          predictions = model(inputs)\n",
    "          plt.scatter(label_indices, predictions[n, :, plot_col_index],\n",
    "                  marker='X', edgecolors='k', label='Predictions',\n",
    "                  c='#ff7f0e', s=64)\n",
    "            \n",
    "        if n == 0:\n",
    "          plt.legend()\n",
    "        \n",
    "    plt.xlabel('Year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Main code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edit parameters here, but do not rename variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Read, preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('../Data/newSA3.csv')\n",
    "\n",
    "\n",
    "\n",
    "#Parameters\n",
    "validation_start = 2002\n",
    "test_start = 2006\n",
    "#\n",
    "\n",
    "\n",
    "\n",
    "train_df, val_df, test_df = split_ts_data(raw_data, validation_start, test_start)\n",
    "\n",
    "train_df = train_df[train_df.columns.difference([\"Unnamed: 0\",\"Year\"])]\n",
    "val_df = val_df[val_df.columns.difference([\"Unnamed: 0\",\"Year\"])]\n",
    "test_df = test_df[test_df.columns.difference([\"Unnamed: 0\",\"Year\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-31 13:19:45.531275: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-07-31 13:19:45.531583: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2022-07-31 13:19:45.660168: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    }
   ],
   "source": [
    "#Parameters\n",
    "input_width = 2 #data used in prediction\n",
    "label_width = 1 #points to predict\n",
    "shift = 1 #how many years away is the last point to predict\n",
    "#\n",
    "\n",
    "\n",
    "\n",
    "train_ds = make_dataset(train_df, input_width, label_width, shift)\n",
    "val_ds = make_dataset(val_df, input_width, label_width, shift)\n",
    "test_ds = make_dataset(test_df, input_width, label_width, shift)\n",
    "\n",
    "num_cols = next(iter(train_ds))[0].shape[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SACohortModel(kt.HyperModel):\n",
    "    def build(self,hp):\n",
    "        #### Hyperparameters\n",
    "        # add hyperparameters as needed when adding layers\n",
    "        \n",
    "        ##layer hyperparameters\n",
    "        hp_lstm1_units = hp.Choice('units',[10,30,50])\n",
    "        hp_lstm1_act = hp.Choice('activation', [\"relu\"])\n",
    "\n",
    "        ##model hyperparameters -> adjust tf.keras.models type and model.add layers\n",
    "        model = tf.keras.models.Sequential()\n",
    "        model.add(tf.keras.layers.LSTM(units = hp_lstm1_units, \n",
    "                                       activation=hp_lstm1_act, \n",
    "                                       return_sequences=False))\n",
    "        \n",
    "        model.add(tf.keras.layers.Dense(label_width * num_cols))\n",
    "        model.add(tf.keras.layers.Reshape([label_width,num_cols]))\n",
    "        \n",
    "        ##compilation hyperparameters\n",
    "        hp_epochs = hp.Choice(\"epochs\",[10,20,30])\n",
    "        hp_input_optimizer = hp.Choice('input_optimizer',[\"adam\", \"adadelta\"])\n",
    "        loss_fun = \"mse\"\n",
    "        \n",
    "        ####\n",
    "        \n",
    "        #Do not edit\n",
    "        model.compile(loss = loss_fun)\n",
    "        \n",
    "        return model\n",
    "        #Do not edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trial 5 Complete [00h 00m 02s]\n",
      "val_loss: 4691693.0\n",
      "\n",
      "Best val_loss So Far: 1078770.75\n",
      "Total elapsed time: 00h 00m 10s\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "#Parameter\n",
    "num_epochs = 10\n",
    "#\n",
    "\n",
    "train_inputs = next(iter(train_ds))[0]\n",
    "train_labels = next(iter(train_ds))[1]\n",
    "\n",
    "val_inputs = next(iter(val_ds))[0]\n",
    "val_labels = next(iter(val_ds))[1]\n",
    "\n",
    "test_inputs = next(iter(test_ds))[0]\n",
    "\n",
    "tuner = kt.RandomSearch(\n",
    "    SACohortModel(),\n",
    "    objective='val_loss',\n",
    "    max_trials=5)\n",
    "\n",
    "tuner.search(train_inputs, train_labels, epochs = num_epochs, validation_data = (val_inputs, val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model with above parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-31 13:19:56.386305: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 802ms/step - loss: 4147470.2500 - val_loss: 4691749.5000\n",
      "Epoch 2/20\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 4136716.5000 - val_loss: 4691746.0000\n",
      "Epoch 3/20\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 4136713.7500 - val_loss: 4691742.5000\n",
      "Epoch 4/20\n",
      "1/1 [==============================] - ETA: 0s - loss: 4136710.2500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-31 13:19:56.668659: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 54ms/step - loss: 4136710.2500 - val_loss: 4691739.0000\n",
      "Epoch 5/20\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 4136707.2500 - val_loss: 4691736.0000\n",
      "Epoch 6/20\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 4136704.0000 - val_loss: 4691732.0000\n",
      "Epoch 7/20\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 4136701.0000 - val_loss: 4691729.0000\n",
      "Epoch 8/20\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 4136697.7500 - val_loss: 4691725.0000\n",
      "Epoch 9/20\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 4136694.7500 - val_loss: 4691722.0000\n",
      "Epoch 10/20\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 4136691.5000 - val_loss: 4691718.5000\n",
      "Epoch 11/20\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 4136688.0000 - val_loss: 4691715.0000\n",
      "Epoch 12/20\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 4136685.0000 - val_loss: 4691711.5000\n",
      "Epoch 13/20\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 4136681.7500 - val_loss: 4691708.0000\n",
      "Epoch 14/20\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 4136678.7500 - val_loss: 4691704.5000\n",
      "Epoch 15/20\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 4136675.5000 - val_loss: 4691701.0000\n",
      "Epoch 16/20\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 4136673.0000 - val_loss: 4691698.0000\n",
      "Epoch 17/20\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 4136669.2500 - val_loss: 4691694.0000\n",
      "Epoch 18/20\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 4136666.2500 - val_loss: 4691691.0000\n",
      "Epoch 19/20\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 4136663.2500 - val_loss: 4691688.0000\n",
      "Epoch 20/20\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 4136660.0000 - val_loss: 4691684.0000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x29a049fd0>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_optimizer = 'adam'\n",
    "loss_fun = 'mse'\n",
    "\n",
    "\n",
    "np.random.seed(1337)\n",
    "full_model = tf.keras.models.Sequential()\n",
    "full_model.add(tf.keras.layers.LSTM(units = 30, \n",
    "                                       activation=\"relu\", \n",
    "                                       return_sequences=False))\n",
    "        \n",
    "full_model.add(tf.keras.layers.Dense(label_width * num_cols))\n",
    "full_model.add(tf.keras.layers.Reshape([label_width,num_cols]))\n",
    "compile_and_fit(full_model, num_epochs=20, input_optimizer=model_optimizer, input_loss=loss_fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_1 (LSTM)               (None, 30)                1407720   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 11700)             362700    \n",
      "                                                                 \n",
      " reshape_1 (Reshape)         (None, 1, 11700)          0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,770,420\n",
      "Trainable params: 1,770,420\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "full_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store full model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1991-2001\n",
    "\n",
    "full_train_inputs = next(iter(train_ds))[0] #pairs from 1991-2000\n",
    "full_train_labels = next(iter(train_ds))[1] #1993-2001\n",
    "full_train_predictions = full_model(full_train_inputs) #1993-2001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(9, 1, 11700), dtype=float32, numpy=\n",
       "array([[[2382., 2354., 2126., ...,  209.,  105.,   59.]],\n",
       "\n",
       "       [[2357., 2351., 2084., ...,  212.,  112.,   65.]],\n",
       "\n",
       "       [[2318., 2357., 2055., ...,  247.,  116.,   69.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[2132., 2348., 2039., ...,  389.,  163.,   95.]],\n",
       "\n",
       "       [[2107., 2335., 2029., ...,  439.,  190.,  103.]],\n",
       "\n",
       "       [[2077., 2259., 2051., ...,  457.,  221.,  119.]]], dtype=float32)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(9, 1, 11700), dtype=float32, numpy=\n",
       "array([[[0.02000188, 0.01997657, 0.0200624 , ..., 0.01983915,\n",
       "         0.01831472, 0.02011553]],\n",
       "\n",
       "       [[0.02000188, 0.01997657, 0.0200624 , ..., 0.01983915,\n",
       "         0.01831472, 0.02011553]],\n",
       "\n",
       "       [[0.02000188, 0.01997657, 0.0200624 , ..., 0.01983915,\n",
       "         0.01831472, 0.02011553]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.02000188, 0.01997657, 0.0200624 , ..., 0.01983915,\n",
       "         0.01831472, 0.02011553]],\n",
       "\n",
       "       [[0.02000188, 0.01997657, 0.0200624 , ..., 0.01983915,\n",
       "         0.01831472, 0.02011553]],\n",
       "\n",
       "       [[0.02000188, 0.01997657, 0.0200624 , ..., 0.01983915,\n",
       "         0.01831472, 0.02011553]]], dtype=float32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_train_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2002-2005\n",
    "\n",
    "full_val_inputs = next(iter(val_ds))[0] #pairs 2002,2003 and 2003,2004\n",
    "full_val_labels = next(iter(val_ds))[1] #2004 and 2005\n",
    "full_val_predictions = full_model(val_inputs) #2004 and 2005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_test_inputs = next(iter(test_ds))[0] #pairs from 2006-2010\n",
    "full_test_labels = next(iter(test_ds))[1] #2008-2011\n",
    "full_test_predictions = full_model(test_inputs) #2008-2011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10101 f0.4</th>\n",
       "      <th>10101 f10.14</th>\n",
       "      <th>10101 f15.19</th>\n",
       "      <th>10101 f20.24</th>\n",
       "      <th>10101 f25.29</th>\n",
       "      <th>10101 f30.34</th>\n",
       "      <th>10101 f35.39</th>\n",
       "      <th>10101 f40.44</th>\n",
       "      <th>10101 f45.49</th>\n",
       "      <th>10101 f5.9</th>\n",
       "      <th>...</th>\n",
       "      <th>80109 m45.49</th>\n",
       "      <th>80109 m5.9</th>\n",
       "      <th>80109 m50.54</th>\n",
       "      <th>80109 m55.59</th>\n",
       "      <th>80109 m60.64</th>\n",
       "      <th>80109 m65.69</th>\n",
       "      <th>80109 m70.74</th>\n",
       "      <th>80109 m75.79</th>\n",
       "      <th>80109 m80.84</th>\n",
       "      <th>80109 m85.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2400</td>\n",
       "      <td>2347</td>\n",
       "      <td>2277</td>\n",
       "      <td>1820</td>\n",
       "      <td>2152</td>\n",
       "      <td>2315</td>\n",
       "      <td>2185</td>\n",
       "      <td>2146</td>\n",
       "      <td>1859</td>\n",
       "      <td>2495</td>\n",
       "      <td>...</td>\n",
       "      <td>1219</td>\n",
       "      <td>1002</td>\n",
       "      <td>1226</td>\n",
       "      <td>920</td>\n",
       "      <td>784</td>\n",
       "      <td>606</td>\n",
       "      <td>328</td>\n",
       "      <td>180</td>\n",
       "      <td>98</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2392</td>\n",
       "      <td>2344</td>\n",
       "      <td>2194</td>\n",
       "      <td>1830</td>\n",
       "      <td>2067</td>\n",
       "      <td>2339</td>\n",
       "      <td>2204</td>\n",
       "      <td>2139</td>\n",
       "      <td>1954</td>\n",
       "      <td>2480</td>\n",
       "      <td>...</td>\n",
       "      <td>1257</td>\n",
       "      <td>1009</td>\n",
       "      <td>1209</td>\n",
       "      <td>931</td>\n",
       "      <td>794</td>\n",
       "      <td>617</td>\n",
       "      <td>376</td>\n",
       "      <td>200</td>\n",
       "      <td>104</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2382</td>\n",
       "      <td>2354</td>\n",
       "      <td>2126</td>\n",
       "      <td>1813</td>\n",
       "      <td>1977</td>\n",
       "      <td>2329</td>\n",
       "      <td>2225</td>\n",
       "      <td>2139</td>\n",
       "      <td>2037</td>\n",
       "      <td>2451</td>\n",
       "      <td>...</td>\n",
       "      <td>1285</td>\n",
       "      <td>993</td>\n",
       "      <td>1154</td>\n",
       "      <td>963</td>\n",
       "      <td>792</td>\n",
       "      <td>634</td>\n",
       "      <td>413</td>\n",
       "      <td>209</td>\n",
       "      <td>105</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2357</td>\n",
       "      <td>2351</td>\n",
       "      <td>2084</td>\n",
       "      <td>1777</td>\n",
       "      <td>1913</td>\n",
       "      <td>2316</td>\n",
       "      <td>2232</td>\n",
       "      <td>2158</td>\n",
       "      <td>2079</td>\n",
       "      <td>2418</td>\n",
       "      <td>...</td>\n",
       "      <td>1284</td>\n",
       "      <td>988</td>\n",
       "      <td>1123</td>\n",
       "      <td>998</td>\n",
       "      <td>778</td>\n",
       "      <td>649</td>\n",
       "      <td>459</td>\n",
       "      <td>212</td>\n",
       "      <td>112</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2318</td>\n",
       "      <td>2357</td>\n",
       "      <td>2055</td>\n",
       "      <td>1734</td>\n",
       "      <td>1869</td>\n",
       "      <td>2272</td>\n",
       "      <td>2256</td>\n",
       "      <td>2165</td>\n",
       "      <td>2111</td>\n",
       "      <td>2403</td>\n",
       "      <td>...</td>\n",
       "      <td>1277</td>\n",
       "      <td>998</td>\n",
       "      <td>1105</td>\n",
       "      <td>1020</td>\n",
       "      <td>819</td>\n",
       "      <td>648</td>\n",
       "      <td>486</td>\n",
       "      <td>247</td>\n",
       "      <td>116</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2280</td>\n",
       "      <td>2360</td>\n",
       "      <td>2060</td>\n",
       "      <td>1656</td>\n",
       "      <td>1869</td>\n",
       "      <td>2218</td>\n",
       "      <td>2290</td>\n",
       "      <td>2186</td>\n",
       "      <td>2150</td>\n",
       "      <td>2400</td>\n",
       "      <td>...</td>\n",
       "      <td>1244</td>\n",
       "      <td>1015</td>\n",
       "      <td>1112</td>\n",
       "      <td>1051</td>\n",
       "      <td>833</td>\n",
       "      <td>672</td>\n",
       "      <td>513</td>\n",
       "      <td>277</td>\n",
       "      <td>124</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2242</td>\n",
       "      <td>2327</td>\n",
       "      <td>2054</td>\n",
       "      <td>1555</td>\n",
       "      <td>1905</td>\n",
       "      <td>2104</td>\n",
       "      <td>2340</td>\n",
       "      <td>2208</td>\n",
       "      <td>2153</td>\n",
       "      <td>2393</td>\n",
       "      <td>...</td>\n",
       "      <td>1188</td>\n",
       "      <td>985</td>\n",
       "      <td>1117</td>\n",
       "      <td>1086</td>\n",
       "      <td>835</td>\n",
       "      <td>676</td>\n",
       "      <td>527</td>\n",
       "      <td>316</td>\n",
       "      <td>123</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2171</td>\n",
       "      <td>2338</td>\n",
       "      <td>2051</td>\n",
       "      <td>1480</td>\n",
       "      <td>1912</td>\n",
       "      <td>2054</td>\n",
       "      <td>2403</td>\n",
       "      <td>2171</td>\n",
       "      <td>2153</td>\n",
       "      <td>2400</td>\n",
       "      <td>...</td>\n",
       "      <td>1157</td>\n",
       "      <td>975</td>\n",
       "      <td>1127</td>\n",
       "      <td>1083</td>\n",
       "      <td>847</td>\n",
       "      <td>690</td>\n",
       "      <td>572</td>\n",
       "      <td>358</td>\n",
       "      <td>142</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2132</td>\n",
       "      <td>2348</td>\n",
       "      <td>2039</td>\n",
       "      <td>1450</td>\n",
       "      <td>1892</td>\n",
       "      <td>2005</td>\n",
       "      <td>2395</td>\n",
       "      <td>2217</td>\n",
       "      <td>2195</td>\n",
       "      <td>2400</td>\n",
       "      <td>...</td>\n",
       "      <td>1149</td>\n",
       "      <td>975</td>\n",
       "      <td>1123</td>\n",
       "      <td>1051</td>\n",
       "      <td>851</td>\n",
       "      <td>694</td>\n",
       "      <td>598</td>\n",
       "      <td>389</td>\n",
       "      <td>163</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2107</td>\n",
       "      <td>2335</td>\n",
       "      <td>2029</td>\n",
       "      <td>1452</td>\n",
       "      <td>1848</td>\n",
       "      <td>1991</td>\n",
       "      <td>2363</td>\n",
       "      <td>2251</td>\n",
       "      <td>2201</td>\n",
       "      <td>2393</td>\n",
       "      <td>...</td>\n",
       "      <td>1122</td>\n",
       "      <td>967</td>\n",
       "      <td>1163</td>\n",
       "      <td>997</td>\n",
       "      <td>892</td>\n",
       "      <td>717</td>\n",
       "      <td>577</td>\n",
       "      <td>439</td>\n",
       "      <td>190</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2077</td>\n",
       "      <td>2259</td>\n",
       "      <td>2051</td>\n",
       "      <td>1467</td>\n",
       "      <td>1777</td>\n",
       "      <td>1997</td>\n",
       "      <td>2263</td>\n",
       "      <td>2282</td>\n",
       "      <td>2203</td>\n",
       "      <td>2302</td>\n",
       "      <td>...</td>\n",
       "      <td>1100</td>\n",
       "      <td>945</td>\n",
       "      <td>1200</td>\n",
       "      <td>996</td>\n",
       "      <td>898</td>\n",
       "      <td>676</td>\n",
       "      <td>596</td>\n",
       "      <td>457</td>\n",
       "      <td>221</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows × 11700 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    10101 f0.4  10101 f10.14  10101 f15.19  10101 f20.24  10101 f25.29  \\\n",
       "0         2400          2347          2277          1820          2152   \n",
       "1         2392          2344          2194          1830          2067   \n",
       "2         2382          2354          2126          1813          1977   \n",
       "3         2357          2351          2084          1777          1913   \n",
       "4         2318          2357          2055          1734          1869   \n",
       "5         2280          2360          2060          1656          1869   \n",
       "6         2242          2327          2054          1555          1905   \n",
       "7         2171          2338          2051          1480          1912   \n",
       "8         2132          2348          2039          1450          1892   \n",
       "9         2107          2335          2029          1452          1848   \n",
       "10        2077          2259          2051          1467          1777   \n",
       "\n",
       "    10101 f30.34  10101 f35.39  10101 f40.44  10101 f45.49  10101 f5.9  ...  \\\n",
       "0           2315          2185          2146          1859        2495  ...   \n",
       "1           2339          2204          2139          1954        2480  ...   \n",
       "2           2329          2225          2139          2037        2451  ...   \n",
       "3           2316          2232          2158          2079        2418  ...   \n",
       "4           2272          2256          2165          2111        2403  ...   \n",
       "5           2218          2290          2186          2150        2400  ...   \n",
       "6           2104          2340          2208          2153        2393  ...   \n",
       "7           2054          2403          2171          2153        2400  ...   \n",
       "8           2005          2395          2217          2195        2400  ...   \n",
       "9           1991          2363          2251          2201        2393  ...   \n",
       "10          1997          2263          2282          2203        2302  ...   \n",
       "\n",
       "    80109 m45.49  80109 m5.9  80109 m50.54  80109 m55.59  80109 m60.64  \\\n",
       "0           1219        1002          1226           920           784   \n",
       "1           1257        1009          1209           931           794   \n",
       "2           1285         993          1154           963           792   \n",
       "3           1284         988          1123           998           778   \n",
       "4           1277         998          1105          1020           819   \n",
       "5           1244        1015          1112          1051           833   \n",
       "6           1188         985          1117          1086           835   \n",
       "7           1157         975          1127          1083           847   \n",
       "8           1149         975          1123          1051           851   \n",
       "9           1122         967          1163           997           892   \n",
       "10          1100         945          1200           996           898   \n",
       "\n",
       "    80109 m65.69  80109 m70.74  80109 m75.79  80109 m80.84  80109 m85.  \n",
       "0            606           328           180            98          50  \n",
       "1            617           376           200           104          55  \n",
       "2            634           413           209           105          59  \n",
       "3            649           459           212           112          65  \n",
       "4            648           486           247           116          69  \n",
       "5            672           513           277           124          74  \n",
       "6            676           527           316           123          79  \n",
       "7            690           572           358           142          88  \n",
       "8            694           598           389           163          95  \n",
       "9            717           577           439           190         103  \n",
       "10           676           596           457           221         119  \n",
       "\n",
       "[11 rows x 11700 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(9, 2, 11700), dtype=float32, numpy=\n",
       "array([[[2400., 2347., 2277., ...,  180.,   98.,   50.],\n",
       "        [2392., 2344., 2194., ...,  200.,  104.,   55.]],\n",
       "\n",
       "       [[2392., 2344., 2194., ...,  200.,  104.,   55.],\n",
       "        [2382., 2354., 2126., ...,  209.,  105.,   59.]],\n",
       "\n",
       "       [[2382., 2354., 2126., ...,  209.,  105.,   59.],\n",
       "        [2357., 2351., 2084., ...,  212.,  112.,   65.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[2242., 2327., 2054., ...,  316.,  123.,   79.],\n",
       "        [2171., 2338., 2051., ...,  358.,  142.,   88.]],\n",
       "\n",
       "       [[2171., 2338., 2051., ...,  358.,  142.,   88.],\n",
       "        [2132., 2348., 2039., ...,  389.,  163.,   95.]],\n",
       "\n",
       "       [[2132., 2348., 2039., ...,  389.,  163.,   95.],\n",
       "        [2107., 2335., 2029., ...,  439.,  190.,  103.]]], dtype=float32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_train_inputs #pairs from 1991-2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2, 11700), dtype=float32, numpy=\n",
       "array([[[2107., 2335., 2029., ...,  439.,  190.,  103.],\n",
       "        [2077., 2259., 2051., ...,  457.,  221.,  119.]],\n",
       "\n",
       "       [[2077., 2259., 2051., ...,  457.,  221.,  119.],\n",
       "        [2068., 2272., 2072., ...,  452.,  253.,  128.]]], dtype=float32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2000-2001 input for predicting 2002\n",
    "input_2002 = tf.stack([full_train_labels[7,0,:], full_train_labels[8,0,:]],0)\n",
    "#2001-2002 input for predicting 2003\n",
    "input_2003 = tf.stack([full_train_labels[8,0,:], full_val_inputs[0,0,:]],0)\n",
    "#2000-2001 and 2001-2002 inputs as tensor\n",
    "input_2002_2003 = tf.stack([input_2002,input_2003],0)\n",
    "\n",
    "input_2002_2003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10101 f0.4</th>\n",
       "      <th>10101 f10.14</th>\n",
       "      <th>10101 f15.19</th>\n",
       "      <th>10101 f20.24</th>\n",
       "      <th>10101 f25.29</th>\n",
       "      <th>10101 f30.34</th>\n",
       "      <th>10101 f35.39</th>\n",
       "      <th>10101 f40.44</th>\n",
       "      <th>10101 f45.49</th>\n",
       "      <th>10101 f5.9</th>\n",
       "      <th>...</th>\n",
       "      <th>80109 m45.49</th>\n",
       "      <th>80109 m5.9</th>\n",
       "      <th>80109 m50.54</th>\n",
       "      <th>80109 m55.59</th>\n",
       "      <th>80109 m60.64</th>\n",
       "      <th>80109 m65.69</th>\n",
       "      <th>80109 m70.74</th>\n",
       "      <th>80109 m75.79</th>\n",
       "      <th>80109 m80.84</th>\n",
       "      <th>80109 m85.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2068</td>\n",
       "      <td>2272</td>\n",
       "      <td>2072</td>\n",
       "      <td>1450</td>\n",
       "      <td>1682</td>\n",
       "      <td>2080</td>\n",
       "      <td>2189</td>\n",
       "      <td>2323</td>\n",
       "      <td>2239</td>\n",
       "      <td>2319</td>\n",
       "      <td>...</td>\n",
       "      <td>1114</td>\n",
       "      <td>984</td>\n",
       "      <td>1132</td>\n",
       "      <td>986</td>\n",
       "      <td>960</td>\n",
       "      <td>692</td>\n",
       "      <td>590</td>\n",
       "      <td>452</td>\n",
       "      <td>253</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2033</td>\n",
       "      <td>2317</td>\n",
       "      <td>2078</td>\n",
       "      <td>1466</td>\n",
       "      <td>1622</td>\n",
       "      <td>2116</td>\n",
       "      <td>2172</td>\n",
       "      <td>2394</td>\n",
       "      <td>2217</td>\n",
       "      <td>2299</td>\n",
       "      <td>...</td>\n",
       "      <td>1115</td>\n",
       "      <td>990</td>\n",
       "      <td>1078</td>\n",
       "      <td>1003</td>\n",
       "      <td>920</td>\n",
       "      <td>713</td>\n",
       "      <td>601</td>\n",
       "      <td>454</td>\n",
       "      <td>285</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2035</td>\n",
       "      <td>2372</td>\n",
       "      <td>2058</td>\n",
       "      <td>1481</td>\n",
       "      <td>1585</td>\n",
       "      <td>2089</td>\n",
       "      <td>2152</td>\n",
       "      <td>2419</td>\n",
       "      <td>2220</td>\n",
       "      <td>2256</td>\n",
       "      <td>...</td>\n",
       "      <td>1136</td>\n",
       "      <td>978</td>\n",
       "      <td>1065</td>\n",
       "      <td>993</td>\n",
       "      <td>887</td>\n",
       "      <td>728</td>\n",
       "      <td>584</td>\n",
       "      <td>476</td>\n",
       "      <td>298</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2038</td>\n",
       "      <td>2406</td>\n",
       "      <td>2052</td>\n",
       "      <td>1520</td>\n",
       "      <td>1571</td>\n",
       "      <td>2025</td>\n",
       "      <td>2171</td>\n",
       "      <td>2403</td>\n",
       "      <td>2240</td>\n",
       "      <td>2216</td>\n",
       "      <td>...</td>\n",
       "      <td>1192</td>\n",
       "      <td>979</td>\n",
       "      <td>1027</td>\n",
       "      <td>1030</td>\n",
       "      <td>828</td>\n",
       "      <td>754</td>\n",
       "      <td>599</td>\n",
       "      <td>471</td>\n",
       "      <td>313</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 11700 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    10101 f0.4  10101 f10.14  10101 f15.19  10101 f20.24  10101 f25.29  \\\n",
       "11        2068          2272          2072          1450          1682   \n",
       "12        2033          2317          2078          1466          1622   \n",
       "13        2035          2372          2058          1481          1585   \n",
       "14        2038          2406          2052          1520          1571   \n",
       "\n",
       "    10101 f30.34  10101 f35.39  10101 f40.44  10101 f45.49  10101 f5.9  ...  \\\n",
       "11          2080          2189          2323          2239        2319  ...   \n",
       "12          2116          2172          2394          2217        2299  ...   \n",
       "13          2089          2152          2419          2220        2256  ...   \n",
       "14          2025          2171          2403          2240        2216  ...   \n",
       "\n",
       "    80109 m45.49  80109 m5.9  80109 m50.54  80109 m55.59  80109 m60.64  \\\n",
       "11          1114         984          1132           986           960   \n",
       "12          1115         990          1078          1003           920   \n",
       "13          1136         978          1065           993           887   \n",
       "14          1192         979          1027          1030           828   \n",
       "\n",
       "    80109 m65.69  80109 m70.74  80109 m75.79  80109 m80.84  80109 m85.  \n",
       "11           692           590           452           253         128  \n",
       "12           713           601           454           285         127  \n",
       "13           728           584           476           298         129  \n",
       "14           754           599           471           313         156  \n",
       "\n",
       "[4 rows x 11700 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2, 11700), dtype=float32, numpy=\n",
       "array([[[2068., 2272., 2072., ...,  452.,  253.,  128.],\n",
       "        [2033., 2317., 2078., ...,  454.,  285.,  127.]],\n",
       "\n",
       "       [[2033., 2317., 2078., ...,  454.,  285.,  127.],\n",
       "        [2035., 2372., 2058., ...,  476.,  298.,  129.]]], dtype=float32)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_val_inputs #pairs from 2002-2004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2, 11700), dtype=float32, numpy=\n",
       "array([[[2035., 2372., 2058., ...,  476.,  298.,  129.],\n",
       "        [2038., 2406., 2052., ...,  471.,  313.,  156.]],\n",
       "\n",
       "       [[2038., 2406., 2052., ...,  471.,  313.,  156.],\n",
       "        [2005., 2428., 2034., ...,  499.,  325.,  195.]]], dtype=float32)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2004-2005 input for predicting 2006\n",
    "input_2006 = tf.stack([full_val_labels[0,0,:],full_val_labels[1,0,:]],0)\n",
    "#2005-2006 input for predicting 2007\n",
    "input_2007 = tf.stack([full_val_labels[1,0,:], full_test_inputs[0,0,:]],0)\n",
    "#2004-2005 and 2005-2006 inputs as tensor\n",
    "input_2006_2007 = tf.stack([input_2006,input_2007],0)\n",
    "\n",
    "input_2006_2007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 2, 11700), dtype=float32, numpy=\n",
       "array([[[2005., 2428., 2034., ...,  499.,  325.,  195.],\n",
       "        [2011., 2375., 2112., ...,  480.,  356.,  214.]],\n",
       "\n",
       "       [[2011., 2375., 2112., ...,  480.,  356.,  214.],\n",
       "        [2044., 2343., 2145., ...,  484.,  361.,  234.]],\n",
       "\n",
       "       [[2044., 2343., 2145., ...,  484.,  361.,  234.],\n",
       "        [2109., 2330., 2182., ...,  481.,  363.,  250.]],\n",
       "\n",
       "       [[2109., 2330., 2182., ...,  481.,  363.,  250.],\n",
       "        [2170., 2336., 2230., ...,  492.,  360.,  264.]]], dtype=float32)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_test_inputs #pairs from 2006-2010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10101 f0.4</th>\n",
       "      <th>10101 f10.14</th>\n",
       "      <th>10101 f15.19</th>\n",
       "      <th>10101 f20.24</th>\n",
       "      <th>10101 f25.29</th>\n",
       "      <th>10101 f30.34</th>\n",
       "      <th>10101 f35.39</th>\n",
       "      <th>10101 f40.44</th>\n",
       "      <th>10101 f45.49</th>\n",
       "      <th>10101 f5.9</th>\n",
       "      <th>...</th>\n",
       "      <th>80109 m45.49</th>\n",
       "      <th>80109 m5.9</th>\n",
       "      <th>80109 m50.54</th>\n",
       "      <th>80109 m55.59</th>\n",
       "      <th>80109 m60.64</th>\n",
       "      <th>80109 m65.69</th>\n",
       "      <th>80109 m70.74</th>\n",
       "      <th>80109 m75.79</th>\n",
       "      <th>80109 m80.84</th>\n",
       "      <th>80109 m85.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2005</td>\n",
       "      <td>2428</td>\n",
       "      <td>2034</td>\n",
       "      <td>1570</td>\n",
       "      <td>1566</td>\n",
       "      <td>1936</td>\n",
       "      <td>2246</td>\n",
       "      <td>2337</td>\n",
       "      <td>2292</td>\n",
       "      <td>2232</td>\n",
       "      <td>...</td>\n",
       "      <td>1248</td>\n",
       "      <td>992</td>\n",
       "      <td>1030</td>\n",
       "      <td>1031</td>\n",
       "      <td>795</td>\n",
       "      <td>771</td>\n",
       "      <td>589</td>\n",
       "      <td>499</td>\n",
       "      <td>325</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2011</td>\n",
       "      <td>2375</td>\n",
       "      <td>2112</td>\n",
       "      <td>1542</td>\n",
       "      <td>1583</td>\n",
       "      <td>1875</td>\n",
       "      <td>2288</td>\n",
       "      <td>2247</td>\n",
       "      <td>2352</td>\n",
       "      <td>2210</td>\n",
       "      <td>...</td>\n",
       "      <td>1279</td>\n",
       "      <td>962</td>\n",
       "      <td>1042</td>\n",
       "      <td>988</td>\n",
       "      <td>798</td>\n",
       "      <td>818</td>\n",
       "      <td>599</td>\n",
       "      <td>480</td>\n",
       "      <td>356</td>\n",
       "      <td>214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2044</td>\n",
       "      <td>2343</td>\n",
       "      <td>2145</td>\n",
       "      <td>1594</td>\n",
       "      <td>1640</td>\n",
       "      <td>1840</td>\n",
       "      <td>2314</td>\n",
       "      <td>2246</td>\n",
       "      <td>2418</td>\n",
       "      <td>2210</td>\n",
       "      <td>...</td>\n",
       "      <td>1283</td>\n",
       "      <td>954</td>\n",
       "      <td>1068</td>\n",
       "      <td>971</td>\n",
       "      <td>826</td>\n",
       "      <td>783</td>\n",
       "      <td>627</td>\n",
       "      <td>484</td>\n",
       "      <td>361</td>\n",
       "      <td>234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2109</td>\n",
       "      <td>2330</td>\n",
       "      <td>2182</td>\n",
       "      <td>1670</td>\n",
       "      <td>1662</td>\n",
       "      <td>1857</td>\n",
       "      <td>2312</td>\n",
       "      <td>2256</td>\n",
       "      <td>2463</td>\n",
       "      <td>2213</td>\n",
       "      <td>...</td>\n",
       "      <td>1296</td>\n",
       "      <td>968</td>\n",
       "      <td>1082</td>\n",
       "      <td>975</td>\n",
       "      <td>852</td>\n",
       "      <td>768</td>\n",
       "      <td>643</td>\n",
       "      <td>481</td>\n",
       "      <td>363</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2170</td>\n",
       "      <td>2336</td>\n",
       "      <td>2230</td>\n",
       "      <td>1722</td>\n",
       "      <td>1691</td>\n",
       "      <td>1883</td>\n",
       "      <td>2274</td>\n",
       "      <td>2328</td>\n",
       "      <td>2474</td>\n",
       "      <td>2217</td>\n",
       "      <td>...</td>\n",
       "      <td>1262</td>\n",
       "      <td>983</td>\n",
       "      <td>1129</td>\n",
       "      <td>962</td>\n",
       "      <td>893</td>\n",
       "      <td>741</td>\n",
       "      <td>674</td>\n",
       "      <td>492</td>\n",
       "      <td>360</td>\n",
       "      <td>264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2225</td>\n",
       "      <td>2381</td>\n",
       "      <td>2178</td>\n",
       "      <td>1698</td>\n",
       "      <td>1681</td>\n",
       "      <td>1864</td>\n",
       "      <td>2252</td>\n",
       "      <td>2426</td>\n",
       "      <td>2430</td>\n",
       "      <td>2217</td>\n",
       "      <td>...</td>\n",
       "      <td>1201</td>\n",
       "      <td>974</td>\n",
       "      <td>1194</td>\n",
       "      <td>927</td>\n",
       "      <td>867</td>\n",
       "      <td>756</td>\n",
       "      <td>716</td>\n",
       "      <td>505</td>\n",
       "      <td>414</td>\n",
       "      <td>282</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 11700 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    10101 f0.4  10101 f10.14  10101 f15.19  10101 f20.24  10101 f25.29  \\\n",
       "15        2005          2428          2034          1570          1566   \n",
       "16        2011          2375          2112          1542          1583   \n",
       "17        2044          2343          2145          1594          1640   \n",
       "18        2109          2330          2182          1670          1662   \n",
       "19        2170          2336          2230          1722          1691   \n",
       "20        2225          2381          2178          1698          1681   \n",
       "\n",
       "    10101 f30.34  10101 f35.39  10101 f40.44  10101 f45.49  10101 f5.9  ...  \\\n",
       "15          1936          2246          2337          2292        2232  ...   \n",
       "16          1875          2288          2247          2352        2210  ...   \n",
       "17          1840          2314          2246          2418        2210  ...   \n",
       "18          1857          2312          2256          2463        2213  ...   \n",
       "19          1883          2274          2328          2474        2217  ...   \n",
       "20          1864          2252          2426          2430        2217  ...   \n",
       "\n",
       "    80109 m45.49  80109 m5.9  80109 m50.54  80109 m55.59  80109 m60.64  \\\n",
       "15          1248         992          1030          1031           795   \n",
       "16          1279         962          1042           988           798   \n",
       "17          1283         954          1068           971           826   \n",
       "18          1296         968          1082           975           852   \n",
       "19          1262         983          1129           962           893   \n",
       "20          1201         974          1194           927           867   \n",
       "\n",
       "    80109 m65.69  80109 m70.74  80109 m75.79  80109 m80.84  80109 m85.  \n",
       "15           771           589           499           325         195  \n",
       "16           818           599           480           356         214  \n",
       "17           783           627           484           361         234  \n",
       "18           768           643           481           363         250  \n",
       "19           741           674           492           360         264  \n",
       "20           756           716           505           414         282  \n",
       "\n",
       "[6 rows x 11700 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all-in-one input\n",
    "\n",
    "all_input = tf.concat([full_train_inputs,input_2002_2003,full_val_inputs,input_2006_2007,full_test_inputs],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(19, 1, 11700), dtype=float32, numpy=\n",
       "array([[[0.02000188, 0.01997657, 0.0200624 , ..., 0.01983915,\n",
       "         0.01831472, 0.02011553]],\n",
       "\n",
       "       [[0.02000188, 0.01997657, 0.0200624 , ..., 0.01983915,\n",
       "         0.01831472, 0.02011553]],\n",
       "\n",
       "       [[0.02000188, 0.01997657, 0.0200624 , ..., 0.01983915,\n",
       "         0.01831472, 0.02011553]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[0.02000188, 0.01997657, 0.0200624 , ..., 0.01983915,\n",
       "         0.01831472, 0.02011553]],\n",
       "\n",
       "       [[0.02000188, 0.01997657, 0.0200624 , ..., 0.01983915,\n",
       "         0.01831472, 0.02011553]],\n",
       "\n",
       "       [[0.02000188, 0.01997657, 0.0200624 , ..., 0.01983915,\n",
       "         0.01831472, 0.02011553]]], dtype=float32)>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predictions for years 1993-2011\n",
    "\n",
    "result = full_model(all_input) #1993-2011\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['10101 f0.4', '10101 f10.14', '10101 f15.19', '10101 f20.24',\n",
       "       '10101 f25.29', '10101 f30.34', '10101 f35.39', '10101 f40.44',\n",
       "       '10101 f45.49', '10101 f5.9',\n",
       "       ...\n",
       "       '80109 m45.49', '80109 m5.9', '80109 m50.54', '80109 m55.59',\n",
       "       '80109 m60.64', '80109 m65.69', '80109 m70.74', '80109 m75.79',\n",
       "       '80109 m80.84', '80109 m85.'],\n",
       "      dtype='object', length=11700)"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "Code = []\n",
    "Sex = []\n",
    "Age = []\n",
    "for sets in test_df.columns:\n",
    "    code = sets.split()[0]\n",
    "    sex = sets.split()[1][0]\n",
    "    age = sets.split()[1][1:]\n",
    "    Code.append(code)\n",
    "    Sex.append(sex)\n",
    "    Age.append(age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.02000188 0.01997657 0.0200624  ... 0.01983915 0.01831472 0.02011553]]\n",
      "[[0.02000188 0.01997657 0.0200624  ... 0.01983915 0.01831472 0.02011553]]\n",
      "[[0.02000188 0.01997657 0.0200624  ... 0.01983915 0.01831472 0.02011553]]\n",
      "[[0.02000188 0.01997657 0.0200624  ... 0.01983915 0.01831472 0.02011553]]\n",
      "[[0.02000188 0.01997657 0.0200624  ... 0.01983915 0.01831472 0.02011553]]\n",
      "[[0.02000188 0.01997657 0.0200624  ... 0.01983915 0.01831472 0.02011553]]\n",
      "[[0.02000188 0.01997657 0.0200624  ... 0.01983915 0.01831472 0.02011553]]\n",
      "[[0.02000188 0.01997657 0.0200624  ... 0.01983915 0.01831472 0.02011553]]\n",
      "[[0.02000188 0.01997657 0.0200624  ... 0.01983915 0.01831472 0.02011553]]\n",
      "[[0.02000188 0.01997657 0.0200624  ... 0.01983915 0.01831472 0.02011553]]\n"
     ]
    }
   ],
   "source": [
    "year2002_2011 = result[-10:]\n",
    "year2002_2011 = year2002_2011.numpy()\n",
    "final_result = []\n",
    "final_result.append(Code)\n",
    "final_result.append(Sex)\n",
    "final_result.append(Age)\n",
    "for year in year2002_2011:\n",
    "    print(year)\n",
    "    final_result.append(year[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Code</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>2002</th>\n",
       "      <th>2003</th>\n",
       "      <th>2004</th>\n",
       "      <th>2005</th>\n",
       "      <th>2006</th>\n",
       "      <th>2007</th>\n",
       "      <th>2008</th>\n",
       "      <th>2009</th>\n",
       "      <th>2010</th>\n",
       "      <th>2011</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10101</td>\n",
       "      <td>f</td>\n",
       "      <td>0.4</td>\n",
       "      <td>0.020002</td>\n",
       "      <td>0.020002</td>\n",
       "      <td>0.020002</td>\n",
       "      <td>0.020002</td>\n",
       "      <td>0.020002</td>\n",
       "      <td>0.020002</td>\n",
       "      <td>0.020002</td>\n",
       "      <td>0.020002</td>\n",
       "      <td>0.020002</td>\n",
       "      <td>0.020002</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10101</td>\n",
       "      <td>f</td>\n",
       "      <td>10.14</td>\n",
       "      <td>0.019977</td>\n",
       "      <td>0.019977</td>\n",
       "      <td>0.019977</td>\n",
       "      <td>0.019977</td>\n",
       "      <td>0.019977</td>\n",
       "      <td>0.019977</td>\n",
       "      <td>0.019977</td>\n",
       "      <td>0.019977</td>\n",
       "      <td>0.019977</td>\n",
       "      <td>0.019977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10101</td>\n",
       "      <td>f</td>\n",
       "      <td>15.19</td>\n",
       "      <td>0.020062</td>\n",
       "      <td>0.020062</td>\n",
       "      <td>0.020062</td>\n",
       "      <td>0.020062</td>\n",
       "      <td>0.020062</td>\n",
       "      <td>0.020062</td>\n",
       "      <td>0.020062</td>\n",
       "      <td>0.020062</td>\n",
       "      <td>0.020062</td>\n",
       "      <td>0.020062</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10101</td>\n",
       "      <td>f</td>\n",
       "      <td>20.24</td>\n",
       "      <td>0.019985</td>\n",
       "      <td>0.019985</td>\n",
       "      <td>0.019985</td>\n",
       "      <td>0.019985</td>\n",
       "      <td>0.019985</td>\n",
       "      <td>0.019985</td>\n",
       "      <td>0.019985</td>\n",
       "      <td>0.019985</td>\n",
       "      <td>0.019985</td>\n",
       "      <td>0.019985</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10101</td>\n",
       "      <td>f</td>\n",
       "      <td>25.29</td>\n",
       "      <td>0.020032</td>\n",
       "      <td>0.020032</td>\n",
       "      <td>0.020032</td>\n",
       "      <td>0.020032</td>\n",
       "      <td>0.020032</td>\n",
       "      <td>0.020032</td>\n",
       "      <td>0.020032</td>\n",
       "      <td>0.020032</td>\n",
       "      <td>0.020032</td>\n",
       "      <td>0.020032</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11695</th>\n",
       "      <td>80109</td>\n",
       "      <td>m</td>\n",
       "      <td>65.69</td>\n",
       "      <td>0.020051</td>\n",
       "      <td>0.020051</td>\n",
       "      <td>0.020051</td>\n",
       "      <td>0.020051</td>\n",
       "      <td>0.020051</td>\n",
       "      <td>0.020051</td>\n",
       "      <td>0.020051</td>\n",
       "      <td>0.020051</td>\n",
       "      <td>0.020051</td>\n",
       "      <td>0.020051</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11696</th>\n",
       "      <td>80109</td>\n",
       "      <td>m</td>\n",
       "      <td>70.74</td>\n",
       "      <td>0.020029</td>\n",
       "      <td>0.020029</td>\n",
       "      <td>0.020029</td>\n",
       "      <td>0.020029</td>\n",
       "      <td>0.020029</td>\n",
       "      <td>0.020029</td>\n",
       "      <td>0.020029</td>\n",
       "      <td>0.020029</td>\n",
       "      <td>0.020029</td>\n",
       "      <td>0.020029</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11697</th>\n",
       "      <td>80109</td>\n",
       "      <td>m</td>\n",
       "      <td>75.79</td>\n",
       "      <td>0.019839</td>\n",
       "      <td>0.019839</td>\n",
       "      <td>0.019839</td>\n",
       "      <td>0.019839</td>\n",
       "      <td>0.019839</td>\n",
       "      <td>0.019839</td>\n",
       "      <td>0.019839</td>\n",
       "      <td>0.019839</td>\n",
       "      <td>0.019839</td>\n",
       "      <td>0.019839</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11698</th>\n",
       "      <td>80109</td>\n",
       "      <td>m</td>\n",
       "      <td>80.84</td>\n",
       "      <td>0.018315</td>\n",
       "      <td>0.018315</td>\n",
       "      <td>0.018315</td>\n",
       "      <td>0.018315</td>\n",
       "      <td>0.018315</td>\n",
       "      <td>0.018315</td>\n",
       "      <td>0.018315</td>\n",
       "      <td>0.018315</td>\n",
       "      <td>0.018315</td>\n",
       "      <td>0.018315</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11699</th>\n",
       "      <td>80109</td>\n",
       "      <td>m</td>\n",
       "      <td>85.</td>\n",
       "      <td>0.020116</td>\n",
       "      <td>0.020116</td>\n",
       "      <td>0.020116</td>\n",
       "      <td>0.020116</td>\n",
       "      <td>0.020116</td>\n",
       "      <td>0.020116</td>\n",
       "      <td>0.020116</td>\n",
       "      <td>0.020116</td>\n",
       "      <td>0.020116</td>\n",
       "      <td>0.020116</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11700 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Code Sex    Age      2002      2003      2004      2005      2006  \\\n",
       "0      10101   f    0.4  0.020002  0.020002  0.020002  0.020002  0.020002   \n",
       "1      10101   f  10.14  0.019977  0.019977  0.019977  0.019977  0.019977   \n",
       "2      10101   f  15.19  0.020062  0.020062  0.020062  0.020062  0.020062   \n",
       "3      10101   f  20.24  0.019985  0.019985  0.019985  0.019985  0.019985   \n",
       "4      10101   f  25.29  0.020032  0.020032  0.020032  0.020032  0.020032   \n",
       "...      ...  ..    ...       ...       ...       ...       ...       ...   \n",
       "11695  80109   m  65.69  0.020051  0.020051  0.020051  0.020051  0.020051   \n",
       "11696  80109   m  70.74  0.020029  0.020029  0.020029  0.020029  0.020029   \n",
       "11697  80109   m  75.79  0.019839  0.019839  0.019839  0.019839  0.019839   \n",
       "11698  80109   m  80.84  0.018315  0.018315  0.018315  0.018315  0.018315   \n",
       "11699  80109   m    85.  0.020116  0.020116  0.020116  0.020116  0.020116   \n",
       "\n",
       "           2007      2008      2009      2010      2011  \n",
       "0      0.020002  0.020002  0.020002  0.020002  0.020002  \n",
       "1      0.019977  0.019977  0.019977  0.019977  0.019977  \n",
       "2      0.020062  0.020062  0.020062  0.020062  0.020062  \n",
       "3      0.019985  0.019985  0.019985  0.019985  0.019985  \n",
       "4      0.020032  0.020032  0.020032  0.020032  0.020032  \n",
       "...         ...       ...       ...       ...       ...  \n",
       "11695  0.020051  0.020051  0.020051  0.020051  0.020051  \n",
       "11696  0.020029  0.020029  0.020029  0.020029  0.020029  \n",
       "11697  0.019839  0.019839  0.019839  0.019839  0.019839  \n",
       "11698  0.018315  0.018315  0.018315  0.018315  0.018315  \n",
       "11699  0.020116  0.020116  0.020116  0.020116  0.020116  \n",
       "\n",
       "[11700 rows x 13 columns]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_name = ['Code','Sex','Age', '2002','2003','2004','2005','2006','2007','2008','2009','2010','2011']\n",
    "final_df = pd.DataFrame(final_result).T\n",
    "final_df.columns = column_name\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.rename(column:{})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "52ee2977380704a66854748a73250e0671a9318bd5b3fd45a3df9f851ae61629"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
