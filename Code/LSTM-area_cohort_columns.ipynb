{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import IPython\n",
    "import IPython.display\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import keras_tuner as kt\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (8, 6)\n",
    "mpl.rcParams['axes.grid'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new\n",
    "\n",
    "def split_ts_data(data, val_start, test_start):\n",
    "    year_min = min(data['Year'])\n",
    "    year_max = max(data['Year'])\n",
    "    year_range = year_max-year_min\n",
    "    \n",
    "    assert (val_start >= year_min) & (test_start >= year_min) & (val_start <= year_max) & (test_start <= year_max), \"Parameter out of bounds\"\n",
    "    assert (val_start > year_min) & (test_start > year_min), \"Training set is empty.\"\n",
    "    assert val_start < test_start, \"Validation set is empty.\"\n",
    "    assert year_range > 0, \"Data contains less than 2 years.\"\n",
    "    \n",
    "    \n",
    "    train_data = data[(data['Year']<val_start) & (data['Year']<test_start)]\n",
    "    val_data = data[(data['Year']>=val_start) & (data['Year']<test_start)]\n",
    "    test_data = data[data['Year']>=test_start]\n",
    "    \n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(df, input_width, label_width, shift):\n",
    "    def create_window(tensor):\n",
    "        #input -> length of time series used for training\n",
    "        #shift -> how far off prediction is from last input\n",
    "        #label -> points to predict\n",
    "        total_window_size = input_width + shift\n",
    "        label_start = total_window_size - label_width\n",
    "\n",
    "        input_bounds = slice(0, input_width)\n",
    "        label_bounds = slice(label_start, None)\n",
    "\n",
    "        inputs = tensor[:,input_bounds,:]\n",
    "        labels = tensor[:,label_bounds,:]\n",
    "\n",
    "        inputs.set_shape([None, input_width, None])\n",
    "        labels.set_shape([None, label_width, None])\n",
    "\n",
    "        return inputs, labels\n",
    "    \n",
    "    total_window_size = input_width + shift\n",
    "    \n",
    "    arr = np.array(df, dtype=np.float32)\n",
    "    ds = tf.keras.utils.timeseries_dataset_from_array(\n",
    "      data=arr,\n",
    "      targets=None,\n",
    "      sequence_length=total_window_size,\n",
    "      sequence_stride=1,\n",
    "      shuffle=False,\n",
    "      batch_size=32,)\n",
    "    \n",
    "    ds = ds.map(create_window)\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_and_fit(model, num_epochs, input_optimizer='adam', input_loss='mse'):\n",
    "    model.compile(optimizer=input_optimizer, loss=input_loss)\n",
    "    history = model.fit(x=train_inputs,y=train_labels, batch_size = 32, epochs=num_epochs, validation_data=(val_inputs,val_labels), shuffle=False)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def col_dict(np_df):\n",
    "    return_dict = {col:index for index, col in enumerate(np_df.columns)}\n",
    "    \n",
    "    return return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(df, ds, input_width, label_width, shift, model=None, plot_col='10101 m0.4', max_subplots=3):\n",
    "    #ensure that df and ds match e.g. train_df must be accompanied by train_ds\n",
    "    col_indices = col_dict(df)\n",
    "    \n",
    "    total_window_size = label_width + shift\n",
    "    input_slice = slice(0,input_width)\n",
    "    input_indices = np.arange(total_window_size)[input_slice]\n",
    "    label_start = total_window_size - label_width\n",
    "    labels_slice = slice(label_start, None)\n",
    "    label_indices = np.arange(total_window_size)[labels_slice]\n",
    "    \n",
    "    inputs = next(iter(ds))[0]\n",
    "    labels = next(iter(ds))[1]\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plot_col_index = col_indices[plot_col] \n",
    "    max_n = min(max_subplots, len(inputs))\n",
    "    \n",
    "    for n in range(max_n):\n",
    "        plt.subplot(max_n, 1, n+1)\n",
    "        plt.ylabel(plot_col)\n",
    "        plt.plot(input_indices, inputs[n, :, plot_col_index],\n",
    "             label='Inputs', marker='.', zorder=-10)\n",
    "        \n",
    "        plt.scatter(label_indices, labels[n, :, plot_col_index],\n",
    "                edgecolors='k', label='Labels', c='#2ca02c', s=64)\n",
    "        \n",
    "        if model is not None:\n",
    "          predictions = model(inputs)\n",
    "          plt.scatter(label_indices, predictions[n, :, plot_col_index],\n",
    "                  marker='X', edgecolors='k', label='Predictions',\n",
    "                  c='#ff7f0e', s=64)\n",
    "            \n",
    "        if n == 0:\n",
    "          plt.legend()\n",
    "        \n",
    "    plt.xlabel('Year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Main code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edit parameters here, but do not rename variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Read, preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('../Data/newSA3.csv')\n",
    "\n",
    "\n",
    "\n",
    "#Parameters\n",
    "validation_start = 2002\n",
    "test_start = 2006\n",
    "#\n",
    "\n",
    "\n",
    "\n",
    "train_df, val_df, test_df = split_ts_data(raw_data, validation_start, test_start)\n",
    "\n",
    "train_df = train_df[train_df.columns.difference([\"Unnamed: 0\",\"Year\"])]\n",
    "val_df = val_df[val_df.columns.difference([\"Unnamed: 0\",\"Year\"])]\n",
    "test_df = test_df[test_df.columns.difference([\"Unnamed: 0\",\"Year\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "input_width = 2 #data used in prediction\n",
    "label_width = 1 #points to predict\n",
    "shift = 1 #how many years away is the last point to predict\n",
    "#\n",
    "\n",
    "\n",
    "\n",
    "train_ds = make_dataset(train_df, input_width, label_width, shift)\n",
    "val_ds = make_dataset(val_df, input_width, label_width, shift)\n",
    "test_ds = make_dataset(test_df, input_width, label_width, shift)\n",
    "\n",
    "num_cols = next(iter(train_ds))[0].shape[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SACohortModel(kt.HyperModel):\n",
    "    def build(self,hp):\n",
    "        #### Hyperparameters\n",
    "        # add hyperparameters as needed when adding layers\n",
    "        \n",
    "        ##layer hyperparameters\n",
    "        hp_lstm1_units = hp.Choice('units',[10,30,50])\n",
    "        hp_lstm1_act = hp.Choice('activation', [\"relu\"])\n",
    "\n",
    "        ##model hyperparameters -> adjust tf.keras.models type and model.add layers\n",
    "        model = tf.keras.models.Sequential()\n",
    "        model.add(tf.keras.layers.LSTM(units = hp_lstm1_units, \n",
    "                                       activation=hp_lstm1_act, \n",
    "                                       return_sequences=False))\n",
    "        \n",
    "        model.add(tf.keras.layers.Dense(label_width * num_cols))\n",
    "        model.add(tf.keras.layers.Reshape([label_width,num_cols]))\n",
    "        \n",
    "        ##compilation hyperparameters\n",
    "        hp_epochs = hp.Choice(\"epochs\",[10,20,30])\n",
    "        hp_input_optimizer = hp.Choice('input_optimizer',[\"adam\", \"adadelta\"])\n",
    "        loss_fun = \"mse\"\n",
    "        \n",
    "        ####\n",
    "        \n",
    "        #Do not edit\n",
    "        model.compile(loss = loss_fun)\n",
    "        \n",
    "        return model\n",
    "        #Do not edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project .\\untitled_project\\oracle.json\n",
      "INFO:tensorflow:Reloading Tuner from .\\untitled_project\\tuner0.json\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "#Parameter\n",
    "num_epochs = 10\n",
    "#\n",
    "\n",
    "train_inputs = next(iter(train_ds))[0]\n",
    "train_labels = next(iter(train_ds))[1]\n",
    "\n",
    "val_inputs = next(iter(val_ds))[0]\n",
    "val_labels = next(iter(val_ds))[1]\n",
    "\n",
    "test_inputs = next(iter(test_ds))[0]\n",
    "\n",
    "tuner = kt.RandomSearch(\n",
    "    SACohortModel(),\n",
    "    objective='val_loss',\n",
    "    max_trials=5)\n",
    "\n",
    "tuner.search(train_inputs, train_labels, epochs = num_epochs, validation_data = (val_inputs, val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model with above parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "1/1 [==============================] - 4s 4s/step - loss: 4138923.0000 - val_loss: 4711191.5000\n",
      "Epoch 2/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 4156189.2500 - val_loss: 4642102.0000\n",
      "Epoch 3/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 4094916.0000 - val_loss: 4615849.0000\n",
      "Epoch 4/200\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 4073287.0000 - val_loss: 4520344.0000\n",
      "Epoch 5/200\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 3991287.5000 - val_loss: 4385751.0000\n",
      "Epoch 6/200\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 3875326.7500 - val_loss: 4241473.0000\n",
      "Epoch 7/200\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 3750502.7500 - val_loss: 4104215.0000\n",
      "Epoch 8/200\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 3631001.7500 - val_loss: 3963814.5000\n",
      "Epoch 9/200\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 3508579.0000 - val_loss: 3798443.0000\n",
      "Epoch 10/200\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 3365197.7500 - val_loss: 3607700.0000\n",
      "Epoch 11/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 3200667.0000 - val_loss: 3408658.5000\n",
      "Epoch 12/200\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 3029273.0000 - val_loss: 3212993.0000\n",
      "Epoch 13/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2860405.0000 - val_loss: 3017856.2500\n",
      "Epoch 14/200\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 2690917.5000 - val_loss: 2818206.5000\n",
      "Epoch 15/200\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 2514355.7500 - val_loss: 2614506.5000\n",
      "Epoch 16/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 2334772.5000 - val_loss: 2409673.5000\n",
      "Epoch 17/200\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 2152253.0000 - val_loss: 2220428.5000\n",
      "Epoch 18/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 1981012.0000 - val_loss: 2040395.7500\n",
      "Epoch 19/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 1817362.1250 - val_loss: 1862595.0000\n",
      "Epoch 20/200\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 1656429.1250 - val_loss: 1686221.7500\n",
      "Epoch 21/200\n",
      "1/1 [==============================] - 0s 74ms/step - loss: 1498062.3750 - val_loss: 1518232.1250\n",
      "Epoch 22/200\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 1347986.6250 - val_loss: 1365483.7500\n",
      "Epoch 23/200\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 1211276.6250 - val_loss: 1229082.7500\n",
      "Epoch 24/200\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 1087889.5000 - val_loss: 1106690.0000\n",
      "Epoch 25/200\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 975097.0000 - val_loss: 997690.8125\n",
      "Epoch 26/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 872256.1250 - val_loss: 903471.6875\n",
      "Epoch 27/200\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 781127.6875 - val_loss: 823337.8750\n",
      "Epoch 28/200\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 702233.1250 - val_loss: 752694.4375\n",
      "Epoch 29/200\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 632894.3750 - val_loss: 686795.3750\n",
      "Epoch 30/200\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 569854.5000 - val_loss: 624992.1250\n",
      "Epoch 31/200\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 512529.0000 - val_loss: 569671.0625\n",
      "Epoch 32/200\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 462169.3750 - val_loss: 522160.1875\n",
      "Epoch 33/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 418758.2188 - val_loss: 481248.5000\n",
      "Epoch 34/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 380259.0312 - val_loss: 445296.4688\n",
      "Epoch 35/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 344847.6875 - val_loss: 413918.0625\n",
      "Epoch 36/200\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 312485.4062 - val_loss: 386810.3125\n",
      "Epoch 37/200\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 283741.6250 - val_loss: 362180.4375\n",
      "Epoch 38/200\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 258060.6719 - val_loss: 337740.6875\n",
      "Epoch 39/200\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 234139.9219 - val_loss: 313036.5625\n",
      "Epoch 40/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 211600.0781 - val_loss: 289660.5625\n",
      "Epoch 41/200\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 191188.6875 - val_loss: 269208.5000\n",
      "Epoch 42/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 173348.3125 - val_loss: 251941.9688\n",
      "Epoch 43/200\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 157503.5469 - val_loss: 237473.4375\n",
      "Epoch 44/200\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 142993.4531 - val_loss: 225641.1406\n",
      "Epoch 45/200\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 129925.3906 - val_loss: 216049.4375\n",
      "Epoch 46/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 118618.8359 - val_loss: 207476.0625\n",
      "Epoch 47/200\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 108755.5703 - val_loss: 198675.9844\n",
      "Epoch 48/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 99694.9062 - val_loss: 189613.2500\n",
      "Epoch 49/200\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 91320.9062 - val_loss: 181266.2812\n",
      "Epoch 50/200\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 83947.9297 - val_loss: 174394.7500\n",
      "Epoch 51/200\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 77560.7031 - val_loss: 169051.3125\n",
      "Epoch 52/200\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 71746.0000 - val_loss: 165057.0312\n",
      "Epoch 53/200\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 66321.5781 - val_loss: 162181.2188\n",
      "Epoch 54/200\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 61478.0625 - val_loss: 159799.0625\n",
      "Epoch 55/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 57260.7891 - val_loss: 157064.9844\n",
      "Epoch 56/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 53400.5820 - val_loss: 153703.4219\n",
      "Epoch 57/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 49752.5469 - val_loss: 150239.1250\n",
      "Epoch 58/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 46471.8594 - val_loss: 147328.7500\n",
      "Epoch 59/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 43639.3047 - val_loss: 145221.0312\n",
      "Epoch 60/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 41086.5977 - val_loss: 143870.5469\n",
      "Epoch 61/200\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 38702.5234 - val_loss: 143089.3750\n",
      "Epoch 62/200\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 36580.7266 - val_loss: 142441.1562\n",
      "Epoch 63/200\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 34760.1523 - val_loss: 141397.6875\n",
      "Epoch 64/200\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 33108.2773 - val_loss: 139821.4062\n",
      "Epoch 65/200\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 31550.8926 - val_loss: 138077.1406\n",
      "Epoch 66/200\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 30161.8965 - val_loss: 136605.3438\n",
      "Epoch 67/200\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 28961.6641 - val_loss: 135592.6250\n",
      "Epoch 68/200\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 27855.1074 - val_loss: 135012.7188\n",
      "Epoch 69/200\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 26806.5176 - val_loss: 134687.0625\n",
      "Epoch 70/200\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 25873.4277 - val_loss: 134285.4062\n",
      "Epoch 71/200\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 25055.4375 - val_loss: 133532.7969\n",
      "Epoch 72/200\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 24286.4570 - val_loss: 132477.5781\n",
      "Epoch 73/200\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 23564.1582 - val_loss: 131423.5938\n",
      "Epoch 74/200\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 22935.0332 - val_loss: 130631.0781\n",
      "Epoch 75/200\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 22385.8633 - val_loss: 130178.6875\n",
      "Epoch 76/200\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 21874.4199 - val_loss: 129997.4688\n",
      "Epoch 77/200\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 21413.1074 - val_loss: 129895.6719\n",
      "Epoch 78/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 21024.9648 - val_loss: 129645.2891\n",
      "Epoch 79/200\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 20683.4844 - val_loss: 129172.5078\n",
      "Epoch 80/200\n",
      "1/1 [==============================] - 0s 71ms/step - loss: 20364.6973 - val_loss: 128620.8594\n",
      "Epoch 81/200\n",
      "1/1 [==============================] - 0s 69ms/step - loss: 20085.7734 - val_loss: 128191.7422\n",
      "Epoch 82/200\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 19851.5137 - val_loss: 127986.8438\n",
      "Epoch 83/200\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 19636.5547 - val_loss: 127985.3125\n",
      "Epoch 84/200\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 19435.1914 - val_loss: 128067.4844\n",
      "Epoch 85/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 19261.8672 - val_loss: 128070.2344\n",
      "Epoch 86/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 19109.5723 - val_loss: 127909.0469\n",
      "Epoch 87/200\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 18962.7910 - val_loss: 127650.4219\n",
      "Epoch 88/200\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 18828.0703 - val_loss: 127432.2969\n",
      "Epoch 89/200\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 18712.5840 - val_loss: 127343.9219\n",
      "Epoch 90/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 18605.8027 - val_loss: 127383.8594\n",
      "Epoch 91/200\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 18503.7031 - val_loss: 127472.9609\n",
      "Epoch 92/200\n",
      "1/1 [==============================] - 0s 72ms/step - loss: 18414.8711 - val_loss: 127498.5859\n",
      "Epoch 93/200\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 18337.8477 - val_loss: 127399.4688\n",
      "Epoch 94/200\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 18264.7949 - val_loss: 127217.3125\n",
      "Epoch 95/200\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 18198.8223 - val_loss: 127048.1797\n",
      "Epoch 96/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 18143.6914 - val_loss: 126958.5859\n",
      "Epoch 97/200\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 18093.6543 - val_loss: 126948.9922\n",
      "Epoch 98/200\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 18046.4238 - val_loss: 126962.2656\n",
      "Epoch 99/200\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 18005.9824 - val_loss: 126922.1719\n",
      "Epoch 100/200\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 17970.6973 - val_loss: 126794.7500\n",
      "Epoch 101/200\n",
      "1/1 [==============================] - 0s 75ms/step - loss: 17936.5527 - val_loss: 126616.9062\n",
      "Epoch 102/200\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 17905.5723 - val_loss: 126457.4453\n",
      "Epoch 103/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 17879.0957 - val_loss: 126359.7812\n",
      "Epoch 104/200\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 17854.0039 - val_loss: 126318.0156\n",
      "Epoch 105/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 17830.1641 - val_loss: 126286.7500\n",
      "Epoch 106/200\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 17809.7500 - val_loss: 126216.2031\n",
      "Epoch 107/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 17791.3750 - val_loss: 126094.9453\n",
      "Epoch 108/200\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 17773.7109 - val_loss: 125958.2656\n",
      "Epoch 109/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 17758.3828 - val_loss: 125853.5625\n",
      "Epoch 110/200\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 17745.3438 - val_loss: 125802.5000\n",
      "Epoch 111/200\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 17733.0215 - val_loss: 125789.7500\n",
      "Epoch 112/200\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 17722.0078 - val_loss: 125776.9609\n",
      "Epoch 113/200\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 17712.8926 - val_loss: 125734.2031\n",
      "Epoch 114/200\n",
      "1/1 [==============================] - 0s 83ms/step - loss: 17704.4336 - val_loss: 125665.1875\n",
      "Epoch 115/200\n",
      "1/1 [==============================] - 0s 73ms/step - loss: 17696.5117 - val_loss: 125600.6875\n",
      "Epoch 116/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 17689.8672 - val_loss: 125569.1250\n",
      "Epoch 117/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 17683.8789 - val_loss: 125574.2031\n",
      "Epoch 118/200\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 17678.0879 - val_loss: 125594.3438\n",
      "Epoch 119/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 17673.1172 - val_loss: 125600.5469\n",
      "Epoch 120/200\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 17668.8047 - val_loss: 125580.5469\n",
      "Epoch 121/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 17664.6094 - val_loss: 125547.6797\n",
      "Epoch 122/200\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 17660.9043 - val_loss: 125525.8750\n",
      "Epoch 123/200\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 17657.7734 - val_loss: 125527.7812\n",
      "Epoch 124/200\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 17654.7539 - val_loss: 125545.5938\n",
      "Epoch 125/200\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 17651.9922 - val_loss: 125558.5312\n",
      "Epoch 126/200\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 17649.6836 - val_loss: 125551.0156\n",
      "Epoch 127/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 17647.4941 - val_loss: 125525.8750\n",
      "Epoch 128/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 17645.4336 - val_loss: 125500.2812\n",
      "Epoch 129/200\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 17643.7051 - val_loss: 125489.2188\n",
      "Epoch 130/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 17642.1016 - val_loss: 125493.4609\n",
      "Epoch 131/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 17640.5664 - val_loss: 125500.3438\n",
      "Epoch 132/200\n",
      "1/1 [==============================] - 0s 79ms/step - loss: 17639.2734 - val_loss: 125495.7969\n",
      "Epoch 133/200\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 17638.1133 - val_loss: 125477.2500\n",
      "Epoch 134/200\n",
      "1/1 [==============================] - 0s 68ms/step - loss: 17636.9883 - val_loss: 125454.9844\n",
      "Epoch 135/200\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 17636.0312 - val_loss: 125441.8438\n",
      "Epoch 136/200\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 17635.1855 - val_loss: 125441.7578\n",
      "Epoch 137/200\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 17634.3594 - val_loss: 125447.2969\n",
      "Epoch 138/200\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 17633.6387 - val_loss: 125447.1094\n",
      "Epoch 139/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 17633.0117 - val_loss: 125436.4844\n",
      "Epoch 140/200\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 17632.3945 - val_loss: 125421.1484\n",
      "Epoch 141/200\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 17631.8477 - val_loss: 125411.0000\n",
      "Epoch 142/200\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 17631.3691 - val_loss: 125410.6953\n",
      "Epoch 143/200\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 17630.9062 - val_loss: 125415.9844\n",
      "Epoch 144/200\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 17630.4902 - val_loss: 125418.1875\n",
      "Epoch 145/200\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 17630.1289 - val_loss: 125412.4766\n",
      "Epoch 146/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 17629.7812 - val_loss: 125402.1016\n",
      "Epoch 147/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 17629.4629 - val_loss: 125394.5312\n",
      "Epoch 148/200\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 17629.1914 - val_loss: 125394.1953\n",
      "Epoch 149/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 17628.9238 - val_loss: 125398.7188\n",
      "Epoch 150/200\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 17628.6777 - val_loss: 125401.7734\n",
      "Epoch 151/200\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 17628.4629 - val_loss: 125399.2969\n",
      "Epoch 152/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 17628.2539 - val_loss: 125393.2422\n",
      "Epoch 153/200\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 17628.0566 - val_loss: 125389.1406\n",
      "Epoch 154/200\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 17627.8848 - val_loss: 125390.5469\n",
      "Epoch 155/200\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 17627.7129 - val_loss: 125395.7734\n",
      "Epoch 156/200\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 17627.5527 - val_loss: 125399.9844\n",
      "Epoch 157/200\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 17627.4102 - val_loss: 125399.9688\n",
      "Epoch 158/200\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 17627.2676 - val_loss: 125397.0625\n",
      "Epoch 159/200\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 17627.1328 - val_loss: 125395.3750\n",
      "Epoch 160/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 17627.0098 - val_loss: 125397.5078\n",
      "Epoch 161/200\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 17626.8926 - val_loss: 125402.1016\n",
      "Epoch 162/200\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17626.7812 - val_loss: 125405.4375\n",
      "Epoch 163/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 17626.6777 - val_loss: 125405.2031\n",
      "Epoch 164/200\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 17626.5723 - val_loss: 125402.6719\n",
      "Epoch 165/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 17626.4766 - val_loss: 125400.9531\n",
      "Epoch 166/200\n",
      "1/1 [==============================] - 0s 62ms/step - loss: 17626.3848 - val_loss: 125401.8750\n",
      "Epoch 167/200\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 17626.2930 - val_loss: 125404.1875\n",
      "Epoch 168/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 17626.2070 - val_loss: 125405.0859\n",
      "Epoch 169/200\n",
      "1/1 [==============================] - 0s 61ms/step - loss: 17626.1230 - val_loss: 125403.2344\n",
      "Epoch 170/200\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 17626.0391 - val_loss: 125399.8828\n",
      "Epoch 171/200\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 17625.9609 - val_loss: 125397.4844\n",
      "Epoch 172/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 17625.8848 - val_loss: 125397.1641\n",
      "Epoch 173/200\n",
      "1/1 [==============================] - 0s 65ms/step - loss: 17625.8086 - val_loss: 125397.6562\n",
      "Epoch 174/200\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 17625.7363 - val_loss: 125396.9219\n",
      "Epoch 175/200\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 17625.6660 - val_loss: 125394.3516\n",
      "Epoch 176/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 17625.5996 - val_loss: 125391.2109\n",
      "Epoch 177/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 17625.5312 - val_loss: 125389.2812\n",
      "Epoch 178/200\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 17625.4668 - val_loss: 125389.0078\n",
      "Epoch 179/200\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 17625.4023 - val_loss: 125389.1250\n",
      "Epoch 180/200\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 17625.3398 - val_loss: 125388.2031\n",
      "Epoch 181/200\n",
      "1/1 [==============================] - 0s 59ms/step - loss: 17625.2793 - val_loss: 125386.0781\n",
      "Epoch 182/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 17625.2188 - val_loss: 125383.9141\n",
      "Epoch 183/200\n",
      "1/1 [==============================] - 0s 81ms/step - loss: 17625.1602 - val_loss: 125382.8438\n",
      "Epoch 184/200\n",
      "1/1 [==============================] - 0s 55ms/step - loss: 17625.1016 - val_loss: 125382.8125\n",
      "Epoch 185/200\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 17625.0410 - val_loss: 125382.6875\n",
      "Epoch 186/200\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 17624.9863 - val_loss: 125381.5156\n",
      "Epoch 187/200\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 17624.9297 - val_loss: 125379.5391\n",
      "Epoch 188/200\n",
      "1/1 [==============================] - 0s 64ms/step - loss: 17624.8711 - val_loss: 125377.7812\n",
      "Epoch 189/200\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 17624.8203 - val_loss: 125376.8906\n",
      "Epoch 190/200\n",
      "1/1 [==============================] - 0s 63ms/step - loss: 17624.7656 - val_loss: 125376.5156\n",
      "Epoch 191/200\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 17624.7109 - val_loss: 125375.7188\n",
      "Epoch 192/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 17624.6582 - val_loss: 125374.0625\n",
      "Epoch 193/200\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 17624.6055 - val_loss: 125372.0547\n",
      "Epoch 194/200\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 17624.5547 - val_loss: 125370.4844\n",
      "Epoch 195/200\n",
      "1/1 [==============================] - 0s 67ms/step - loss: 17624.5020 - val_loss: 125369.5859\n",
      "Epoch 196/200\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 17624.4492 - val_loss: 125368.8438\n",
      "Epoch 197/200\n",
      "1/1 [==============================] - 0s 70ms/step - loss: 17624.3984 - val_loss: 125367.5938\n",
      "Epoch 198/200\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 17624.3496 - val_loss: 125365.8281\n",
      "Epoch 199/200\n",
      "1/1 [==============================] - 0s 66ms/step - loss: 17624.2988 - val_loss: 125364.0703\n",
      "Epoch 200/200\n",
      "1/1 [==============================] - 0s 60ms/step - loss: 17624.2500 - val_loss: 125362.7812\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e2263d6850>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_optimizer = 'adam'\n",
    "loss_fun = 'mse'\n",
    "\n",
    "\n",
    "full_model = tf.keras.models.Sequential()\n",
    "full_model.add(tf.keras.layers.LSTM(units = 30, \n",
    "                                       activation=tf.keras.activations.relu, \n",
    "                                       return_sequences=False))\n",
    "        \n",
    "full_model.add(tf.keras.layers.Dense(label_width * num_cols, activation=\"linear\"))\n",
    "full_model.add(tf.keras.layers.Reshape([label_width,num_cols]))\n",
    "compile_and_fit(full_model, num_epochs=200, input_optimizer=model_optimizer, input_loss=loss_fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_18\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_18 (LSTM)              (None, 30)                1407720   \n",
      "                                                                 \n",
      " dense_18 (Dense)            (None, 11700)             362700    \n",
      "                                                                 \n",
      " reshape_17 (Reshape)        (None, 1, 11700)          0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,770,420\n",
      "Trainable params: 1,770,420\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "full_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store full model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1991-2001\n",
    "\n",
    "full_train_inputs = next(iter(train_ds))[0] #pairs from 1991-2000\n",
    "full_train_labels = next(iter(train_ds))[1] #1993-2001\n",
    "full_train_predictions = full_model(full_train_inputs) #1993-2001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(9, 1, 11700), dtype=float32, numpy=\n",
       "array([[[2382., 2354., 2126., ...,  209.,  105.,   59.]],\n",
       "\n",
       "       [[2357., 2351., 2084., ...,  212.,  112.,   65.]],\n",
       "\n",
       "       [[2318., 2357., 2055., ...,  247.,  116.,   69.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[2132., 2348., 2039., ...,  389.,  163.,   95.]],\n",
       "\n",
       "       [[2107., 2335., 2029., ...,  439.,  190.,  103.]],\n",
       "\n",
       "       [[2077., 2259., 2051., ...,  457.,  221.,  119.]]], dtype=float32)>"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(9, 1, 11700), dtype=float32, numpy=\n",
       "array([[[2164.401   , 2269.98    , 2002.4312  , ...,  315.13034 ,\n",
       "          140.55989 ,   81.423744]],\n",
       "\n",
       "       [[2177.8188  , 2284.0527  , 2014.845   , ...,  317.08392 ,\n",
       "          141.43126 ,   81.92846 ]],\n",
       "\n",
       "       [[2192.166   , 2299.0999  , 2028.1185  , ...,  319.17285 ,\n",
       "          142.36296 ,   82.46813 ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[2257.086   , 2367.187   , 2088.1804  , ...,  328.6249  ,\n",
       "          146.57886 ,   84.9101  ]],\n",
       "\n",
       "       [[2274.0562  , 2384.985   , 2103.8809  , ...,  331.0957  ,\n",
       "          147.6809  ,   85.54844 ]],\n",
       "\n",
       "       [[2293.4932  , 2405.37    , 2121.8633  , ...,  333.92563 ,\n",
       "          148.94312 ,   86.27956 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_train_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2002-2005\n",
    "\n",
    "full_val_inputs = next(iter(val_ds))[0] #pairs 2002,2003 and 2003,2004\n",
    "full_val_labels = next(iter(val_ds))[1] #2004 and 2005\n",
    "full_val_predictions = full_model(val_inputs) #2004 and 2005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_test_inputs = next(iter(test_ds))[0] #pairs from 2006-2010\n",
    "full_test_labels = next(iter(test_ds))[1] #2008-2011\n",
    "full_test_predictions = full_model(test_inputs) #2008-2011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10101 f0.4</th>\n",
       "      <th>10101 f10.14</th>\n",
       "      <th>10101 f15.19</th>\n",
       "      <th>10101 f20.24</th>\n",
       "      <th>10101 f25.29</th>\n",
       "      <th>10101 f30.34</th>\n",
       "      <th>10101 f35.39</th>\n",
       "      <th>10101 f40.44</th>\n",
       "      <th>10101 f45.49</th>\n",
       "      <th>10101 f5.9</th>\n",
       "      <th>...</th>\n",
       "      <th>80109 m45.49</th>\n",
       "      <th>80109 m5.9</th>\n",
       "      <th>80109 m50.54</th>\n",
       "      <th>80109 m55.59</th>\n",
       "      <th>80109 m60.64</th>\n",
       "      <th>80109 m65.69</th>\n",
       "      <th>80109 m70.74</th>\n",
       "      <th>80109 m75.79</th>\n",
       "      <th>80109 m80.84</th>\n",
       "      <th>80109 m85.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2400</td>\n",
       "      <td>2347</td>\n",
       "      <td>2277</td>\n",
       "      <td>1820</td>\n",
       "      <td>2152</td>\n",
       "      <td>2315</td>\n",
       "      <td>2185</td>\n",
       "      <td>2146</td>\n",
       "      <td>1859</td>\n",
       "      <td>2495</td>\n",
       "      <td>...</td>\n",
       "      <td>1219</td>\n",
       "      <td>1002</td>\n",
       "      <td>1226</td>\n",
       "      <td>920</td>\n",
       "      <td>784</td>\n",
       "      <td>606</td>\n",
       "      <td>328</td>\n",
       "      <td>180</td>\n",
       "      <td>98</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2392</td>\n",
       "      <td>2344</td>\n",
       "      <td>2194</td>\n",
       "      <td>1830</td>\n",
       "      <td>2067</td>\n",
       "      <td>2339</td>\n",
       "      <td>2204</td>\n",
       "      <td>2139</td>\n",
       "      <td>1954</td>\n",
       "      <td>2480</td>\n",
       "      <td>...</td>\n",
       "      <td>1257</td>\n",
       "      <td>1009</td>\n",
       "      <td>1209</td>\n",
       "      <td>931</td>\n",
       "      <td>794</td>\n",
       "      <td>617</td>\n",
       "      <td>376</td>\n",
       "      <td>200</td>\n",
       "      <td>104</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2382</td>\n",
       "      <td>2354</td>\n",
       "      <td>2126</td>\n",
       "      <td>1813</td>\n",
       "      <td>1977</td>\n",
       "      <td>2329</td>\n",
       "      <td>2225</td>\n",
       "      <td>2139</td>\n",
       "      <td>2037</td>\n",
       "      <td>2451</td>\n",
       "      <td>...</td>\n",
       "      <td>1285</td>\n",
       "      <td>993</td>\n",
       "      <td>1154</td>\n",
       "      <td>963</td>\n",
       "      <td>792</td>\n",
       "      <td>634</td>\n",
       "      <td>413</td>\n",
       "      <td>209</td>\n",
       "      <td>105</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2357</td>\n",
       "      <td>2351</td>\n",
       "      <td>2084</td>\n",
       "      <td>1777</td>\n",
       "      <td>1913</td>\n",
       "      <td>2316</td>\n",
       "      <td>2232</td>\n",
       "      <td>2158</td>\n",
       "      <td>2079</td>\n",
       "      <td>2418</td>\n",
       "      <td>...</td>\n",
       "      <td>1284</td>\n",
       "      <td>988</td>\n",
       "      <td>1123</td>\n",
       "      <td>998</td>\n",
       "      <td>778</td>\n",
       "      <td>649</td>\n",
       "      <td>459</td>\n",
       "      <td>212</td>\n",
       "      <td>112</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2318</td>\n",
       "      <td>2357</td>\n",
       "      <td>2055</td>\n",
       "      <td>1734</td>\n",
       "      <td>1869</td>\n",
       "      <td>2272</td>\n",
       "      <td>2256</td>\n",
       "      <td>2165</td>\n",
       "      <td>2111</td>\n",
       "      <td>2403</td>\n",
       "      <td>...</td>\n",
       "      <td>1277</td>\n",
       "      <td>998</td>\n",
       "      <td>1105</td>\n",
       "      <td>1020</td>\n",
       "      <td>819</td>\n",
       "      <td>648</td>\n",
       "      <td>486</td>\n",
       "      <td>247</td>\n",
       "      <td>116</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2280</td>\n",
       "      <td>2360</td>\n",
       "      <td>2060</td>\n",
       "      <td>1656</td>\n",
       "      <td>1869</td>\n",
       "      <td>2218</td>\n",
       "      <td>2290</td>\n",
       "      <td>2186</td>\n",
       "      <td>2150</td>\n",
       "      <td>2400</td>\n",
       "      <td>...</td>\n",
       "      <td>1244</td>\n",
       "      <td>1015</td>\n",
       "      <td>1112</td>\n",
       "      <td>1051</td>\n",
       "      <td>833</td>\n",
       "      <td>672</td>\n",
       "      <td>513</td>\n",
       "      <td>277</td>\n",
       "      <td>124</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2242</td>\n",
       "      <td>2327</td>\n",
       "      <td>2054</td>\n",
       "      <td>1555</td>\n",
       "      <td>1905</td>\n",
       "      <td>2104</td>\n",
       "      <td>2340</td>\n",
       "      <td>2208</td>\n",
       "      <td>2153</td>\n",
       "      <td>2393</td>\n",
       "      <td>...</td>\n",
       "      <td>1188</td>\n",
       "      <td>985</td>\n",
       "      <td>1117</td>\n",
       "      <td>1086</td>\n",
       "      <td>835</td>\n",
       "      <td>676</td>\n",
       "      <td>527</td>\n",
       "      <td>316</td>\n",
       "      <td>123</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2171</td>\n",
       "      <td>2338</td>\n",
       "      <td>2051</td>\n",
       "      <td>1480</td>\n",
       "      <td>1912</td>\n",
       "      <td>2054</td>\n",
       "      <td>2403</td>\n",
       "      <td>2171</td>\n",
       "      <td>2153</td>\n",
       "      <td>2400</td>\n",
       "      <td>...</td>\n",
       "      <td>1157</td>\n",
       "      <td>975</td>\n",
       "      <td>1127</td>\n",
       "      <td>1083</td>\n",
       "      <td>847</td>\n",
       "      <td>690</td>\n",
       "      <td>572</td>\n",
       "      <td>358</td>\n",
       "      <td>142</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2132</td>\n",
       "      <td>2348</td>\n",
       "      <td>2039</td>\n",
       "      <td>1450</td>\n",
       "      <td>1892</td>\n",
       "      <td>2005</td>\n",
       "      <td>2395</td>\n",
       "      <td>2217</td>\n",
       "      <td>2195</td>\n",
       "      <td>2400</td>\n",
       "      <td>...</td>\n",
       "      <td>1149</td>\n",
       "      <td>975</td>\n",
       "      <td>1123</td>\n",
       "      <td>1051</td>\n",
       "      <td>851</td>\n",
       "      <td>694</td>\n",
       "      <td>598</td>\n",
       "      <td>389</td>\n",
       "      <td>163</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2107</td>\n",
       "      <td>2335</td>\n",
       "      <td>2029</td>\n",
       "      <td>1452</td>\n",
       "      <td>1848</td>\n",
       "      <td>1991</td>\n",
       "      <td>2363</td>\n",
       "      <td>2251</td>\n",
       "      <td>2201</td>\n",
       "      <td>2393</td>\n",
       "      <td>...</td>\n",
       "      <td>1122</td>\n",
       "      <td>967</td>\n",
       "      <td>1163</td>\n",
       "      <td>997</td>\n",
       "      <td>892</td>\n",
       "      <td>717</td>\n",
       "      <td>577</td>\n",
       "      <td>439</td>\n",
       "      <td>190</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2077</td>\n",
       "      <td>2259</td>\n",
       "      <td>2051</td>\n",
       "      <td>1467</td>\n",
       "      <td>1777</td>\n",
       "      <td>1997</td>\n",
       "      <td>2263</td>\n",
       "      <td>2282</td>\n",
       "      <td>2203</td>\n",
       "      <td>2302</td>\n",
       "      <td>...</td>\n",
       "      <td>1100</td>\n",
       "      <td>945</td>\n",
       "      <td>1200</td>\n",
       "      <td>996</td>\n",
       "      <td>898</td>\n",
       "      <td>676</td>\n",
       "      <td>596</td>\n",
       "      <td>457</td>\n",
       "      <td>221</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows  11700 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    10101 f0.4  10101 f10.14  10101 f15.19  10101 f20.24  10101 f25.29  \\\n",
       "0         2400          2347          2277          1820          2152   \n",
       "1         2392          2344          2194          1830          2067   \n",
       "2         2382          2354          2126          1813          1977   \n",
       "3         2357          2351          2084          1777          1913   \n",
       "4         2318          2357          2055          1734          1869   \n",
       "5         2280          2360          2060          1656          1869   \n",
       "6         2242          2327          2054          1555          1905   \n",
       "7         2171          2338          2051          1480          1912   \n",
       "8         2132          2348          2039          1450          1892   \n",
       "9         2107          2335          2029          1452          1848   \n",
       "10        2077          2259          2051          1467          1777   \n",
       "\n",
       "    10101 f30.34  10101 f35.39  10101 f40.44  10101 f45.49  10101 f5.9  ...  \\\n",
       "0           2315          2185          2146          1859        2495  ...   \n",
       "1           2339          2204          2139          1954        2480  ...   \n",
       "2           2329          2225          2139          2037        2451  ...   \n",
       "3           2316          2232          2158          2079        2418  ...   \n",
       "4           2272          2256          2165          2111        2403  ...   \n",
       "5           2218          2290          2186          2150        2400  ...   \n",
       "6           2104          2340          2208          2153        2393  ...   \n",
       "7           2054          2403          2171          2153        2400  ...   \n",
       "8           2005          2395          2217          2195        2400  ...   \n",
       "9           1991          2363          2251          2201        2393  ...   \n",
       "10          1997          2263          2282          2203        2302  ...   \n",
       "\n",
       "    80109 m45.49  80109 m5.9  80109 m50.54  80109 m55.59  80109 m60.64  \\\n",
       "0           1219        1002          1226           920           784   \n",
       "1           1257        1009          1209           931           794   \n",
       "2           1285         993          1154           963           792   \n",
       "3           1284         988          1123           998           778   \n",
       "4           1277         998          1105          1020           819   \n",
       "5           1244        1015          1112          1051           833   \n",
       "6           1188         985          1117          1086           835   \n",
       "7           1157         975          1127          1083           847   \n",
       "8           1149         975          1123          1051           851   \n",
       "9           1122         967          1163           997           892   \n",
       "10          1100         945          1200           996           898   \n",
       "\n",
       "    80109 m65.69  80109 m70.74  80109 m75.79  80109 m80.84  80109 m85.  \n",
       "0            606           328           180            98          50  \n",
       "1            617           376           200           104          55  \n",
       "2            634           413           209           105          59  \n",
       "3            649           459           212           112          65  \n",
       "4            648           486           247           116          69  \n",
       "5            672           513           277           124          74  \n",
       "6            676           527           316           123          79  \n",
       "7            690           572           358           142          88  \n",
       "8            694           598           389           163          95  \n",
       "9            717           577           439           190         103  \n",
       "10           676           596           457           221         119  \n",
       "\n",
       "[11 rows x 11700 columns]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(9, 2, 11700), dtype=float32, numpy=\n",
       "array([[[2400., 2347., 2277., ...,  180.,   98.,   50.],\n",
       "        [2392., 2344., 2194., ...,  200.,  104.,   55.]],\n",
       "\n",
       "       [[2392., 2344., 2194., ...,  200.,  104.,   55.],\n",
       "        [2382., 2354., 2126., ...,  209.,  105.,   59.]],\n",
       "\n",
       "       [[2382., 2354., 2126., ...,  209.,  105.,   59.],\n",
       "        [2357., 2351., 2084., ...,  212.,  112.,   65.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[2242., 2327., 2054., ...,  316.,  123.,   79.],\n",
       "        [2171., 2338., 2051., ...,  358.,  142.,   88.]],\n",
       "\n",
       "       [[2171., 2338., 2051., ...,  358.,  142.,   88.],\n",
       "        [2132., 2348., 2039., ...,  389.,  163.,   95.]],\n",
       "\n",
       "       [[2132., 2348., 2039., ...,  389.,  163.,   95.],\n",
       "        [2107., 2335., 2029., ...,  439.,  190.,  103.]]], dtype=float32)>"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_train_inputs #pairs from 1991-2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2, 11700), dtype=float32, numpy=\n",
       "array([[[2107., 2335., 2029., ...,  439.,  190.,  103.],\n",
       "        [2077., 2259., 2051., ...,  457.,  221.,  119.]],\n",
       "\n",
       "       [[2077., 2259., 2051., ...,  457.,  221.,  119.],\n",
       "        [2068., 2272., 2072., ...,  452.,  253.,  128.]]], dtype=float32)>"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2000-2001 input for predicting 2002\n",
    "input_2002 = tf.stack([full_train_labels[7,0,:], full_train_labels[8,0,:]],0)\n",
    "#2001-2002 input for predicting 2003\n",
    "input_2003 = tf.stack([full_train_labels[8,0,:], full_val_inputs[0,0,:]],0)\n",
    "#2000-2001 and 2001-2002 inputs as tensor\n",
    "input_2002_2003 = tf.stack([input_2002,input_2003],0)\n",
    "\n",
    "input_2002_2003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10101 f0.4</th>\n",
       "      <th>10101 f10.14</th>\n",
       "      <th>10101 f15.19</th>\n",
       "      <th>10101 f20.24</th>\n",
       "      <th>10101 f25.29</th>\n",
       "      <th>10101 f30.34</th>\n",
       "      <th>10101 f35.39</th>\n",
       "      <th>10101 f40.44</th>\n",
       "      <th>10101 f45.49</th>\n",
       "      <th>10101 f5.9</th>\n",
       "      <th>...</th>\n",
       "      <th>80109 m45.49</th>\n",
       "      <th>80109 m5.9</th>\n",
       "      <th>80109 m50.54</th>\n",
       "      <th>80109 m55.59</th>\n",
       "      <th>80109 m60.64</th>\n",
       "      <th>80109 m65.69</th>\n",
       "      <th>80109 m70.74</th>\n",
       "      <th>80109 m75.79</th>\n",
       "      <th>80109 m80.84</th>\n",
       "      <th>80109 m85.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2068</td>\n",
       "      <td>2272</td>\n",
       "      <td>2072</td>\n",
       "      <td>1450</td>\n",
       "      <td>1682</td>\n",
       "      <td>2080</td>\n",
       "      <td>2189</td>\n",
       "      <td>2323</td>\n",
       "      <td>2239</td>\n",
       "      <td>2319</td>\n",
       "      <td>...</td>\n",
       "      <td>1114</td>\n",
       "      <td>984</td>\n",
       "      <td>1132</td>\n",
       "      <td>986</td>\n",
       "      <td>960</td>\n",
       "      <td>692</td>\n",
       "      <td>590</td>\n",
       "      <td>452</td>\n",
       "      <td>253</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2033</td>\n",
       "      <td>2317</td>\n",
       "      <td>2078</td>\n",
       "      <td>1466</td>\n",
       "      <td>1622</td>\n",
       "      <td>2116</td>\n",
       "      <td>2172</td>\n",
       "      <td>2394</td>\n",
       "      <td>2217</td>\n",
       "      <td>2299</td>\n",
       "      <td>...</td>\n",
       "      <td>1115</td>\n",
       "      <td>990</td>\n",
       "      <td>1078</td>\n",
       "      <td>1003</td>\n",
       "      <td>920</td>\n",
       "      <td>713</td>\n",
       "      <td>601</td>\n",
       "      <td>454</td>\n",
       "      <td>285</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2035</td>\n",
       "      <td>2372</td>\n",
       "      <td>2058</td>\n",
       "      <td>1481</td>\n",
       "      <td>1585</td>\n",
       "      <td>2089</td>\n",
       "      <td>2152</td>\n",
       "      <td>2419</td>\n",
       "      <td>2220</td>\n",
       "      <td>2256</td>\n",
       "      <td>...</td>\n",
       "      <td>1136</td>\n",
       "      <td>978</td>\n",
       "      <td>1065</td>\n",
       "      <td>993</td>\n",
       "      <td>887</td>\n",
       "      <td>728</td>\n",
       "      <td>584</td>\n",
       "      <td>476</td>\n",
       "      <td>298</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2038</td>\n",
       "      <td>2406</td>\n",
       "      <td>2052</td>\n",
       "      <td>1520</td>\n",
       "      <td>1571</td>\n",
       "      <td>2025</td>\n",
       "      <td>2171</td>\n",
       "      <td>2403</td>\n",
       "      <td>2240</td>\n",
       "      <td>2216</td>\n",
       "      <td>...</td>\n",
       "      <td>1192</td>\n",
       "      <td>979</td>\n",
       "      <td>1027</td>\n",
       "      <td>1030</td>\n",
       "      <td>828</td>\n",
       "      <td>754</td>\n",
       "      <td>599</td>\n",
       "      <td>471</td>\n",
       "      <td>313</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows  11700 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    10101 f0.4  10101 f10.14  10101 f15.19  10101 f20.24  10101 f25.29  \\\n",
       "11        2068          2272          2072          1450          1682   \n",
       "12        2033          2317          2078          1466          1622   \n",
       "13        2035          2372          2058          1481          1585   \n",
       "14        2038          2406          2052          1520          1571   \n",
       "\n",
       "    10101 f30.34  10101 f35.39  10101 f40.44  10101 f45.49  10101 f5.9  ...  \\\n",
       "11          2080          2189          2323          2239        2319  ...   \n",
       "12          2116          2172          2394          2217        2299  ...   \n",
       "13          2089          2152          2419          2220        2256  ...   \n",
       "14          2025          2171          2403          2240        2216  ...   \n",
       "\n",
       "    80109 m45.49  80109 m5.9  80109 m50.54  80109 m55.59  80109 m60.64  \\\n",
       "11          1114         984          1132           986           960   \n",
       "12          1115         990          1078          1003           920   \n",
       "13          1136         978          1065           993           887   \n",
       "14          1192         979          1027          1030           828   \n",
       "\n",
       "    80109 m65.69  80109 m70.74  80109 m75.79  80109 m80.84  80109 m85.  \n",
       "11           692           590           452           253         128  \n",
       "12           713           601           454           285         127  \n",
       "13           728           584           476           298         129  \n",
       "14           754           599           471           313         156  \n",
       "\n",
       "[4 rows x 11700 columns]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2, 11700), dtype=float32, numpy=\n",
       "array([[[2068., 2272., 2072., ...,  452.,  253.,  128.],\n",
       "        [2033., 2317., 2078., ...,  454.,  285.,  127.]],\n",
       "\n",
       "       [[2033., 2317., 2078., ...,  454.,  285.,  127.],\n",
       "        [2035., 2372., 2058., ...,  476.,  298.,  129.]]], dtype=float32)>"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_val_inputs #pairs from 2002-2004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2, 11700), dtype=float32, numpy=\n",
       "array([[[2035., 2372., 2058., ...,  476.,  298.,  129.],\n",
       "        [2038., 2406., 2052., ...,  471.,  313.,  156.]],\n",
       "\n",
       "       [[2038., 2406., 2052., ...,  471.,  313.,  156.],\n",
       "        [2005., 2428., 2034., ...,  499.,  325.,  195.]]], dtype=float32)>"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2004-2005 input for predicting 2006\n",
    "input_2006 = tf.stack([full_val_labels[0,0,:],full_val_labels[1,0,:]],0)\n",
    "#2005-2006 input for predicting 2007\n",
    "input_2007 = tf.stack([full_val_labels[1,0,:], full_test_inputs[0,0,:]],0)\n",
    "#2004-2005 and 2005-2006 inputs as tensor\n",
    "input_2006_2007 = tf.stack([input_2006,input_2007],0)\n",
    "\n",
    "input_2006_2007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 2, 11700), dtype=float32, numpy=\n",
       "array([[[2005., 2428., 2034., ...,  499.,  325.,  195.],\n",
       "        [2011., 2375., 2112., ...,  480.,  356.,  214.]],\n",
       "\n",
       "       [[2011., 2375., 2112., ...,  480.,  356.,  214.],\n",
       "        [2044., 2343., 2145., ...,  484.,  361.,  234.]],\n",
       "\n",
       "       [[2044., 2343., 2145., ...,  484.,  361.,  234.],\n",
       "        [2109., 2330., 2182., ...,  481.,  363.,  250.]],\n",
       "\n",
       "       [[2109., 2330., 2182., ...,  481.,  363.,  250.],\n",
       "        [2170., 2336., 2230., ...,  492.,  360.,  264.]]], dtype=float32)>"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_test_inputs #pairs from 2006-2010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10101 f0.4</th>\n",
       "      <th>10101 f10.14</th>\n",
       "      <th>10101 f15.19</th>\n",
       "      <th>10101 f20.24</th>\n",
       "      <th>10101 f25.29</th>\n",
       "      <th>10101 f30.34</th>\n",
       "      <th>10101 f35.39</th>\n",
       "      <th>10101 f40.44</th>\n",
       "      <th>10101 f45.49</th>\n",
       "      <th>10101 f5.9</th>\n",
       "      <th>...</th>\n",
       "      <th>80109 m45.49</th>\n",
       "      <th>80109 m5.9</th>\n",
       "      <th>80109 m50.54</th>\n",
       "      <th>80109 m55.59</th>\n",
       "      <th>80109 m60.64</th>\n",
       "      <th>80109 m65.69</th>\n",
       "      <th>80109 m70.74</th>\n",
       "      <th>80109 m75.79</th>\n",
       "      <th>80109 m80.84</th>\n",
       "      <th>80109 m85.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2005</td>\n",
       "      <td>2428</td>\n",
       "      <td>2034</td>\n",
       "      <td>1570</td>\n",
       "      <td>1566</td>\n",
       "      <td>1936</td>\n",
       "      <td>2246</td>\n",
       "      <td>2337</td>\n",
       "      <td>2292</td>\n",
       "      <td>2232</td>\n",
       "      <td>...</td>\n",
       "      <td>1248</td>\n",
       "      <td>992</td>\n",
       "      <td>1030</td>\n",
       "      <td>1031</td>\n",
       "      <td>795</td>\n",
       "      <td>771</td>\n",
       "      <td>589</td>\n",
       "      <td>499</td>\n",
       "      <td>325</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2011</td>\n",
       "      <td>2375</td>\n",
       "      <td>2112</td>\n",
       "      <td>1542</td>\n",
       "      <td>1583</td>\n",
       "      <td>1875</td>\n",
       "      <td>2288</td>\n",
       "      <td>2247</td>\n",
       "      <td>2352</td>\n",
       "      <td>2210</td>\n",
       "      <td>...</td>\n",
       "      <td>1279</td>\n",
       "      <td>962</td>\n",
       "      <td>1042</td>\n",
       "      <td>988</td>\n",
       "      <td>798</td>\n",
       "      <td>818</td>\n",
       "      <td>599</td>\n",
       "      <td>480</td>\n",
       "      <td>356</td>\n",
       "      <td>214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2044</td>\n",
       "      <td>2343</td>\n",
       "      <td>2145</td>\n",
       "      <td>1594</td>\n",
       "      <td>1640</td>\n",
       "      <td>1840</td>\n",
       "      <td>2314</td>\n",
       "      <td>2246</td>\n",
       "      <td>2418</td>\n",
       "      <td>2210</td>\n",
       "      <td>...</td>\n",
       "      <td>1283</td>\n",
       "      <td>954</td>\n",
       "      <td>1068</td>\n",
       "      <td>971</td>\n",
       "      <td>826</td>\n",
       "      <td>783</td>\n",
       "      <td>627</td>\n",
       "      <td>484</td>\n",
       "      <td>361</td>\n",
       "      <td>234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2109</td>\n",
       "      <td>2330</td>\n",
       "      <td>2182</td>\n",
       "      <td>1670</td>\n",
       "      <td>1662</td>\n",
       "      <td>1857</td>\n",
       "      <td>2312</td>\n",
       "      <td>2256</td>\n",
       "      <td>2463</td>\n",
       "      <td>2213</td>\n",
       "      <td>...</td>\n",
       "      <td>1296</td>\n",
       "      <td>968</td>\n",
       "      <td>1082</td>\n",
       "      <td>975</td>\n",
       "      <td>852</td>\n",
       "      <td>768</td>\n",
       "      <td>643</td>\n",
       "      <td>481</td>\n",
       "      <td>363</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2170</td>\n",
       "      <td>2336</td>\n",
       "      <td>2230</td>\n",
       "      <td>1722</td>\n",
       "      <td>1691</td>\n",
       "      <td>1883</td>\n",
       "      <td>2274</td>\n",
       "      <td>2328</td>\n",
       "      <td>2474</td>\n",
       "      <td>2217</td>\n",
       "      <td>...</td>\n",
       "      <td>1262</td>\n",
       "      <td>983</td>\n",
       "      <td>1129</td>\n",
       "      <td>962</td>\n",
       "      <td>893</td>\n",
       "      <td>741</td>\n",
       "      <td>674</td>\n",
       "      <td>492</td>\n",
       "      <td>360</td>\n",
       "      <td>264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2225</td>\n",
       "      <td>2381</td>\n",
       "      <td>2178</td>\n",
       "      <td>1698</td>\n",
       "      <td>1681</td>\n",
       "      <td>1864</td>\n",
       "      <td>2252</td>\n",
       "      <td>2426</td>\n",
       "      <td>2430</td>\n",
       "      <td>2217</td>\n",
       "      <td>...</td>\n",
       "      <td>1201</td>\n",
       "      <td>974</td>\n",
       "      <td>1194</td>\n",
       "      <td>927</td>\n",
       "      <td>867</td>\n",
       "      <td>756</td>\n",
       "      <td>716</td>\n",
       "      <td>505</td>\n",
       "      <td>414</td>\n",
       "      <td>282</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows  11700 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    10101 f0.4  10101 f10.14  10101 f15.19  10101 f20.24  10101 f25.29  \\\n",
       "15        2005          2428          2034          1570          1566   \n",
       "16        2011          2375          2112          1542          1583   \n",
       "17        2044          2343          2145          1594          1640   \n",
       "18        2109          2330          2182          1670          1662   \n",
       "19        2170          2336          2230          1722          1691   \n",
       "20        2225          2381          2178          1698          1681   \n",
       "\n",
       "    10101 f30.34  10101 f35.39  10101 f40.44  10101 f45.49  10101 f5.9  ...  \\\n",
       "15          1936          2246          2337          2292        2232  ...   \n",
       "16          1875          2288          2247          2352        2210  ...   \n",
       "17          1840          2314          2246          2418        2210  ...   \n",
       "18          1857          2312          2256          2463        2213  ...   \n",
       "19          1883          2274          2328          2474        2217  ...   \n",
       "20          1864          2252          2426          2430        2217  ...   \n",
       "\n",
       "    80109 m45.49  80109 m5.9  80109 m50.54  80109 m55.59  80109 m60.64  \\\n",
       "15          1248         992          1030          1031           795   \n",
       "16          1279         962          1042           988           798   \n",
       "17          1283         954          1068           971           826   \n",
       "18          1296         968          1082           975           852   \n",
       "19          1262         983          1129           962           893   \n",
       "20          1201         974          1194           927           867   \n",
       "\n",
       "    80109 m65.69  80109 m70.74  80109 m75.79  80109 m80.84  80109 m85.  \n",
       "15           771           589           499           325         195  \n",
       "16           818           599           480           356         214  \n",
       "17           783           627           484           361         234  \n",
       "18           768           643           481           363         250  \n",
       "19           741           674           492           360         264  \n",
       "20           756           716           505           414         282  \n",
       "\n",
       "[6 rows x 11700 columns]"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all-in-one input\n",
    "\n",
    "all_input = tf.concat([full_train_inputs,input_2002_2003,full_val_inputs,input_2006_2007,full_test_inputs],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(19, 1, 11700), dtype=float32, numpy=\n",
       "array([[[2164.401   , 2269.98    , 2002.4312  , ...,  315.13034 ,\n",
       "          140.55989 ,   81.423744]],\n",
       "\n",
       "       [[2177.8188  , 2284.0527  , 2014.845   , ...,  317.08392 ,\n",
       "          141.43126 ,   81.92846 ]],\n",
       "\n",
       "       [[2192.166   , 2299.0999  , 2028.1185  , ...,  319.17285 ,\n",
       "          142.36296 ,   82.46813 ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[2526.04    , 2649.2625  , 2337.0085  , ...,  367.78357 ,\n",
       "          164.04471 ,   95.02684 ]],\n",
       "\n",
       "       [[2578.0527  , 2703.8127  , 2385.1292  , ...,  375.3564  ,\n",
       "          167.4224  ,   96.98331 ]],\n",
       "\n",
       "       [[2617.6406  , 2745.332   , 2421.7546  , ...,  381.12024 ,\n",
       "          169.99323 ,   98.47241 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predictions for years 1993-2011\n",
    "\n",
    "result = full_model(all_input) #1993-2011\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['10101 f0.4', '10101 f10.14', '10101 f15.19', '10101 f20.24',\n",
       "       '10101 f25.29', '10101 f30.34', '10101 f35.39', '10101 f40.44',\n",
       "       '10101 f45.49', '10101 f5.9',\n",
       "       ...\n",
       "       '80109 m45.49', '80109 m5.9', '80109 m50.54', '80109 m55.59',\n",
       "       '80109 m60.64', '80109 m65.69', '80109 m70.74', '80109 m75.79',\n",
       "       '80109 m80.84', '80109 m85.'],\n",
       "      dtype='object', length=11700)"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "Code = []\n",
    "Sex = []\n",
    "Age = []\n",
    "for sets in test_df.columns:\n",
    "    code = sets.split()[0]\n",
    "    sex = sets.split()[1][0]\n",
    "    age = sets.split()[1][1:]\n",
    "    Code.append(code)\n",
    "    Sex.append(sex)\n",
    "    Age.append(age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2324.297    2437.677    2150.362    ...  338.41058   150.94353\n",
      "    87.438255]]\n",
      "[[2345.879   2460.3118  2170.329   ...  341.55283  152.34506   88.25006]]\n",
      "[[2367.934    2483.443    2190.7336   ...  344.76398   153.77731\n",
      "    89.079666]]\n",
      "[[2387.8474   2504.3276   2209.1567   ...  347.66324   155.07048\n",
      "    89.828705]]\n",
      "[[2411.9263   2529.5815   2231.434    ...  351.16907   156.63417\n",
      "    90.734436]]\n",
      "[[2439.6074   2558.613    2257.0437   ...  355.1993    158.43176\n",
      "    91.775665]]\n",
      "[[2479.6355   2600.594    2294.0767   ...  361.02725   161.03119\n",
      "    93.281334]]\n",
      "[[2526.04    2649.2625  2337.0085  ...  367.78357  164.04471   95.02684]]\n",
      "[[2578.0527  2703.8127  2385.1292  ...  375.3564   167.4224    96.98331]]\n",
      "[[2617.6406  2745.332   2421.7546  ...  381.12024  169.99323   98.47241]]\n"
     ]
    }
   ],
   "source": [
    "year2002_2011 = result[-10:]\n",
    "year2002_2011 = year2002_2011.numpy()\n",
    "final_result = []\n",
    "final_result.append(Code)\n",
    "final_result.append(Sex)\n",
    "final_result.append(Age)\n",
    "for year in year2002_2011:\n",
    "    print(year)\n",
    "    final_result.append(year[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Code</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>2002</th>\n",
       "      <th>2003</th>\n",
       "      <th>2004</th>\n",
       "      <th>2005</th>\n",
       "      <th>2006</th>\n",
       "      <th>2007</th>\n",
       "      <th>2008</th>\n",
       "      <th>2009</th>\n",
       "      <th>2010</th>\n",
       "      <th>2011</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10101</td>\n",
       "      <td>f</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2324.297119</td>\n",
       "      <td>2345.878906</td>\n",
       "      <td>2367.934082</td>\n",
       "      <td>2387.847412</td>\n",
       "      <td>2411.92627</td>\n",
       "      <td>2439.607422</td>\n",
       "      <td>2479.635498</td>\n",
       "      <td>2526.040039</td>\n",
       "      <td>2578.052734</td>\n",
       "      <td>2617.640625</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10101</td>\n",
       "      <td>f</td>\n",
       "      <td>10.14</td>\n",
       "      <td>2437.677002</td>\n",
       "      <td>2460.311768</td>\n",
       "      <td>2483.443115</td>\n",
       "      <td>2504.327637</td>\n",
       "      <td>2529.581543</td>\n",
       "      <td>2558.613037</td>\n",
       "      <td>2600.593994</td>\n",
       "      <td>2649.262451</td>\n",
       "      <td>2703.812744</td>\n",
       "      <td>2745.332031</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10101</td>\n",
       "      <td>f</td>\n",
       "      <td>15.19</td>\n",
       "      <td>2150.362061</td>\n",
       "      <td>2170.329102</td>\n",
       "      <td>2190.733643</td>\n",
       "      <td>2209.156738</td>\n",
       "      <td>2231.434082</td>\n",
       "      <td>2257.043701</td>\n",
       "      <td>2294.07666</td>\n",
       "      <td>2337.008545</td>\n",
       "      <td>2385.12915</td>\n",
       "      <td>2421.754639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10101</td>\n",
       "      <td>f</td>\n",
       "      <td>20.24</td>\n",
       "      <td>1665.245605</td>\n",
       "      <td>1680.707886</td>\n",
       "      <td>1696.509399</td>\n",
       "      <td>1710.776245</td>\n",
       "      <td>1728.02771</td>\n",
       "      <td>1747.859863</td>\n",
       "      <td>1776.538086</td>\n",
       "      <td>1809.78479</td>\n",
       "      <td>1847.049316</td>\n",
       "      <td>1875.412109</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10101</td>\n",
       "      <td>f</td>\n",
       "      <td>25.29</td>\n",
       "      <td>1965.959595</td>\n",
       "      <td>1984.214233</td>\n",
       "      <td>2002.869141</td>\n",
       "      <td>2019.712402</td>\n",
       "      <td>2040.079224</td>\n",
       "      <td>2063.49292</td>\n",
       "      <td>2097.350098</td>\n",
       "      <td>2136.600586</td>\n",
       "      <td>2180.594727</td>\n",
       "      <td>2214.079346</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11695</th>\n",
       "      <td>80109</td>\n",
       "      <td>m</td>\n",
       "      <td>65.69</td>\n",
       "      <td>702.57782</td>\n",
       "      <td>709.101501</td>\n",
       "      <td>715.768188</td>\n",
       "      <td>721.787476</td>\n",
       "      <td>729.065979</td>\n",
       "      <td>737.433289</td>\n",
       "      <td>749.532837</td>\n",
       "      <td>763.559814</td>\n",
       "      <td>779.281982</td>\n",
       "      <td>791.248413</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11696</th>\n",
       "      <td>80109</td>\n",
       "      <td>m</td>\n",
       "      <td>70.74</td>\n",
       "      <td>550.874146</td>\n",
       "      <td>555.989197</td>\n",
       "      <td>561.21637</td>\n",
       "      <td>565.935913</td>\n",
       "      <td>571.642761</td>\n",
       "      <td>578.203308</td>\n",
       "      <td>587.690247</td>\n",
       "      <td>598.688354</td>\n",
       "      <td>611.015625</td>\n",
       "      <td>620.398132</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11697</th>\n",
       "      <td>80109</td>\n",
       "      <td>m</td>\n",
       "      <td>75.79</td>\n",
       "      <td>338.410583</td>\n",
       "      <td>341.552826</td>\n",
       "      <td>344.763977</td>\n",
       "      <td>347.663239</td>\n",
       "      <td>351.169067</td>\n",
       "      <td>355.19931</td>\n",
       "      <td>361.027252</td>\n",
       "      <td>367.783569</td>\n",
       "      <td>375.356415</td>\n",
       "      <td>381.120239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11698</th>\n",
       "      <td>80109</td>\n",
       "      <td>m</td>\n",
       "      <td>80.84</td>\n",
       "      <td>150.943527</td>\n",
       "      <td>152.345062</td>\n",
       "      <td>153.777313</td>\n",
       "      <td>155.07048</td>\n",
       "      <td>156.634171</td>\n",
       "      <td>158.431763</td>\n",
       "      <td>161.031189</td>\n",
       "      <td>164.044708</td>\n",
       "      <td>167.422394</td>\n",
       "      <td>169.993225</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11699</th>\n",
       "      <td>80109</td>\n",
       "      <td>m</td>\n",
       "      <td>85.</td>\n",
       "      <td>87.438255</td>\n",
       "      <td>88.250061</td>\n",
       "      <td>89.079666</td>\n",
       "      <td>89.828705</td>\n",
       "      <td>90.734436</td>\n",
       "      <td>91.775665</td>\n",
       "      <td>93.281334</td>\n",
       "      <td>95.02684</td>\n",
       "      <td>96.983307</td>\n",
       "      <td>98.472412</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11700 rows  13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Code Sex    Age         2002         2003         2004         2005  \\\n",
       "0      10101   f    0.4  2324.297119  2345.878906  2367.934082  2387.847412   \n",
       "1      10101   f  10.14  2437.677002  2460.311768  2483.443115  2504.327637   \n",
       "2      10101   f  15.19  2150.362061  2170.329102  2190.733643  2209.156738   \n",
       "3      10101   f  20.24  1665.245605  1680.707886  1696.509399  1710.776245   \n",
       "4      10101   f  25.29  1965.959595  1984.214233  2002.869141  2019.712402   \n",
       "...      ...  ..    ...          ...          ...          ...          ...   \n",
       "11695  80109   m  65.69    702.57782   709.101501   715.768188   721.787476   \n",
       "11696  80109   m  70.74   550.874146   555.989197    561.21637   565.935913   \n",
       "11697  80109   m  75.79   338.410583   341.552826   344.763977   347.663239   \n",
       "11698  80109   m  80.84   150.943527   152.345062   153.777313    155.07048   \n",
       "11699  80109   m    85.    87.438255    88.250061    89.079666    89.828705   \n",
       "\n",
       "              2006         2007         2008         2009         2010  \\\n",
       "0       2411.92627  2439.607422  2479.635498  2526.040039  2578.052734   \n",
       "1      2529.581543  2558.613037  2600.593994  2649.262451  2703.812744   \n",
       "2      2231.434082  2257.043701   2294.07666  2337.008545   2385.12915   \n",
       "3       1728.02771  1747.859863  1776.538086   1809.78479  1847.049316   \n",
       "4      2040.079224   2063.49292  2097.350098  2136.600586  2180.594727   \n",
       "...            ...          ...          ...          ...          ...   \n",
       "11695   729.065979   737.433289   749.532837   763.559814   779.281982   \n",
       "11696   571.642761   578.203308   587.690247   598.688354   611.015625   \n",
       "11697   351.169067    355.19931   361.027252   367.783569   375.356415   \n",
       "11698   156.634171   158.431763   161.031189   164.044708   167.422394   \n",
       "11699    90.734436    91.775665    93.281334     95.02684    96.983307   \n",
       "\n",
       "              2011  \n",
       "0      2617.640625  \n",
       "1      2745.332031  \n",
       "2      2421.754639  \n",
       "3      1875.412109  \n",
       "4      2214.079346  \n",
       "...            ...  \n",
       "11695   791.248413  \n",
       "11696   620.398132  \n",
       "11697   381.120239  \n",
       "11698   169.993225  \n",
       "11699    98.472412  \n",
       "\n",
       "[11700 rows x 13 columns]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_name = ['Code','Sex','Age', '2002','2003','2004','2005','2006','2007','2008','2009','2010','2011']\n",
    "final_df = pd.DataFrame(final_result).T\n",
    "final_df.columns = column_name\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.to_csv(\"predictions_area_cohort_column.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (3919008450.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  Input \u001b[1;32mIn [36]\u001b[1;36m\u001b[0m\n\u001b[1;33m    final_df.rename(column:{})\u001b[0m\n\u001b[1;37m                          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "final_df.rename(column:{})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "52ee2977380704a66854748a73250e0671a9318bd5b3fd45a3df9f851ae61629"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
