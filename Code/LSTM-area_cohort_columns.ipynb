{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import IPython\n",
    "import IPython.display\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import keras_tuner as kt\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (8, 6)\n",
    "mpl.rcParams['axes.grid'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new\n",
    "\n",
    "def split_ts_data(data, val_start, test_start):\n",
    "    year_min = min(data['Year'])\n",
    "    year_max = max(data['Year'])\n",
    "    year_range = year_max-year_min\n",
    "    \n",
    "    assert (val_start >= year_min) & (test_start >= year_min) & (val_start <= year_max) & (test_start <= year_max), \"Parameter out of bounds\"\n",
    "    assert (val_start > year_min) & (test_start > year_min), \"Training set is empty.\"\n",
    "    assert val_start < test_start, \"Validation set is empty.\"\n",
    "    assert year_range > 0, \"Data contains less than 2 years.\"\n",
    "    \n",
    "    \n",
    "    train_data = data[(data['Year']<val_start) & (data['Year']<test_start)]\n",
    "    val_data = data[(data['Year']>=val_start) & (data['Year']<test_start)]\n",
    "    test_data = data[data['Year']>=test_start]\n",
    "    \n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(df, input_width, label_width, shift):\n",
    "    def create_window(tensor):\n",
    "        #input -> length of time series used for training\n",
    "        #shift -> how far off prediction is from last input\n",
    "        #label -> points to predict\n",
    "        total_window_size = input_width + shift\n",
    "        label_start = total_window_size - label_width\n",
    "\n",
    "        input_bounds = slice(0, input_width)\n",
    "        label_bounds = slice(label_start, None)\n",
    "\n",
    "        inputs = tensor[:,input_bounds,:]\n",
    "        labels = tensor[:,label_bounds,:]\n",
    "\n",
    "        inputs.set_shape([None, input_width, None])\n",
    "        labels.set_shape([None, label_width, None])\n",
    "\n",
    "        return inputs, labels\n",
    "    \n",
    "    total_window_size = input_width + shift\n",
    "    \n",
    "    arr = np.array(df, dtype=np.float32)\n",
    "    ds = tf.keras.utils.timeseries_dataset_from_array(\n",
    "      data=arr,\n",
    "      targets=None,\n",
    "      sequence_length=total_window_size,\n",
    "      sequence_stride=1,\n",
    "      shuffle=False,\n",
    "      batch_size=32,)\n",
    "    \n",
    "    ds = ds.map(create_window)\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_and_fit(model, epochs, input_optimizer='adam', input_loss='mse'):\n",
    "    model.compile(optimizer=input_optimizer, loss=input_loss)\n",
    "    history = model.fit(x=train_ds, epochs=epochs, validation_data=val_ds)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def col_dict(np_df):\n",
    "    return_dict = {col:index for index, col in enumerate(np_df.columns)}\n",
    "    \n",
    "    return return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(df, ds, input_width, label_width, shift, model=None, plot_col='10101 m0.4', max_subplots=3):\n",
    "    #ensure that df and ds match e.g. train_df must be accompanied by train_ds\n",
    "    col_indices = col_dict(df)\n",
    "    \n",
    "    total_window_size = label_width + shift\n",
    "    input_slice = slice(0,input_width)\n",
    "    input_indices = np.arange(total_window_size)[input_slice]\n",
    "    label_start = total_window_size - label_width\n",
    "    labels_slice = slice(label_start, None)\n",
    "    label_indices = np.arange(total_window_size)[labels_slice]\n",
    "    \n",
    "    inputs = next(iter(ds))[0]\n",
    "    labels = next(iter(ds))[1]\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plot_col_index = col_indices[plot_col] \n",
    "    max_n = min(max_subplots, len(inputs))\n",
    "    \n",
    "    for n in range(max_n):\n",
    "        plt.subplot(max_n, 1, n+1)\n",
    "        plt.ylabel(plot_col)\n",
    "        plt.plot(input_indices, inputs[n, :, plot_col_index],\n",
    "             label='Inputs', marker='.', zorder=-10)\n",
    "        \n",
    "        plt.scatter(label_indices, labels[n, :, plot_col_index],\n",
    "                edgecolors='k', label='Labels', c='#2ca02c', s=64)\n",
    "        \n",
    "        if model is not None:\n",
    "          predictions = model(inputs)\n",
    "          plt.scatter(label_indices, predictions[n, :, plot_col_index],\n",
    "                  marker='X', edgecolors='k', label='Predictions',\n",
    "                  c='#ff7f0e', s=64)\n",
    "            \n",
    "        if n == 0:\n",
    "          plt.legend()\n",
    "        \n",
    "    plt.xlabel('Year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Main code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edit parameters here, but do not rename variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Read, preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('newSA3.csv')\n",
    "\n",
    "\n",
    "\n",
    "#Parameters\n",
    "validation_start = 2002\n",
    "test_start = 2006\n",
    "#\n",
    "\n",
    "\n",
    "\n",
    "train_df, val_df, test_df = split_ts_data(raw_data, validation_start, test_start)\n",
    "\n",
    "train_df = train_df[train_df.columns.difference([\"Unnamed: 0\",\"Year\"])]\n",
    "val_df = val_df[val_df.columns.difference([\"Unnamed: 0\",\"Year\"])]\n",
    "test_df = test_df[test_df.columns.difference([\"Unnamed: 0\",\"Year\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "input_width = 2 #data used in prediction\n",
    "label_width = 1 #points to predict\n",
    "shift = 1 #how many years away is the last point to predict\n",
    "#\n",
    "\n",
    "\n",
    "\n",
    "train_ds = make_dataset(train_df, input_width, label_width, shift)\n",
    "val_ds = make_dataset(val_df, input_width, label_width, shift)\n",
    "test_ds = make_dataset(test_df, input_width, label_width, shift)\n",
    "\n",
    "num_cols = next(iter(train_ds))[0].shape[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SACohortModel(kt.HyperModel):\n",
    "    def build(self,hp):\n",
    "        #### Hyperparameters\n",
    "        # add hyperparameters as needed when adding layers\n",
    "        \n",
    "        ##layer hyperparameters\n",
    "        hp_lstm1_units = hp.Choice('units',[10,30,50])\n",
    "        hp_lstm1_act = hp.Choice('activation', [\"relu\"])\n",
    "\n",
    "        ##model hyperparameters -> adjust tf.keras.models type and model.add layers\n",
    "        model = tf.keras.models.Sequential()\n",
    "        model.add(tf.keras.layers.LSTM(units = hp_lstm1_units, \n",
    "                                       activation=hp_lstm1_act, \n",
    "                                       return_sequences=False))\n",
    "        \n",
    "        model.add(tf.keras.layers.Dense(label_width * num_cols))\n",
    "        model.add(tf.keras.layers.Reshape([label_width,num_cols]))\n",
    "        \n",
    "        ##compilation hyperparameters\n",
    "        hp_epochs = hp.Choice(\"epochs\",[10,20,30])\n",
    "        hp_input_optimizer = hp.Choice('input_optimizer',[\"adam\", \"adadelta\"])\n",
    "        loss_fun = \"mse\"\n",
    "        \n",
    "        ####\n",
    "        \n",
    "        #Do not edit\n",
    "        model.compile(loss = loss_fun)\n",
    "        \n",
    "        return model\n",
    "        #Do not edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project .\\untitled_project\\oracle.json\n",
      "INFO:tensorflow:Reloading Tuner from .\\untitled_project\\tuner0.json\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "#Parameter\n",
    "num_epochs = 10\n",
    "#\n",
    "\n",
    "train_inputs = next(iter(train_ds))[0]\n",
    "train_labels = next(iter(train_ds))[1]\n",
    "\n",
    "val_inputs = next(iter(val_ds))[0]\n",
    "val_labels = next(iter(val_ds))[1]\n",
    "\n",
    "test_inputs = next(iter(test_ds))[0]\n",
    "\n",
    "tuner = kt.RandomSearch(\n",
    "    SACohortModel(),\n",
    "    objective='val_loss',\n",
    "    max_trials=5)\n",
    "\n",
    "tuner.search(train_inputs, train_labels, epochs = num_epochs, validation_data = (train_inputs, train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'units': 30, 'activation': 'sigmoid', 'epochs': 20, 'input_optimizer': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "for model in tuner.get_best_hyperparameters():\n",
    "    print(model.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model with above parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1/1 [==============================] - 5s 5s/step - loss: 4162026.7500 - val_loss: 4686760.0000\n",
      "Epoch 2/20\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 4133157.2500 - val_loss: 4659462.0000\n",
      "Epoch 3/20\n",
      "1/1 [==============================] - 0s 127ms/step - loss: 4111223.0000 - val_loss: 4623453.0000\n",
      "Epoch 4/20\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 4081430.2500 - val_loss: 4532918.0000\n",
      "Epoch 5/20\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 4005416.5000 - val_loss: 4414479.0000\n",
      "Epoch 6/20\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 3905773.2500 - val_loss: 4290786.0000\n",
      "Epoch 7/20\n",
      "1/1 [==============================] - 0s 119ms/step - loss: 4048850.7500 - val_loss: 4187228.0000\n",
      "Epoch 8/20\n",
      "1/1 [==============================] - 0s 121ms/step - loss: 3766806.2500 - val_loss: 4136708.5000\n",
      "Epoch 9/20\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 3672241.0000 - val_loss: 4052012.5000\n",
      "Epoch 10/20\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 3600553.2500 - val_loss: 3900944.5000\n",
      "Epoch 11/20\n",
      "1/1 [==============================] - 0s 122ms/step - loss: 3471337.7500 - val_loss: 3687656.0000\n",
      "Epoch 12/20\n",
      "1/1 [==============================] - 0s 152ms/step - loss: 3218129.0000 - val_loss: 3285047.0000\n",
      "Epoch 13/20\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 2924061.0000 - val_loss: 3073989.5000\n",
      "Epoch 14/20\n",
      "1/1 [==============================] - 0s 116ms/step - loss: 2736673.2500 - val_loss: 2797510.0000\n",
      "Epoch 15/20\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 2500044.0000 - val_loss: 2528640.5000\n",
      "Epoch 16/20\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 2273763.0000 - val_loss: 2290990.5000\n",
      "Epoch 17/20\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 2072491.7500 - val_loss: 2057810.6250\n",
      "Epoch 18/20\n",
      "1/1 [==============================] - 0s 117ms/step - loss: 1868409.3750 - val_loss: 1826039.1250\n",
      "Epoch 19/20\n",
      "1/1 [==============================] - 0s 107ms/step - loss: 1656572.8750 - val_loss: 1630331.0000\n",
      "Epoch 20/20\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 1468236.1250 - val_loss: 1474468.2500\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e93a0cdc70>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_optimizer = 'adam'\n",
    "loss_fun = 'mse'\n",
    "\n",
    "full_model = tf.keras.models.Sequential()\n",
    "full_model.add(tf.keras.layers.LSTM(units = 30, \n",
    "                                       activation=\"relu\", \n",
    "                                       return_sequences=False))\n",
    "        \n",
    "full_model.add(tf.keras.layers.Dense(label_width * num_cols))\n",
    "full_model.add(tf.keras.layers.Reshape([label_width,num_cols]))\n",
    "compile_and_fit(full_model, epochs=20, input_optimizer=model_optimizer, input_loss=loss_fun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store full model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1991-2001\n",
    "\n",
    "full_train_inputs = next(iter(train_ds))[0] #pairs from 1991-2000\n",
    "full_train_labels = next(iter(train_ds))[1] #1993-2001\n",
    "full_train_predictions = full_model(train_inputs) #1993-2001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2002-2005\n",
    "\n",
    "full_val_inputs = next(iter(val_ds))[0] #pairs 2002,2003 and 2003,2004\n",
    "full_val_labels = next(iter(val_ds))[1] #2004 and 2005\n",
    "full_val_predictions = full_model(val_inputs) #2004 and 2005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_test_inputs = next(iter(test_ds))[0] #pairs from 2006-2010\n",
    "full_test_labels = next(iter(test_ds))[1] #2008-2011\n",
    "full_test_predictions = full_model(test_inputs) #2008-2011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_inputs #pairs from 1991-2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2000-2001 input for predicting 2002\n",
    "input_2002 = tf.stack([full_train_labels[7,0,:], full_train_labels[8,0,:]],0)\n",
    "#2001-2002 input for predicting 2003\n",
    "input_2003 = tf.stack([full_train_labels[8,0,:], full_val_inputs[0,0,:]],0)\n",
    "#2000-2001 and 2001-2002 inputs as tensor\n",
    "input_2002_2003 = tf.stack([input_2002,input_2003],0)\n",
    "\n",
    "input_2002_2003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_val_inputs #pairs from 2002-2004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2004-2005 input for predicting 2006\n",
    "input_2006 = tf.stack([full_val_labels[0,0,:],full_val_labels[1,0,:]],0)\n",
    "#2005-2006 input for predicting 2007\n",
    "input_2007 = tf.stack([full_val_labels[1,0,:], full_test_inputs[0,0,:]],0)\n",
    "#2004-2005 and 2005-2006 inputs as tensor\n",
    "input_2006_2007 = tf.stack([input_2006,input_2007],0)\n",
    "\n",
    "input_2006_2007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_test_inputs #pairs from 2006-2010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all-in-one input\n",
    "\n",
    "all_input = tf.concat([full_train_inputs,input_2002_2003,full_val_inputs,input_2006_2007,full_test_inputs],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(19, 1, 11700), dtype=float32, numpy=\n",
       "array([[[2509.1895  , 3198.7053  ,  567.51215 , ...,  453.19766 ,\n",
       "          268.1739  ,  111.13661 ]],\n",
       "\n",
       "       [[2547.102   , 3238.1907  ,  571.01245 , ...,  460.8364  ,\n",
       "          271.96707 ,  115.925186]],\n",
       "\n",
       "       [[2582.126   , 3276.721   ,  575.42413 , ...,  467.70932 ,\n",
       "          275.53125 ,  119.62692 ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[3182.2969  , 4005.0186  ,  690.4095  , ...,  587.13934 ,\n",
       "          331.17096 ,  168.4201  ]],\n",
       "\n",
       "       [[3246.9346  , 4090.7864  ,  707.01306 , ...,  599.6581  ,\n",
       "          337.07806 ,  171.4721  ]],\n",
       "\n",
       "       [[3307.771   , 4165.9683  ,  719.47217 , ...,  612.18604 ,\n",
       "          342.23685 ,  176.59187 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predictions for years 1993-2011\n",
    "\n",
    "full_model(all_input) #1993-2011"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "52ee2977380704a66854748a73250e0671a9318bd5b3fd45a3df9f851ae61629"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
