{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import IPython\n",
    "import IPython.display\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import keras_tuner as kt\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (8, 6)\n",
    "mpl.rcParams['axes.grid'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new\n",
    "\n",
    "def split_ts_data(data, val_start, test_start):\n",
    "    year_min = min(data['Year'])\n",
    "    year_max = max(data['Year'])\n",
    "    year_range = year_max-year_min\n",
    "    \n",
    "    assert (val_start >= year_min) & (test_start >= year_min) & (val_start <= year_max) & (test_start <= year_max), \"Parameter out of bounds\"\n",
    "    assert (val_start > year_min) & (test_start > year_min), \"Training set is empty.\"\n",
    "    assert val_start < test_start, \"Validation set is empty.\"\n",
    "    assert year_range > 0, \"Data contains less than 2 years.\"\n",
    "    \n",
    "    \n",
    "    train_data = data[(data['Year']<val_start) & (data['Year']<test_start)]\n",
    "    val_data = data[(data['Year']>=val_start) & (data['Year']<test_start)]\n",
    "    test_data = data[data['Year']>=test_start]\n",
    "    \n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(df, input_width, label_width, shift):\n",
    "    def create_window(tensor):\n",
    "        #input -> length of time series used for training\n",
    "        #shift -> how far off prediction is from last input\n",
    "        #label -> points to predict\n",
    "        total_window_size = input_width + shift\n",
    "        label_start = total_window_size - label_width\n",
    "\n",
    "        input_bounds = slice(0, input_width)\n",
    "        label_bounds = slice(label_start, None)\n",
    "\n",
    "        inputs = tensor[:,input_bounds,:]\n",
    "        labels = tensor[:,label_bounds,:]\n",
    "\n",
    "        inputs.set_shape([None, input_width, None])\n",
    "        labels.set_shape([None, label_width, None])\n",
    "\n",
    "        return inputs, labels\n",
    "    \n",
    "    total_window_size = input_width + shift\n",
    "    \n",
    "    arr = np.array(df, dtype=np.float32)\n",
    "    ds = tf.keras.utils.timeseries_dataset_from_array(\n",
    "      data=arr,\n",
    "      targets=None,\n",
    "      sequence_length=total_window_size,\n",
    "      sequence_stride=1,\n",
    "      shuffle=False,\n",
    "      batch_size=32,)\n",
    "    \n",
    "    ds = ds.map(create_window)\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_and_fit(model, epochs, input_optimizer='adam', input_loss='mse'):\n",
    "    model.compile(optimizer=input_optimizer, loss=input_loss)\n",
    "    history = model.fit(x=train_ds, epochs=epochs, validation_data=val_ds)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def col_dict(np_df):\n",
    "    return_dict = {col:index for index, col in enumerate(np_df.columns)}\n",
    "    \n",
    "    return return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(df, ds, input_width, label_width, shift, model=None, plot_col='10101 m0.4', max_subplots=3):\n",
    "    #ensure that df and ds match e.g. train_df must be accompanied by train_ds\n",
    "    col_indices = col_dict(df)\n",
    "    \n",
    "    total_window_size = label_width + shift\n",
    "    input_slice = slice(0,input_width)\n",
    "    input_indices = np.arange(total_window_size)[input_slice]\n",
    "    label_start = total_window_size - label_width\n",
    "    labels_slice = slice(label_start, None)\n",
    "    label_indices = np.arange(total_window_size)[labels_slice]\n",
    "    \n",
    "    inputs = next(iter(ds))[0]\n",
    "    labels = next(iter(ds))[1]\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plot_col_index = col_indices[plot_col] \n",
    "    max_n = min(max_subplots, len(inputs))\n",
    "    \n",
    "    for n in range(max_n):\n",
    "        plt.subplot(max_n, 1, n+1)\n",
    "        plt.ylabel(plot_col)\n",
    "        plt.plot(input_indices, inputs[n, :, plot_col_index],\n",
    "             label='Inputs', marker='.', zorder=-10)\n",
    "        \n",
    "        plt.scatter(label_indices, labels[n, :, plot_col_index],\n",
    "                edgecolors='k', label='Labels', c='#2ca02c', s=64)\n",
    "        \n",
    "        if model is not None:\n",
    "          predictions = model(inputs)\n",
    "          plt.scatter(label_indices, predictions[n, :, plot_col_index],\n",
    "                  marker='X', edgecolors='k', label='Predictions',\n",
    "                  c='#ff7f0e', s=64)\n",
    "            \n",
    "        if n == 0:\n",
    "          plt.legend()\n",
    "        \n",
    "    plt.xlabel('Year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Main code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edit parameters here, but do not rename variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Read, preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('newSA3.csv')\n",
    "\n",
    "\n",
    "\n",
    "#Parameters\n",
    "validation_start = 2002\n",
    "test_start = 2006\n",
    "#\n",
    "\n",
    "\n",
    "\n",
    "train_df, val_df, test_df = split_ts_data(raw_data, validation_start, test_start)\n",
    "\n",
    "train_df = train_df[train_df.columns.difference([\"Unnamed: 0\",\"Year\"])]\n",
    "val_df = val_df[val_df.columns.difference([\"Unnamed: 0\",\"Year\"])]\n",
    "test_df = test_df[test_df.columns.difference([\"Unnamed: 0\",\"Year\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Parameters\n",
    "input_width = 2 #data used in prediction\n",
    "label_width = 1 #points to predict\n",
    "shift = 1 #how many years away is the last point to predict\n",
    "#\n",
    "\n",
    "\n",
    "\n",
    "train_ds = make_dataset(train_df, input_width, label_width, shift)\n",
    "val_ds = make_dataset(val_df, input_width, label_width, shift)\n",
    "test_ds = make_dataset(test_df, input_width, label_width, shift)\n",
    "\n",
    "num_cols = next(iter(train_ds))[0].shape[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SACohortModel(kt.HyperModel):\n",
    "    def build(self,hp):\n",
    "        #### Hyperparameters\n",
    "        # add hyperparameters as needed when adding layers\n",
    "        \n",
    "        ##layer hyperparameters\n",
    "        hp_lstm1_units = hp.Choice('units',[10,30,50])\n",
    "        hp_lstm1_act = hp.Choice('activation', [\"relu\"])\n",
    "\n",
    "        ##model hyperparameters -> adjust tf.keras.models type and model.add layers\n",
    "        model = tf.keras.models.Sequential()\n",
    "        model.add(tf.keras.layers.LSTM(units = hp_lstm1_units, \n",
    "                                       activation=hp_lstm1_act, \n",
    "                                       return_sequences=False))\n",
    "        \n",
    "        model.add(tf.keras.layers.Dense(label_width * num_cols))\n",
    "        model.add(tf.keras.layers.Reshape([label_width,num_cols]))\n",
    "        \n",
    "        ##compilation hyperparameters\n",
    "        hp_epochs = hp.Choice(\"epochs\",[10,20,30])\n",
    "        hp_input_optimizer = hp.Choice('input_optimizer',[\"adam\", \"adadelta\"])\n",
    "        loss_fun = \"mse\"\n",
    "        \n",
    "        ####\n",
    "        \n",
    "        #Do not edit\n",
    "        model.compile(loss = loss_fun)\n",
    "        \n",
    "        return model\n",
    "        #Do not edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project .\\untitled_project\\oracle.json\n",
      "INFO:tensorflow:Reloading Tuner from .\\untitled_project\\tuner0.json\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "#Parameter\n",
    "num_epochs = 10\n",
    "#\n",
    "\n",
    "train_inputs = next(iter(train_ds))[0]\n",
    "train_labels = next(iter(train_ds))[1]\n",
    "\n",
    "val_inputs = next(iter(val_ds))[0]\n",
    "val_labels = next(iter(val_ds))[1]\n",
    "\n",
    "test_inputs = next(iter(test_ds))[0]\n",
    "\n",
    "tuner = kt.RandomSearch(\n",
    "    SACohortModel(),\n",
    "    objective='val_loss',\n",
    "    max_trials=5)\n",
    "\n",
    "tuner.search(train_inputs, train_labels, epochs = num_epochs, validation_data = (train_inputs, train_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'units': 30, 'activation': 'sigmoid', 'epochs': 20, 'input_optimizer': 'adam'}\n"
     ]
    }
   ],
   "source": [
    "for model in tuner.get_best_hyperparameters():\n",
    "    print(model.values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model with above parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "1/1 [==============================] - 4s 4s/step - loss: 4145244.5000 - val_loss: 4665744.0000\n",
      "Epoch 2/20\n",
      "1/1 [==============================] - 0s 112ms/step - loss: 4117641.7500 - val_loss: 4627232.5000\n",
      "Epoch 3/20\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 4080446.7500 - val_loss: 4529316.5000\n",
      "Epoch 4/20\n",
      "1/1 [==============================] - 0s 108ms/step - loss: 3998206.2500 - val_loss: 4323756.5000\n",
      "Epoch 5/20\n",
      "1/1 [==============================] - 0s 103ms/step - loss: 3822377.2500 - val_loss: 4266271.0000\n",
      "Epoch 6/20\n",
      "1/1 [==============================] - 0s 106ms/step - loss: 3778070.7500 - val_loss: 3920618.2500\n",
      "Epoch 7/20\n",
      "1/1 [==============================] - 0s 104ms/step - loss: 3487093.0000 - val_loss: 3594100.5000\n",
      "Epoch 8/20\n",
      "1/1 [==============================] - 0s 105ms/step - loss: 3206489.2500 - val_loss: 3299739.0000\n",
      "Epoch 9/20\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 2950304.5000 - val_loss: 3066981.7500\n",
      "Epoch 10/20\n",
      "1/1 [==============================] - 0s 109ms/step - loss: 2744313.0000 - val_loss: 3398186.0000\n",
      "Epoch 11/20\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 2680795.0000 - val_loss: 3156433.5000\n",
      "Epoch 12/20\n",
      "1/1 [==============================] - 0s 110ms/step - loss: 2815994.5000 - val_loss: 2913033.0000\n",
      "Epoch 13/20\n",
      "1/1 [==============================] - 0s 111ms/step - loss: 2605415.0000 - val_loss: 2663064.5000\n",
      "Epoch 14/20\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 2387197.5000 - val_loss: 2398339.5000\n",
      "Epoch 15/20\n",
      "1/1 [==============================] - 0s 99ms/step - loss: 2152995.2500 - val_loss: 1937349.1250\n",
      "Epoch 16/20\n",
      "1/1 [==============================] - 0s 97ms/step - loss: 1928505.5000 - val_loss: 2065988.0000\n",
      "Epoch 17/20\n",
      "1/1 [==============================] - 0s 96ms/step - loss: 1743686.5000 - val_loss: 2066022.2500\n",
      "Epoch 18/20\n",
      "1/1 [==============================] - 0s 98ms/step - loss: 1799824.5000 - val_loss: 1583336.0000\n",
      "Epoch 19/20\n",
      "1/1 [==============================] - 0s 113ms/step - loss: 1387271.6250 - val_loss: 1152561.2500\n",
      "Epoch 20/20\n",
      "1/1 [==============================] - 0s 101ms/step - loss: 1025970.2500 - val_loss: 1002770.8750\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x2af6e4ae730>"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_optimizer = 'adam'\n",
    "loss_fun = 'mse'\n",
    "\n",
    "full_model = tf.keras.models.Sequential()\n",
    "full_model.add(tf.keras.layers.LSTM(units = 30, \n",
    "                                       activation=\"relu\", \n",
    "                                       return_sequences=False))\n",
    "        \n",
    "full_model.add(tf.keras.layers.Dense(label_width * num_cols))\n",
    "full_model.add(tf.keras.layers.Reshape([label_width,num_cols]))\n",
    "compile_and_fit(full_model, epochs=20, input_optimizer=model_optimizer, input_loss=loss_fun)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store full model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_train_inputs = next(iter(train_ds))[0]\n",
    "full_train_labels = next(iter(train_ds))[1]\n",
    "full_train_predictions = full_model(train_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_val_inputs = next(iter(val_ds))[0]\n",
    "full_val_labels = next(iter(val_ds))[1]\n",
    "full_val_predictions = full_model(val_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_test_inputs = next(iter(test_ds))[0]\n",
    "full_test_labels = next(iter(test_ds))[1]\n",
    "full_test_predictions = full_model(test_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 1, 11700), dtype=float32, numpy=\n",
       "array([[[2469.48    , 3037.8867  , 3543.0867  , ...,  238.8423  ,\n",
       "          -53.6987  ,   89.09791 ]],\n",
       "\n",
       "       [[2519.5098  , 3095.9048  , 3614.2368  , ...,  242.80296 ,\n",
       "          -52.90878 ,   92.05693 ]],\n",
       "\n",
       "       [[2567.0068  , 3154.3003  , 3685.652   , ...,  247.47961 ,\n",
       "          -53.479965,   94.098206]],\n",
       "\n",
       "       [[2604.481   , 3206.5945  , 3751.6287  , ...,  252.95528 ,\n",
       "          -56.08786 ,   94.497444]]], dtype=float32)>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2008-2011 predictions\n",
    "\n",
    "full_test_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 11700), dtype=float32, numpy=\n",
       "array([[2604.481   , 3206.5945  , 3751.6287  , ...,  252.95528 ,\n",
       "         -56.08786 ,   94.497444]], dtype=float32)>"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2011 label and prediction\n",
    "\n",
    "full_test_predictions[3,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "52ee2977380704a66854748a73250e0671a9318bd5b3fd45a3df9f851ae61629"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
