Documentation of LSTM Model

================================================================================================================================================================================================================================
================================================================================================================================================================================================================================

1. LSTM_Iterative_Predict_Type1.ipynb

(a) Basic
(i) LSTM Defining
Defines the architecture of the LSTM model, such as the type and number of layers. Do not modify unless changing the architecture.

(ii) LSTM Tuning
Here the possible values for the hyperparameters are defined, such as the range of continuous hyperparameters or enumerations of discrete ones.
These hyperparameters can be modified as needed to change the search space for the tuner.

The build_model function is used as input for keras tuners. Here, the hyperparameter instances are defined then used alongside the model defining function.
This function should only be modified when the architecture of the model changes.

(b) Logic
Create 1-D input(population for age-group) for rolling-predicted LSTM models.
Train two models for two gender (male and female) seperately, then predict year by year. 
After refitting the latest prediction, updatte the input for next forecast.

Comment for: Preprocessing by Splitting Population into Sex-Group
Prepare the required input data for two models (male and female). 
Store population (value) for each age-group (key) into dictionary for male and female.

Comment for: Create Dataframe for Storing Prediction Result
Prepare for the output csv file that contain both gender for each age-group prediction.
Store the predicted result into the same format year-by-year.

================================================================================================================================================================================================================================
================================================================================================================================================================================================================================

2. LSTM_Iterative_Predict_Type2.ipynb

(a) Potential Methods
(i) Data Scaling
(ii) Input Data Merging / .tf Type Data Input Converting
(iii) Random Training & Validation Spliting (train_test_split from sklearn) of Full Training Set Starts from 1991 to 2001
(iv) Non-negative Processing during Rolling Prediction

(b) Applied Methods
(i) Random Training & Validation Spliting -- Validation Set weights = 0.2
Reason: Allows us to collect the more valuable information from 1999-2001 (closest to the target years) rather than having a fixed validation set from 1999-2001

(ii) Non-negative Processing -- Replace negative result from each year's predicted result during rolling updating process by 0
Reason: Population in all areas should not be negative

(c) Discarded Methods
(i) Min-Max Scaling -- With scaling & random spliting & non-negative process, the model does not provide a better result but introducing the risk of distorting the model (extremely large error rate in prediction year of 2011)
Reason: Scaling will let the data lies in range between [0, 1], which might clash with the process of non-negative replacement by 0 since the data scale are largely changed but the replacement is still 0 (unscaled)

(ii) Input Data Merging / .tf Type Data Input Converting
This method is origianlly designed for saving computational time but both of them does not provides a good result (or a acceptable trade-off)
Reason 1: Input Data Merging helps to cut half of the model fitting time (e.g. 3.5hrs of LSTM Model Type 1 with merged data but 7 hrs for the original one). However, the error rate has significantly increased.
Reason 2: .tf Type Data Input Converting does not save any computational time but lead to redundant preprocessing of the Dataframe

(d) Potential Improvement -- All the methods below do not improve the accuracy / save the computational time
(i) Early Stopping of the LSTM Model's Fitting
When looking for the best hyperparameters, the search stops early if the validation metric stops decreasing for a certain number of epochs.
Can potentially reduce computation time, although current implementations only apply this to hyperparameter tuning.
(ii) Learning rate
Once the best hyperparameter values are obtained, the learning rate can be reduced if the validation loss stops decreasing for a certain number of epochs.
Reducing the learning rate can help the model avoid overshooting the optimum weight values, but can slow down fitting.
(iii) Different Loss Funciton
Select mean-square-error as the optimal loss function in LSTM Model Fitting rather than the mean abosolute percentage error.

================================================================================================================================================================================================================================
================================================================================================================================================================================================================================

3. LSTM_Iterative_Predict_Type3.ipynb

(a) Extra Feature
(i) Sex -- Different sex labels (male, female)
(ii) Area -- Based on the area code of all 325 areas (for example 10101)
(iii) Total population average -- The total population of one area for all time series divided by the time series length which is 21 in our data
(iv) Birth rate -- The birth rate of one area

(b) Implementation logic
(i) For sex and area, different LSTM models are fitted on every sex label and area code by looping them.
(ii) For total population average and birth rate, the data is seperated into equal length quantiles based on different levels of average population and birth rate, and fit the LSTM model seperately.

(c) Result and findings
(i) Based on the type 2 model implementation logic (looping through all areas and fit LSTM model for each one), the extra feature of total population average and birth rate is not meaningful (Every time the train, validation and test set are the same, so the projection result will be the same).
(ii) The computation time is not increased and almost remain the same (for step 3 costs 3.5hrs), for everytime the model would be the same (known from (c) (i)), and every time the input data would be the same as well (each area time series population), so the running time would be almost the same.

================================================================================================================================================================================================================================
================================================================================================================================================================================================================================

4. LSTM_Iterative_Predict_Type_Extra.ipynb

(a) Implementation logic
We only evaluate the model by 2006 and 2011's predictions, which has a five years gap.
Create an extra model upon the previous version, with a new parameter "gap" and set it equal to 5.
In this case, when we input the data for one year, the model will direcly output the prediction for 5 years later.
So we only need two rounds to get the evaluation data. Input 2001, output 2006; input the predicted 2006, output 2011.

(b) Implementation Reason
With only predicting age-sex cohorts' population in 2006, 2011, the rolling update iteration only requires two steps Processing
Comparing the logic of the Standard type and the Extra type model, gap = 5 only requires 2 times error stacking while gap 1 requires 10 times
In this case, it reduce the stacked error introduced in each round for predicting next step's result