{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import IPython\n",
    "import IPython.display\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "import tensorflow_probability as tfp\n",
    "import keras_tuner as kt\n",
    "\n",
    "mpl.rcParams['figure.figsize'] = (8, 6)\n",
    "mpl.rcParams['axes.grid'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function definitions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#new\n",
    "\n",
    "def split_ts_data(data, val_start, test_start):\n",
    "    year_min = min(data['Year'])\n",
    "    year_max = max(data['Year'])\n",
    "    year_range = year_max-year_min\n",
    "    \n",
    "    assert (val_start >= year_min) & (test_start >= year_min) & (val_start <= year_max) & (test_start <= year_max), \"Parameter out of bounds\"\n",
    "    assert (val_start > year_min) & (test_start > year_min), \"Training set is empty.\"\n",
    "    assert val_start < test_start, \"Validation set is empty.\"\n",
    "    assert year_range > 0, \"Data contains less than 2 years.\"\n",
    "    \n",
    "    \n",
    "    train_data = data[(data['Year']<val_start) & (data['Year']<test_start)]\n",
    "    val_data = data[(data['Year']>=val_start) & (data['Year']<test_start)]\n",
    "    test_data = data[data['Year']>=test_start]\n",
    "    \n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_dataset(df, input_width, label_width, shift):\n",
    "    def create_window(tensor):\n",
    "        #input -> length of time series used for training\n",
    "        #shift -> how far off prediction is from last input\n",
    "        #label -> points to predict\n",
    "        total_window_size = input_width + shift\n",
    "        label_start = total_window_size - label_width\n",
    "\n",
    "        input_bounds = slice(0, input_width)\n",
    "        label_bounds = slice(label_start, None)\n",
    "\n",
    "        inputs = tensor[:,input_bounds,:]\n",
    "        labels = tensor[:,label_bounds,:]\n",
    "\n",
    "        inputs.set_shape([None, input_width, None])\n",
    "        labels.set_shape([None, label_width, None])\n",
    "\n",
    "        return inputs, labels\n",
    "    \n",
    "    total_window_size = input_width + shift\n",
    "    \n",
    "    arr = np.array(df, dtype=np.float32)\n",
    "    ds = tf.keras.utils.timeseries_dataset_from_array(\n",
    "      data=arr,\n",
    "      targets=None,\n",
    "      sequence_length=total_window_size,\n",
    "      sequence_stride=1,\n",
    "      shuffle=False,\n",
    "      batch_size=32,)\n",
    "    \n",
    "    ds = ds.map(create_window)\n",
    "    \n",
    "    return ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compile_and_fit(model, num_epochs, input_optimizer='adam', input_loss='mse'):\n",
    "    model.compile(optimizer=input_optimizer, loss=input_loss)\n",
    "    history = model.fit(x=train_inputs,y=train_labels, batch_size = 32, epochs=num_epochs, validation_data=val_ds, shuffle=False)\n",
    "    \n",
    "    return history"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def col_dict(np_df):\n",
    "    return_dict = {col:index for index, col in enumerate(np_df.columns)}\n",
    "    \n",
    "    return return_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot(df, ds, input_width, label_width, shift, model=None, plot_col='10101 m0.4', max_subplots=3):\n",
    "    #ensure that df and ds match e.g. train_df must be accompanied by train_ds\n",
    "    col_indices = col_dict(df)\n",
    "    \n",
    "    total_window_size = label_width + shift\n",
    "    input_slice = slice(0,input_width)\n",
    "    input_indices = np.arange(total_window_size)[input_slice]\n",
    "    label_start = total_window_size - label_width\n",
    "    labels_slice = slice(label_start, None)\n",
    "    label_indices = np.arange(total_window_size)[labels_slice]\n",
    "    \n",
    "    inputs = next(iter(ds))[0]\n",
    "    labels = next(iter(ds))[1]\n",
    "    plt.figure(figsize=(12, 8))\n",
    "    plot_col_index = col_indices[plot_col] \n",
    "    max_n = min(max_subplots, len(inputs))\n",
    "    \n",
    "    for n in range(max_n):\n",
    "        plt.subplot(max_n, 1, n+1)\n",
    "        plt.ylabel(plot_col)\n",
    "        plt.plot(input_indices, inputs[n, :, plot_col_index],\n",
    "             label='Inputs', marker='.', zorder=-10)\n",
    "        \n",
    "        plt.scatter(label_indices, labels[n, :, plot_col_index],\n",
    "                edgecolors='k', label='Labels', c='#2ca02c', s=64)\n",
    "        \n",
    "        if model is not None:\n",
    "          predictions = model(inputs)\n",
    "          plt.scatter(label_indices, predictions[n, :, plot_col_index],\n",
    "                  marker='X', edgecolors='k', label='Predictions',\n",
    "                  c='#ff7f0e', s=64)\n",
    "            \n",
    "        if n == 0:\n",
    "          plt.legend()\n",
    "        \n",
    "    plt.xlabel('Year')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Main code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Edit parameters here, but do not rename variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Read, preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_data = pd.read_csv('../Data/newSA3.csv')\n",
    "\n",
    "\n",
    "\n",
    "#Parameters\n",
    "validation_start = 2002\n",
    "test_start = 2006\n",
    "#\n",
    "\n",
    "\n",
    "\n",
    "train_df, val_df, test_df = split_ts_data(raw_data, validation_start, test_start)\n",
    "\n",
    "train_df = train_df[train_df.columns.difference([\"Unnamed: 0\",\"Year\"])]\n",
    "val_df = val_df[val_df.columns.difference([\"Unnamed: 0\",\"Year\"])]\n",
    "test_df = test_df[test_df.columns.difference([\"Unnamed: 0\",\"Year\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Create datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1 Pro\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-07 20:58:05.870470: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-08-07 20:58:05.870575: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2022-08-07 20:58:05.975747: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"
     ]
    }
   ],
   "source": [
    "#Parameters\n",
    "input_width = 2 #data used in prediction\n",
    "label_width = 1 #points to predict\n",
    "shift = 1 #how many years away is the last point to predict\n",
    "#\n",
    "\n",
    "\n",
    "\n",
    "train_ds = make_dataset(train_df, input_width, label_width, shift)\n",
    "val_ds = make_dataset(val_df, input_width, label_width, shift)\n",
    "test_ds = make_dataset(test_df, input_width, label_width, shift)\n",
    "\n",
    "num_cols = next(iter(train_ds))[0].shape[2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create and fit model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SACohortModel(kt.HyperModel):\n",
    "    def build(self,hp):\n",
    "        #### Hyperparameters\n",
    "        # add hyperparameters as needed when adding layers\n",
    "        \n",
    "        ##layer hyperparameters\n",
    "        hp_lstm1_units = hp.Choice('units',[10,30,50])\n",
    "        hp_lstm1_act = hp.Choice('activation', [\"relu\"])\n",
    "\n",
    "        ##model hyperparameters -> adjust tf.keras.models type and model.add layers\n",
    "        model = tf.keras.models.Sequential()\n",
    "        model.add(tf.keras.layers.LSTM(units = hp_lstm1_units, \n",
    "                                       activation=hp_lstm1_act, \n",
    "                                       return_sequences=False))\n",
    "        \n",
    "        model.add(tf.keras.layers.Dense(label_width * num_cols))\n",
    "        model.add(tf.keras.layers.Reshape([label_width,num_cols]))\n",
    "        \n",
    "        ##compilation hyperparameters\n",
    "        hp_epochs = hp.Choice(\"epochs\",[10,20,30])\n",
    "        hp_input_optimizer = hp.Choice('input_optimizer',[\"adam\", \"adadelta\"])\n",
    "        loss_fun = \"mse\"\n",
    "        \n",
    "        ####\n",
    "        \n",
    "        #Do not edit\n",
    "        model.compile(loss = loss_fun)\n",
    "        \n",
    "        return model\n",
    "        #Do not edit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Reloading Oracle from existing project ./untitled_project/oracle.json\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "INFO:tensorflow:Reloading Tuner from ./untitled_project/tuner0.json\n",
      "INFO:tensorflow:Oracle triggered exit\n"
     ]
    }
   ],
   "source": [
    "#Parameter\n",
    "num_epochs = 2000\n",
    "#\n",
    "\n",
    "train_inputs = next(iter(train_ds))[0]\n",
    "train_labels = next(iter(train_ds))[1]\n",
    "\n",
    "val_inputs = next(iter(val_ds))[0]\n",
    "val_labels = next(iter(val_ds))[1]\n",
    "\n",
    "test_inputs = next(iter(test_ds))[0]\n",
    "\n",
    "tuner = kt.RandomSearch(\n",
    "    SACohortModel(),\n",
    "    objective='val_loss',\n",
    "    max_trials=5)\n",
    "\n",
    "tuner.search(train_inputs, train_labels, epochs = num_epochs, validation_data = (val_inputs, val_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create model with above parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm_1 will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "Epoch 1/2000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-07 20:58:06.709651: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 1s 890ms/step - loss: 4160483.5000 - val_loss: 4684589.0000\n",
      "Epoch 2/2000\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 4134295.5000 - val_loss: 4629127.0000\n",
      "Epoch 3/2000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 4083659.5000 - val_loss: 4590146.5000\n",
      "Epoch 4/2000\n",
      "1/1 [==============================] - ETA: 0s - loss: 4049027.2500"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-08-07 20:58:07.026782: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 49ms/step - loss: 4049027.2500 - val_loss: 4491301.0000\n",
      "Epoch 5/2000\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 3965840.0000 - val_loss: 4448461.0000\n",
      "Epoch 6/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 3945604.5000 - val_loss: 4298805.0000\n",
      "Epoch 7/2000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 3772121.2500 - val_loss: 4365085.5000\n",
      "Epoch 8/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 3852551.2500 - val_loss: 4447542.0000\n",
      "Epoch 9/2000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 3919300.5000 - val_loss: 4469576.0000\n",
      "Epoch 10/2000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 3936139.5000 - val_loss: 4395324.0000\n",
      "Epoch 11/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 3872397.2500 - val_loss: 4225466.0000\n",
      "Epoch 12/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 3728355.5000 - val_loss: 3981802.5000\n",
      "Epoch 13/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 3522242.2500 - val_loss: 3695450.7500\n",
      "Epoch 14/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 3280225.7500 - val_loss: 3400785.7500\n",
      "Epoch 15/2000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 3031215.5000 - val_loss: 3130996.5000\n",
      "Epoch 16/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 2802962.5000 - val_loss: 2909925.5000\n",
      "Epoch 17/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 2615001.0000 - val_loss: 2742276.2500\n",
      "Epoch 18/2000\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 2470606.0000 - val_loss: 2613290.7500\n",
      "Epoch 19/2000\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 2356935.2500 - val_loss: 2496158.7500\n",
      "Epoch 20/2000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 2251305.7500 - val_loss: 2363887.0000\n",
      "Epoch 21/2000\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 2131154.2500 - val_loss: 2201158.0000\n",
      "Epoch 22/2000\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 1983988.5000 - val_loss: 2008815.3750\n",
      "Epoch 23/2000\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 1811085.1250 - val_loss: 1800133.0000\n",
      "Epoch 24/2000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 1624221.7500 - val_loss: 1593209.5000\n",
      "Epoch 25/2000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 1439143.8750 - val_loss: 1403824.8750\n",
      "Epoch 26/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 1269468.1250 - val_loss: 1241081.7500\n",
      "Epoch 27/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 1122969.6250 - val_loss: 1106410.3750\n",
      "Epoch 28/2000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 1000753.1250 - val_loss: 995342.3750\n",
      "Epoch 29/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 898797.6875 - val_loss: 900778.1250\n",
      "Epoch 30/2000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 810791.3125 - val_loss: 816266.3750\n",
      "Epoch 31/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 730982.5625 - val_loss: 738134.6250\n",
      "Epoch 32/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 656049.2500 - val_loss: 665988.1250\n",
      "Epoch 33/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 585563.3125 - val_loss: 601770.5000\n",
      "Epoch 34/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 521215.0000 - val_loss: 548007.5000\n",
      "Epoch 35/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 465322.6875 - val_loss: 506016.5000\n",
      "Epoch 36/2000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 419298.4062 - val_loss: 474809.3125\n",
      "Epoch 37/2000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 382671.7500 - val_loss: 451145.8750\n",
      "Epoch 38/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 353101.4062 - val_loss: 430683.1250\n",
      "Epoch 39/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 327270.3438 - val_loss: 409621.5625\n",
      "Epoch 40/2000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 302233.3438 - val_loss: 386006.5312\n",
      "Epoch 41/2000\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 276475.9062 - val_loss: 360082.1875\n",
      "Epoch 42/2000\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 250197.9688 - val_loss: 333670.1562\n",
      "Epoch 43/2000\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 224795.8281 - val_loss: 309031.1875\n",
      "Epoch 44/2000\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 201926.2188 - val_loss: 287814.0000\n",
      "Epoch 45/2000\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 182658.6250 - val_loss: 270519.8438\n",
      "Epoch 46/2000\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 167068.3594 - val_loss: 256576.6875\n",
      "Epoch 47/2000\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 154346.0938 - val_loss: 244840.7969\n",
      "Epoch 48/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 143260.8125 - val_loss: 234210.3594\n",
      "Epoch 49/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 132706.2812 - val_loss: 224068.7812\n",
      "Epoch 50/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 122088.4766 - val_loss: 214409.5156\n",
      "Epoch 51/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 111429.3359 - val_loss: 205652.5312\n",
      "Epoch 52/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 101197.0234 - val_loss: 198284.5312\n",
      "Epoch 53/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 91978.3203 - val_loss: 192508.5312\n",
      "Epoch 54/2000\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 84152.9453 - val_loss: 188066.4688\n",
      "Epoch 55/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 77712.4766 - val_loss: 184309.7500\n",
      "Epoch 56/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 72290.5703 - val_loss: 180471.7500\n",
      "Epoch 57/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 67368.9766 - val_loss: 175995.1562\n",
      "Epoch 58/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 62538.2695 - val_loss: 170746.6250\n",
      "Epoch 59/2000\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 57673.7070 - val_loss: 165021.4688\n",
      "Epoch 60/2000\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 52943.3281 - val_loss: 159358.1250\n",
      "Epoch 61/2000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 48665.1797 - val_loss: 154277.3125\n",
      "Epoch 62/2000\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 45107.6328 - val_loss: 150081.4062\n",
      "Epoch 63/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 42344.5938 - val_loss: 146796.0312\n",
      "Epoch 64/2000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 40231.0781 - val_loss: 144249.3750\n",
      "Epoch 65/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 38491.9883 - val_loss: 142218.6875\n",
      "Epoch 66/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 36861.0156 - val_loss: 140554.7188\n",
      "Epoch 67/2000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 35192.2383 - val_loss: 139223.8438\n",
      "Epoch 68/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 33493.3633 - val_loss: 138262.1562\n",
      "Epoch 69/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 31876.5762 - val_loss: 137684.6562\n",
      "Epoch 70/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 30465.3535 - val_loss: 137411.3125\n",
      "Epoch 71/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 29312.7637 - val_loss: 137257.0000\n",
      "Epoch 72/2000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 28373.5840 - val_loss: 136990.8594\n",
      "Epoch 73/2000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 27538.0742 - val_loss: 136430.0312\n",
      "Epoch 74/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 26699.6621 - val_loss: 135513.4844\n",
      "Epoch 75/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 25812.7559 - val_loss: 134317.4219\n",
      "Epoch 76/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 24907.9414 - val_loss: 133009.8906\n",
      "Epoch 77/2000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 24061.3926 - val_loss: 131773.6094\n",
      "Epoch 78/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 45ms/step - loss: 23342.7520 - val_loss: 130740.9844\n",
      "Epoch 79/2000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 22774.9277 - val_loss: 129968.4688\n",
      "Epoch 80/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 22327.0566 - val_loss: 129451.4062\n",
      "Epoch 81/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 21939.2324 - val_loss: 129158.8594\n",
      "Epoch 82/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 21559.3828 - val_loss: 129061.6953\n",
      "Epoch 83/2000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 21169.1074 - val_loss: 129138.2188\n",
      "Epoch 84/2000\n",
      "1/1 [==============================] - 0s 58ms/step - loss: 20785.4395 - val_loss: 129360.8672\n",
      "Epoch 85/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 20441.5781 - val_loss: 129678.4062\n",
      "Epoch 86/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 20161.4375 - val_loss: 130011.3594\n",
      "Epoch 87/2000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 19944.2051 - val_loss: 130267.7656\n",
      "Epoch 88/2000\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 19766.7227 - val_loss: 130371.1797\n",
      "Epoch 89/2000\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 19599.4727 - val_loss: 130286.6406\n",
      "Epoch 90/2000\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 19424.2285 - val_loss: 130029.8125\n",
      "Epoch 91/2000\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 19242.1328 - val_loss: 129656.3984\n",
      "Epoch 92/2000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 19068.5410 - val_loss: 129238.1328\n",
      "Epoch 93/2000\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 18920.0117 - val_loss: 128839.3281\n",
      "Epoch 94/2000\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 18802.8301 - val_loss: 128503.4062\n",
      "Epoch 95/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 18710.0410 - val_loss: 128251.4375\n",
      "Epoch 96/2000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 18627.4629 - val_loss: 128088.6562\n",
      "Epoch 97/2000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 18543.2754 - val_loss: 128010.6094\n",
      "Epoch 98/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 18454.5879 - val_loss: 128005.1562\n",
      "Epoch 99/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 18367.2402 - val_loss: 128051.1797\n",
      "Epoch 100/2000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 18290.0176 - val_loss: 128117.7969\n",
      "Epoch 101/2000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 18228.0449 - val_loss: 128168.1562\n",
      "Epoch 102/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 18179.6387 - val_loss: 128168.2344\n",
      "Epoch 103/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 18138.2500 - val_loss: 128096.7656\n",
      "Epoch 104/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 18097.3164 - val_loss: 127951.8047\n",
      "Epoch 105/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 18054.3379 - val_loss: 127750.1562\n",
      "Epoch 106/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 18011.5977 - val_loss: 127520.8906\n",
      "Epoch 107/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17973.5312 - val_loss: 127295.8438\n",
      "Epoch 108/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17943.1230 - val_loss: 127101.6406\n",
      "Epoch 109/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17919.8906 - val_loss: 126955.7578\n",
      "Epoch 110/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17900.5840 - val_loss: 126865.7500\n",
      "Epoch 111/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17881.6406 - val_loss: 126830.7500\n",
      "Epoch 112/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17861.4902 - val_loss: 126842.7422\n",
      "Epoch 113/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17841.0840 - val_loss: 126887.8125\n",
      "Epoch 114/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17822.6113 - val_loss: 126947.5000\n",
      "Epoch 115/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17807.6426 - val_loss: 127001.9062\n",
      "Epoch 116/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17795.9902 - val_loss: 127033.5938\n",
      "Epoch 117/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17786.0488 - val_loss: 127031.8750\n",
      "Epoch 118/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17776.0938 - val_loss: 126995.0078\n",
      "Epoch 119/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17765.4395 - val_loss: 126930.1562\n",
      "Epoch 120/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17754.7031 - val_loss: 126850.4219\n",
      "Epoch 121/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17745.0762 - val_loss: 126770.8594\n",
      "Epoch 122/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17737.3223 - val_loss: 126704.7344\n",
      "Epoch 123/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17731.2656 - val_loss: 126661.1875\n",
      "Epoch 124/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17726.0410 - val_loss: 126644.2500\n",
      "Epoch 125/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17720.7988 - val_loss: 126652.9688\n",
      "Epoch 126/2000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 17715.2871 - val_loss: 126682.0547\n",
      "Epoch 127/2000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 17709.8828 - val_loss: 126722.8125\n",
      "Epoch 128/2000\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 17705.1699 - val_loss: 126764.9688\n",
      "Epoch 129/2000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 17701.4336 - val_loss: 126798.5625\n",
      "Epoch 130/2000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 17698.4590 - val_loss: 126816.0078\n",
      "Epoch 131/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17695.7480 - val_loss: 126813.7969\n",
      "Epoch 132/2000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 17692.9102 - val_loss: 126792.9688\n",
      "Epoch 133/2000\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 17689.9102 - val_loss: 126758.6094\n",
      "Epoch 134/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17687.0098 - val_loss: 126717.7812\n",
      "Epoch 135/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17684.5059 - val_loss: 126678.0234\n",
      "Epoch 136/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17682.4785 - val_loss: 126645.4844\n",
      "Epoch 137/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17680.7676 - val_loss: 126623.9141\n",
      "Epoch 138/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17679.1250 - val_loss: 126614.2188\n",
      "Epoch 139/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17677.4160 - val_loss: 126614.6719\n",
      "Epoch 140/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17675.6855 - val_loss: 126621.4375\n",
      "Epoch 141/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17674.0898 - val_loss: 126629.4844\n",
      "Epoch 142/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17672.7461 - val_loss: 126633.7969\n",
      "Epoch 143/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17671.6309 - val_loss: 126630.4453\n",
      "Epoch 144/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17670.6152 - val_loss: 126617.4922\n",
      "Epoch 145/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17669.5840 - val_loss: 126595.2734\n",
      "Epoch 146/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17668.5000 - val_loss: 126566.3125\n",
      "Epoch 147/2000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 17667.4316 - val_loss: 126534.3203\n",
      "Epoch 148/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17666.4629 - val_loss: 126503.3438\n",
      "Epoch 149/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17665.6191 - val_loss: 126476.8906\n",
      "Epoch 150/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17664.8574 - val_loss: 126457.1406\n",
      "Epoch 151/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17664.1250 - val_loss: 126444.6875\n",
      "Epoch 152/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17663.3652 - val_loss: 126438.6562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 153/2000\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 17662.6035 - val_loss: 126436.8750\n",
      "Epoch 154/2000\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 17661.8770 - val_loss: 126436.6719\n",
      "Epoch 155/2000\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 17661.2207 - val_loss: 126435.2500\n",
      "Epoch 156/2000\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 17660.6289 - val_loss: 126430.5469\n",
      "Epoch 157/2000\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 17660.0625 - val_loss: 126421.5000\n",
      "Epoch 158/2000\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 17659.4941 - val_loss: 126408.3594\n",
      "Epoch 159/2000\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 17658.9199 - val_loss: 126392.3203\n",
      "Epoch 160/2000\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 17658.3574 - val_loss: 126375.2656\n",
      "Epoch 161/2000\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 17657.8301 - val_loss: 126359.1562\n",
      "Epoch 162/2000\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 17657.3379 - val_loss: 126345.6719\n",
      "Epoch 163/2000\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 17656.8691 - val_loss: 126335.6094\n",
      "Epoch 164/2000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 17656.4062 - val_loss: 126329.0312\n",
      "Epoch 165/2000\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 17655.9375 - val_loss: 126325.1641\n",
      "Epoch 166/2000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 17655.4746 - val_loss: 126322.7734\n",
      "Epoch 167/2000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 17655.0273 - val_loss: 126320.2656\n",
      "Epoch 168/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17654.6035 - val_loss: 126316.3359\n",
      "Epoch 169/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17654.1973 - val_loss: 126310.0781\n",
      "Epoch 170/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17653.7969 - val_loss: 126301.2891\n",
      "Epoch 171/2000\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 17653.3965 - val_loss: 126290.3750\n",
      "Epoch 172/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17653.0000 - val_loss: 126278.2188\n",
      "Epoch 173/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17652.6133 - val_loss: 126265.9844\n",
      "Epoch 174/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17652.2402 - val_loss: 126254.6719\n",
      "Epoch 175/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17651.8789 - val_loss: 126245.0781\n",
      "Epoch 176/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17651.5254 - val_loss: 126237.4531\n",
      "Epoch 177/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17651.1758 - val_loss: 126231.6250\n",
      "Epoch 178/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17650.8301 - val_loss: 126227.0156\n",
      "Epoch 179/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17650.4902 - val_loss: 126222.8438\n",
      "Epoch 180/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17650.1602 - val_loss: 126218.2812\n",
      "Epoch 181/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17649.8379 - val_loss: 126212.7969\n",
      "Epoch 182/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17649.5254 - val_loss: 126206.0781\n",
      "Epoch 183/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17649.2129 - val_loss: 126198.2031\n",
      "Epoch 184/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17648.9023 - val_loss: 126189.6094\n",
      "Epoch 185/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17648.6016 - val_loss: 126180.7734\n",
      "Epoch 186/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17648.3066 - val_loss: 126172.3359\n",
      "Epoch 187/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17648.0156 - val_loss: 126164.7188\n",
      "Epoch 188/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17647.7324 - val_loss: 126158.1406\n",
      "Epoch 189/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17647.4531 - val_loss: 126152.5156\n",
      "Epoch 190/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17647.1758 - val_loss: 126147.5781\n",
      "Epoch 191/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17646.9062 - val_loss: 126142.9375\n",
      "Epoch 192/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17646.6387 - val_loss: 126138.1094\n",
      "Epoch 193/2000\n",
      "1/1 [==============================] - 0s 52ms/step - loss: 17646.3828 - val_loss: 126132.7891\n",
      "Epoch 194/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17646.1250 - val_loss: 126126.7969\n",
      "Epoch 195/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17645.8711 - val_loss: 126120.1719\n",
      "Epoch 196/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17645.6270 - val_loss: 126113.0938\n",
      "Epoch 197/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17645.3828 - val_loss: 126105.9297\n",
      "Epoch 198/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17645.1445 - val_loss: 126098.9219\n",
      "Epoch 199/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17644.9102 - val_loss: 126092.3594\n",
      "Epoch 200/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17644.6816 - val_loss: 126086.3750\n",
      "Epoch 201/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17644.4531 - val_loss: 126080.9375\n",
      "Epoch 202/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17644.2305 - val_loss: 126075.8594\n",
      "Epoch 203/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17644.0117 - val_loss: 126070.9219\n",
      "Epoch 204/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17643.7949 - val_loss: 126065.9453\n",
      "Epoch 205/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17643.5879 - val_loss: 126060.6875\n",
      "Epoch 206/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17643.3789 - val_loss: 126055.1250\n",
      "Epoch 207/2000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 17643.1758 - val_loss: 126049.2500\n",
      "Epoch 208/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17642.9746 - val_loss: 126043.2031\n",
      "Epoch 209/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17642.7754 - val_loss: 126037.1719\n",
      "Epoch 210/2000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 17642.5840 - val_loss: 126031.2656\n",
      "Epoch 211/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17642.3926 - val_loss: 126025.6719\n",
      "Epoch 212/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17642.2051 - val_loss: 126020.4219\n",
      "Epoch 213/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17642.0215 - val_loss: 126015.4688\n",
      "Epoch 214/2000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 17641.8398 - val_loss: 126010.7031\n",
      "Epoch 215/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17641.6641 - val_loss: 126006.0312\n",
      "Epoch 216/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17641.4863 - val_loss: 126001.3281\n",
      "Epoch 217/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17641.3164 - val_loss: 125996.4844\n",
      "Epoch 218/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17641.1504 - val_loss: 125991.4609\n",
      "Epoch 219/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17640.9805 - val_loss: 125986.3125\n",
      "Epoch 220/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17640.8164 - val_loss: 125981.1719\n",
      "Epoch 221/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17640.6582 - val_loss: 125976.0625\n",
      "Epoch 222/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17640.4980 - val_loss: 125971.0938\n",
      "Epoch 223/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17640.3438 - val_loss: 125966.3203\n",
      "Epoch 224/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17640.1934 - val_loss: 125961.7344\n",
      "Epoch 225/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17640.0410 - val_loss: 125957.3516\n",
      "Epoch 226/2000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 17639.8965 - val_loss: 125953.0312\n",
      "Epoch 227/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 45ms/step - loss: 17639.7500 - val_loss: 125948.7656\n",
      "Epoch 228/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17639.6074 - val_loss: 125944.4531\n",
      "Epoch 229/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17639.4688 - val_loss: 125940.1094\n",
      "Epoch 230/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17639.3301 - val_loss: 125935.6562\n",
      "Epoch 231/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17639.1953 - val_loss: 125931.1875\n",
      "Epoch 232/2000\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 17639.0625 - val_loss: 125926.7344\n",
      "Epoch 233/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17638.9316 - val_loss: 125922.3438\n",
      "Epoch 234/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17638.8027 - val_loss: 125918.0703\n",
      "Epoch 235/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17638.6777 - val_loss: 125913.9219\n",
      "Epoch 236/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17638.5527 - val_loss: 125909.8984\n",
      "Epoch 237/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17638.4316 - val_loss: 125905.9609\n",
      "Epoch 238/2000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 17638.3086 - val_loss: 125902.0625\n",
      "Epoch 239/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17638.1914 - val_loss: 125898.1719\n",
      "Epoch 240/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17638.0742 - val_loss: 125894.2812\n",
      "Epoch 241/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17637.9629 - val_loss: 125890.3750\n",
      "Epoch 242/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17637.8477 - val_loss: 125886.4531\n",
      "Epoch 243/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17637.7363 - val_loss: 125882.5781\n",
      "Epoch 244/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17637.6289 - val_loss: 125878.7188\n",
      "Epoch 245/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17637.5215 - val_loss: 125874.9531\n",
      "Epoch 246/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17637.4160 - val_loss: 125871.2656\n",
      "Epoch 247/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17637.3125 - val_loss: 125867.6484\n",
      "Epoch 248/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17637.2090 - val_loss: 125864.1094\n",
      "Epoch 249/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17637.1094 - val_loss: 125860.6172\n",
      "Epoch 250/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17637.0098 - val_loss: 125857.1250\n",
      "Epoch 251/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17636.9141 - val_loss: 125853.6562\n",
      "Epoch 252/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17636.8184 - val_loss: 125850.1719\n",
      "Epoch 253/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17636.7227 - val_loss: 125846.6875\n",
      "Epoch 254/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17636.6289 - val_loss: 125843.2109\n",
      "Epoch 255/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17636.5391 - val_loss: 125839.7891\n",
      "Epoch 256/2000\n",
      "1/1 [==============================] - 0s 40ms/step - loss: 17636.4473 - val_loss: 125836.3906\n",
      "Epoch 257/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17636.3613 - val_loss: 125833.0781\n",
      "Epoch 258/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17636.2754 - val_loss: 125829.7969\n",
      "Epoch 259/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17636.1875 - val_loss: 125826.5859\n",
      "Epoch 260/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17636.1035 - val_loss: 125823.3906\n",
      "Epoch 261/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17636.0215 - val_loss: 125820.2500\n",
      "Epoch 262/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17635.9375 - val_loss: 125817.0938\n",
      "Epoch 263/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17635.8574 - val_loss: 125813.9609\n",
      "Epoch 264/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17635.7773 - val_loss: 125810.8281\n",
      "Epoch 265/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17635.6992 - val_loss: 125807.7188\n",
      "Epoch 266/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17635.6211 - val_loss: 125804.6328\n",
      "Epoch 267/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17635.5449 - val_loss: 125801.6094\n",
      "Epoch 268/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17635.4688 - val_loss: 125798.6250\n",
      "Epoch 269/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17635.4004 - val_loss: 125795.6562\n",
      "Epoch 270/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17635.3242 - val_loss: 125792.7500\n",
      "Epoch 271/2000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 17635.2539 - val_loss: 125789.8672\n",
      "Epoch 272/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17635.1816 - val_loss: 125787.0312\n",
      "Epoch 273/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17635.1113 - val_loss: 125784.1875\n",
      "Epoch 274/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17635.0410 - val_loss: 125781.3594\n",
      "Epoch 275/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17634.9766 - val_loss: 125778.5625\n",
      "Epoch 276/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17634.9062 - val_loss: 125775.7969\n",
      "Epoch 277/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17634.8398 - val_loss: 125773.0469\n",
      "Epoch 278/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17634.7754 - val_loss: 125770.3281\n",
      "Epoch 279/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17634.7129 - val_loss: 125767.6562\n",
      "Epoch 280/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17634.6504 - val_loss: 125765.0156\n",
      "Epoch 281/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17634.5879 - val_loss: 125762.3984\n",
      "Epoch 282/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17634.5254 - val_loss: 125759.7969\n",
      "Epoch 283/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17634.4629 - val_loss: 125757.2422\n",
      "Epoch 284/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17634.4004 - val_loss: 125754.6953\n",
      "Epoch 285/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17634.3418 - val_loss: 125752.1406\n",
      "Epoch 286/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17634.2852 - val_loss: 125749.6562\n",
      "Epoch 287/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17634.2266 - val_loss: 125747.1562\n",
      "Epoch 288/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17634.1680 - val_loss: 125744.6797\n",
      "Epoch 289/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17634.1113 - val_loss: 125742.2344\n",
      "Epoch 290/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17634.0566 - val_loss: 125739.8438\n",
      "Epoch 291/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17634.0000 - val_loss: 125737.4531\n",
      "Epoch 292/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17633.9453 - val_loss: 125735.0938\n",
      "Epoch 293/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17633.8926 - val_loss: 125732.7500\n",
      "Epoch 294/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17633.8379 - val_loss: 125730.4219\n",
      "Epoch 295/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17633.7852 - val_loss: 125728.1328\n",
      "Epoch 296/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17633.7324 - val_loss: 125725.8438\n",
      "Epoch 297/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17633.6816 - val_loss: 125723.5781\n",
      "Epoch 298/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17633.6289 - val_loss: 125721.3516\n",
      "Epoch 299/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17633.5801 - val_loss: 125719.1250\n",
      "Epoch 300/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17633.5312 - val_loss: 125716.9375\n",
      "Epoch 301/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 45ms/step - loss: 17633.4785 - val_loss: 125714.7656\n",
      "Epoch 302/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17633.4316 - val_loss: 125712.6250\n",
      "Epoch 303/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17633.3828 - val_loss: 125710.5000\n",
      "Epoch 304/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17633.3340 - val_loss: 125708.3906\n",
      "Epoch 305/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17633.2852 - val_loss: 125706.3125\n",
      "Epoch 306/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17633.2402 - val_loss: 125704.2344\n",
      "Epoch 307/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17633.1934 - val_loss: 125702.1719\n",
      "Epoch 308/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17633.1465 - val_loss: 125700.1484\n",
      "Epoch 309/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17633.1016 - val_loss: 125698.1094\n",
      "Epoch 310/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17633.0566 - val_loss: 125696.1250\n",
      "Epoch 311/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17633.0098 - val_loss: 125694.1406\n",
      "Epoch 312/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17632.9648 - val_loss: 125692.1875\n",
      "Epoch 313/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17632.9199 - val_loss: 125690.2500\n",
      "Epoch 314/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17632.8770 - val_loss: 125688.3281\n",
      "Epoch 315/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17632.8340 - val_loss: 125686.4375\n",
      "Epoch 316/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17632.7910 - val_loss: 125684.5469\n",
      "Epoch 317/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17632.7461 - val_loss: 125682.6641\n",
      "Epoch 318/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17632.7051 - val_loss: 125680.8125\n",
      "Epoch 319/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17632.6641 - val_loss: 125678.9531\n",
      "Epoch 320/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17632.6211 - val_loss: 125677.1484\n",
      "Epoch 321/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17632.5801 - val_loss: 125675.3359\n",
      "Epoch 322/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17632.5391 - val_loss: 125673.5469\n",
      "Epoch 323/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17632.4961 - val_loss: 125671.7656\n",
      "Epoch 324/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17632.4590 - val_loss: 125670.0156\n",
      "Epoch 325/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17632.4160 - val_loss: 125668.2812\n",
      "Epoch 326/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17632.3770 - val_loss: 125666.5625\n",
      "Epoch 327/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17632.3379 - val_loss: 125664.8594\n",
      "Epoch 328/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17632.2988 - val_loss: 125663.1250\n",
      "Epoch 329/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17632.2598 - val_loss: 125661.4609\n",
      "Epoch 330/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17632.2188 - val_loss: 125659.7891\n",
      "Epoch 331/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17632.1836 - val_loss: 125658.1406\n",
      "Epoch 332/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17632.1426 - val_loss: 125656.5000\n",
      "Epoch 333/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17632.1035 - val_loss: 125654.9062\n",
      "Epoch 334/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17632.0664 - val_loss: 125653.2656\n",
      "Epoch 335/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17632.0293 - val_loss: 125651.6719\n",
      "Epoch 336/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17631.9941 - val_loss: 125650.0781\n",
      "Epoch 337/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17631.9551 - val_loss: 125648.5156\n",
      "Epoch 338/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17631.9160 - val_loss: 125646.9609\n",
      "Epoch 339/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17631.8809 - val_loss: 125645.4219\n",
      "Epoch 340/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17631.8438 - val_loss: 125643.8828\n",
      "Epoch 341/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17631.8086 - val_loss: 125642.3750\n",
      "Epoch 342/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17631.7715 - val_loss: 125640.8594\n",
      "Epoch 343/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17631.7363 - val_loss: 125639.3438\n",
      "Epoch 344/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17631.6992 - val_loss: 125637.8672\n",
      "Epoch 345/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17631.6641 - val_loss: 125636.4062\n",
      "Epoch 346/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17631.6289 - val_loss: 125634.9531\n",
      "Epoch 347/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17631.5918 - val_loss: 125633.5078\n",
      "Epoch 348/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17631.5586 - val_loss: 125632.0625\n",
      "Epoch 349/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17631.5254 - val_loss: 125630.6406\n",
      "Epoch 350/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17631.4902 - val_loss: 125629.2422\n",
      "Epoch 351/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17631.4512 - val_loss: 125627.8281\n",
      "Epoch 352/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17631.4199 - val_loss: 125626.4375\n",
      "Epoch 353/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17631.3848 - val_loss: 125625.0469\n",
      "Epoch 354/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17631.3516 - val_loss: 125623.7031\n",
      "Epoch 355/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17631.3164 - val_loss: 125622.3438\n",
      "Epoch 356/2000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 17631.2812 - val_loss: 125620.9766\n",
      "Epoch 357/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17631.2461 - val_loss: 125619.6562\n",
      "Epoch 358/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17631.2148 - val_loss: 125618.3125\n",
      "Epoch 359/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17631.1816 - val_loss: 125617.0078\n",
      "Epoch 360/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17631.1465 - val_loss: 125615.6875\n",
      "Epoch 361/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17631.1133 - val_loss: 125614.3906\n",
      "Epoch 362/2000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 17631.0801 - val_loss: 125613.1016\n",
      "Epoch 363/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17631.0469 - val_loss: 125611.8281\n",
      "Epoch 364/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17631.0137 - val_loss: 125610.5469\n",
      "Epoch 365/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17630.9824 - val_loss: 125609.2969\n",
      "Epoch 366/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17630.9473 - val_loss: 125608.0547\n",
      "Epoch 367/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17630.9160 - val_loss: 125606.7969\n",
      "Epoch 368/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17630.8848 - val_loss: 125605.5781\n",
      "Epoch 369/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17630.8516 - val_loss: 125604.3438\n",
      "Epoch 370/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17630.8184 - val_loss: 125603.1406\n",
      "Epoch 371/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17630.7891 - val_loss: 125601.9375\n",
      "Epoch 372/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17630.7539 - val_loss: 125600.7422\n",
      "Epoch 373/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17630.7227 - val_loss: 125599.5234\n",
      "Epoch 374/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17630.6914 - val_loss: 125598.3516\n",
      "Epoch 375/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 46ms/step - loss: 17630.6582 - val_loss: 125597.1719\n",
      "Epoch 376/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17630.6250 - val_loss: 125596.0078\n",
      "Epoch 377/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17630.5938 - val_loss: 125594.8359\n",
      "Epoch 378/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17630.5625 - val_loss: 125593.7031\n",
      "Epoch 379/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17630.5293 - val_loss: 125592.5469\n",
      "Epoch 380/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17630.4980 - val_loss: 125591.4062\n",
      "Epoch 381/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17630.4688 - val_loss: 125590.2656\n",
      "Epoch 382/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17630.4375 - val_loss: 125589.1562\n",
      "Epoch 383/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17630.4062 - val_loss: 125588.0391\n",
      "Epoch 384/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17630.3750 - val_loss: 125586.9375\n",
      "Epoch 385/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17630.3438 - val_loss: 125585.8281\n",
      "Epoch 386/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17630.3125 - val_loss: 125584.7344\n",
      "Epoch 387/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17630.2812 - val_loss: 125583.6406\n",
      "Epoch 388/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17630.2520 - val_loss: 125582.5469\n",
      "Epoch 389/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17630.2188 - val_loss: 125581.4688\n",
      "Epoch 390/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17630.1895 - val_loss: 125580.4141\n",
      "Epoch 391/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17630.1562 - val_loss: 125579.3750\n",
      "Epoch 392/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17630.1289 - val_loss: 125578.2891\n",
      "Epoch 393/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17630.0957 - val_loss: 125577.2578\n",
      "Epoch 394/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17630.0664 - val_loss: 125576.1875\n",
      "Epoch 395/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17630.0352 - val_loss: 125575.1719\n",
      "Epoch 396/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17630.0059 - val_loss: 125574.1406\n",
      "Epoch 397/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17629.9766 - val_loss: 125573.1094\n",
      "Epoch 398/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17629.9453 - val_loss: 125572.0938\n",
      "Epoch 399/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17629.9160 - val_loss: 125571.0859\n",
      "Epoch 400/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17629.8848 - val_loss: 125570.0625\n",
      "Epoch 401/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17629.8535 - val_loss: 125569.0703\n",
      "Epoch 402/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17629.8223 - val_loss: 125568.0625\n",
      "Epoch 403/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17629.7930 - val_loss: 125567.0625\n",
      "Epoch 404/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17629.7637 - val_loss: 125566.0781\n",
      "Epoch 405/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17629.7324 - val_loss: 125565.1094\n",
      "Epoch 406/2000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 17629.7031 - val_loss: 125564.1250\n",
      "Epoch 407/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17629.6758 - val_loss: 125563.1719\n",
      "Epoch 408/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17629.6465 - val_loss: 125562.2031\n",
      "Epoch 409/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17629.6152 - val_loss: 125561.2344\n",
      "Epoch 410/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17629.5879 - val_loss: 125560.2812\n",
      "Epoch 411/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17629.5566 - val_loss: 125559.3281\n",
      "Epoch 412/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17629.5254 - val_loss: 125558.3828\n",
      "Epoch 413/2000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 17629.4961 - val_loss: 125557.4375\n",
      "Epoch 414/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17629.4668 - val_loss: 125556.4922\n",
      "Epoch 415/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17629.4375 - val_loss: 125555.5547\n",
      "Epoch 416/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17629.4062 - val_loss: 125554.6484\n",
      "Epoch 417/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17629.3789 - val_loss: 125553.7109\n",
      "Epoch 418/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17629.3496 - val_loss: 125552.7969\n",
      "Epoch 419/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17629.3203 - val_loss: 125551.8594\n",
      "Epoch 420/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17629.2910 - val_loss: 125550.9688\n",
      "Epoch 421/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17629.2617 - val_loss: 125550.0781\n",
      "Epoch 422/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17629.2324 - val_loss: 125549.1719\n",
      "Epoch 423/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17629.2051 - val_loss: 125548.2500\n",
      "Epoch 424/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17629.1758 - val_loss: 125547.3594\n",
      "Epoch 425/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17629.1465 - val_loss: 125546.4531\n",
      "Epoch 426/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17629.1191 - val_loss: 125545.5781\n",
      "Epoch 427/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17629.0879 - val_loss: 125544.7109\n",
      "Epoch 428/2000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 17629.0605 - val_loss: 125543.8281\n",
      "Epoch 429/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17629.0312 - val_loss: 125542.9531\n",
      "Epoch 430/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17629.0039 - val_loss: 125542.0781\n",
      "Epoch 431/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17628.9727 - val_loss: 125541.1953\n",
      "Epoch 432/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17628.9453 - val_loss: 125540.3438\n",
      "Epoch 433/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17628.9160 - val_loss: 125539.4609\n",
      "Epoch 434/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17628.8867 - val_loss: 125538.5938\n",
      "Epoch 435/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17628.8574 - val_loss: 125537.7500\n",
      "Epoch 436/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17628.8320 - val_loss: 125536.8906\n",
      "Epoch 437/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17628.8008 - val_loss: 125536.0391\n",
      "Epoch 438/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17628.7754 - val_loss: 125535.1875\n",
      "Epoch 439/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17628.7461 - val_loss: 125534.3594\n",
      "Epoch 440/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17628.7168 - val_loss: 125533.5312\n",
      "Epoch 441/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17628.6875 - val_loss: 125532.6875\n",
      "Epoch 442/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17628.6602 - val_loss: 125531.8594\n",
      "Epoch 443/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17628.6328 - val_loss: 125531.0000\n",
      "Epoch 444/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17628.6035 - val_loss: 125530.1875\n",
      "Epoch 445/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17628.5762 - val_loss: 125529.3594\n",
      "Epoch 446/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17628.5488 - val_loss: 125528.5391\n",
      "Epoch 447/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17628.5195 - val_loss: 125527.7422\n",
      "Epoch 448/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17628.4941 - val_loss: 125526.9219\n",
      "Epoch 449/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 45ms/step - loss: 17628.4629 - val_loss: 125526.0938\n",
      "Epoch 450/2000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 17628.4355 - val_loss: 125525.2969\n",
      "Epoch 451/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17628.4062 - val_loss: 125524.4688\n",
      "Epoch 452/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17628.3809 - val_loss: 125523.6719\n",
      "Epoch 453/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17628.3535 - val_loss: 125522.8828\n",
      "Epoch 454/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17628.3242 - val_loss: 125522.0781\n",
      "Epoch 455/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17628.2949 - val_loss: 125521.2656\n",
      "Epoch 456/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17628.2676 - val_loss: 125520.4688\n",
      "Epoch 457/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17628.2402 - val_loss: 125519.6719\n",
      "Epoch 458/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17628.2148 - val_loss: 125518.8750\n",
      "Epoch 459/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17628.1855 - val_loss: 125518.0938\n",
      "Epoch 460/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17628.1582 - val_loss: 125517.3281\n",
      "Epoch 461/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17628.1328 - val_loss: 125516.5312\n",
      "Epoch 462/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17628.1035 - val_loss: 125515.7500\n",
      "Epoch 463/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17628.0762 - val_loss: 125514.9688\n",
      "Epoch 464/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17628.0488 - val_loss: 125514.1719\n",
      "Epoch 465/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17628.0215 - val_loss: 125513.4062\n",
      "Epoch 466/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17627.9941 - val_loss: 125512.6328\n",
      "Epoch 467/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17627.9668 - val_loss: 125511.8672\n",
      "Epoch 468/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17627.9375 - val_loss: 125511.0938\n",
      "Epoch 469/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17627.9141 - val_loss: 125510.3281\n",
      "Epoch 470/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17627.8848 - val_loss: 125509.5469\n",
      "Epoch 471/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17627.8594 - val_loss: 125508.8125\n",
      "Epoch 472/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17627.8301 - val_loss: 125508.0312\n",
      "Epoch 473/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17627.8027 - val_loss: 125507.2812\n",
      "Epoch 474/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17627.7773 - val_loss: 125506.5234\n",
      "Epoch 475/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17627.7500 - val_loss: 125505.7656\n",
      "Epoch 476/2000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 17627.7246 - val_loss: 125505.0078\n",
      "Epoch 477/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17627.6953 - val_loss: 125504.2422\n",
      "Epoch 478/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17627.6699 - val_loss: 125503.5156\n",
      "Epoch 479/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17627.6426 - val_loss: 125502.7578\n",
      "Epoch 480/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17627.6152 - val_loss: 125502.0000\n",
      "Epoch 481/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17627.5898 - val_loss: 125501.2734\n",
      "Epoch 482/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17627.5625 - val_loss: 125500.5312\n",
      "Epoch 483/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17627.5352 - val_loss: 125499.7891\n",
      "Epoch 484/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17627.5098 - val_loss: 125499.0469\n",
      "Epoch 485/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17627.4824 - val_loss: 125498.2969\n",
      "Epoch 486/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17627.4570 - val_loss: 125497.5703\n",
      "Epoch 487/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17627.4316 - val_loss: 125496.8203\n",
      "Epoch 488/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17627.4023 - val_loss: 125496.0938\n",
      "Epoch 489/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17627.3770 - val_loss: 125495.3750\n",
      "Epoch 490/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17627.3516 - val_loss: 125494.6562\n",
      "Epoch 491/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17627.3242 - val_loss: 125493.9062\n",
      "Epoch 492/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17627.2988 - val_loss: 125493.1875\n",
      "Epoch 493/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17627.2715 - val_loss: 125492.4531\n",
      "Epoch 494/2000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 17627.2461 - val_loss: 125491.7344\n",
      "Epoch 495/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17627.2188 - val_loss: 125491.0000\n",
      "Epoch 496/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17627.1934 - val_loss: 125490.2969\n",
      "Epoch 497/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17627.1660 - val_loss: 125489.5547\n",
      "Epoch 498/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17627.1406 - val_loss: 125488.8438\n",
      "Epoch 499/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17627.1152 - val_loss: 125488.1250\n",
      "Epoch 500/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17627.0879 - val_loss: 125487.4062\n",
      "Epoch 501/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17627.0605 - val_loss: 125486.7031\n",
      "Epoch 502/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17627.0371 - val_loss: 125485.9688\n",
      "Epoch 503/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17627.0098 - val_loss: 125485.2656\n",
      "Epoch 504/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17626.9844 - val_loss: 125484.5547\n",
      "Epoch 505/2000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 17626.9590 - val_loss: 125483.8438\n",
      "Epoch 506/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17626.9336 - val_loss: 125483.1328\n",
      "Epoch 507/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17626.9062 - val_loss: 125482.4297\n",
      "Epoch 508/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17626.8828 - val_loss: 125481.7344\n",
      "Epoch 509/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17626.8574 - val_loss: 125481.0156\n",
      "Epoch 510/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17626.8340 - val_loss: 125480.3125\n",
      "Epoch 511/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17626.8086 - val_loss: 125479.6172\n",
      "Epoch 512/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17626.7910 - val_loss: 125478.9062\n",
      "Epoch 513/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17626.7773 - val_loss: 125478.2500\n",
      "Epoch 514/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17626.7676 - val_loss: 125477.5312\n",
      "Epoch 515/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17626.7461 - val_loss: 125476.8125\n",
      "Epoch 516/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17626.6973 - val_loss: 125476.1016\n",
      "Epoch 517/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17626.6523 - val_loss: 125475.4062\n",
      "Epoch 518/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17626.6387 - val_loss: 125474.7500\n",
      "Epoch 519/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17626.6250 - val_loss: 125474.0156\n",
      "Epoch 520/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17626.5898 - val_loss: 125473.3125\n",
      "Epoch 521/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17626.5527 - val_loss: 125472.6094\n",
      "Epoch 522/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17626.5352 - val_loss: 125471.9375\n",
      "Epoch 523/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 44ms/step - loss: 17626.5176 - val_loss: 125471.2188\n",
      "Epoch 524/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17626.4805 - val_loss: 125470.5312\n",
      "Epoch 525/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17626.4551 - val_loss: 125469.8438\n",
      "Epoch 526/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17626.4375 - val_loss: 125469.1562\n",
      "Epoch 527/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17626.4082 - val_loss: 125468.4688\n",
      "Epoch 528/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17626.3770 - val_loss: 125467.7812\n",
      "Epoch 529/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17626.3574 - val_loss: 125467.1094\n",
      "Epoch 530/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17626.3340 - val_loss: 125466.4062\n",
      "Epoch 531/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17626.3066 - val_loss: 125465.7188\n",
      "Epoch 532/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17626.2812 - val_loss: 125465.0469\n",
      "Epoch 533/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17626.2578 - val_loss: 125464.3438\n",
      "Epoch 534/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17626.2285 - val_loss: 125463.6719\n",
      "Epoch 535/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17626.2051 - val_loss: 125462.9922\n",
      "Epoch 536/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17626.1836 - val_loss: 125462.2969\n",
      "Epoch 537/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17626.1562 - val_loss: 125461.6328\n",
      "Epoch 538/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17626.1309 - val_loss: 125460.9453\n",
      "Epoch 539/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17626.1074 - val_loss: 125460.2812\n",
      "Epoch 540/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17626.0840 - val_loss: 125459.5938\n",
      "Epoch 541/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17626.0566 - val_loss: 125458.9219\n",
      "Epoch 542/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17626.0332 - val_loss: 125458.2344\n",
      "Epoch 543/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17626.0098 - val_loss: 125457.5547\n",
      "Epoch 544/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17625.9844 - val_loss: 125456.8906\n",
      "Epoch 545/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17625.9629 - val_loss: 125456.2031\n",
      "Epoch 546/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17625.9375 - val_loss: 125455.5312\n",
      "Epoch 547/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17625.9141 - val_loss: 125454.8750\n",
      "Epoch 548/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17625.8887 - val_loss: 125454.1953\n",
      "Epoch 549/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17625.8691 - val_loss: 125453.5156\n",
      "Epoch 550/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17625.8398 - val_loss: 125452.8438\n",
      "Epoch 551/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17625.8164 - val_loss: 125452.1875\n",
      "Epoch 552/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17625.7930 - val_loss: 125451.5234\n",
      "Epoch 553/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17625.7695 - val_loss: 125450.8516\n",
      "Epoch 554/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17625.7461 - val_loss: 125450.1719\n",
      "Epoch 555/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17625.7227 - val_loss: 125449.5156\n",
      "Epoch 556/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17625.6973 - val_loss: 125448.8359\n",
      "Epoch 557/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17625.6758 - val_loss: 125448.1875\n",
      "Epoch 558/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17625.6562 - val_loss: 125447.5312\n",
      "Epoch 559/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17625.6406 - val_loss: 125446.8906\n",
      "Epoch 560/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17625.6328 - val_loss: 125446.2344\n",
      "Epoch 561/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17625.6328 - val_loss: 125445.6172\n",
      "Epoch 562/2000\n",
      "1/1 [==============================] - 0s 56ms/step - loss: 17625.6191 - val_loss: 125444.8594\n",
      "Epoch 563/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17625.5586 - val_loss: 125444.2500\n",
      "Epoch 564/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17625.5137 - val_loss: 125443.5625\n",
      "Epoch 565/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17625.5176 - val_loss: 125442.9922\n",
      "Epoch 566/2000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 17625.5137 - val_loss: 125442.1719\n",
      "Epoch 567/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17625.4824 - val_loss: 125441.7500\n",
      "Epoch 568/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17625.4785 - val_loss: 125440.8906\n",
      "Epoch 569/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17625.4844 - val_loss: 125440.4062\n",
      "Epoch 570/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17625.4316 - val_loss: 125439.6250\n",
      "Epoch 571/2000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 17625.3711 - val_loss: 125438.9141\n",
      "Epoch 572/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17625.3516 - val_loss: 125438.3828\n",
      "Epoch 573/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17625.3340 - val_loss: 125437.5859\n",
      "Epoch 574/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17625.3086 - val_loss: 125437.0078\n",
      "Epoch 575/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17625.2852 - val_loss: 125436.3594\n",
      "Epoch 576/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17625.2461 - val_loss: 125435.6250\n",
      "Epoch 577/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17625.2246 - val_loss: 125435.0938\n",
      "Epoch 578/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17625.2129 - val_loss: 125434.3750\n",
      "Epoch 579/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17625.1816 - val_loss: 125433.6719\n",
      "Epoch 580/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17625.1504 - val_loss: 125433.1250\n",
      "Epoch 581/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17625.1348 - val_loss: 125432.4062\n",
      "Epoch 582/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17625.1113 - val_loss: 125431.7578\n",
      "Epoch 583/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17625.0801 - val_loss: 125431.1641\n",
      "Epoch 584/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17625.0625 - val_loss: 125430.4531\n",
      "Epoch 585/2000\n",
      "1/1 [==============================] - 0s 51ms/step - loss: 17625.0410 - val_loss: 125429.8438\n",
      "Epoch 586/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17625.0117 - val_loss: 125429.2031\n",
      "Epoch 587/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17624.9941 - val_loss: 125428.5000\n",
      "Epoch 588/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17624.9688 - val_loss: 125427.9062\n",
      "Epoch 589/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17624.9453 - val_loss: 125427.2500\n",
      "Epoch 590/2000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 17624.9238 - val_loss: 125426.5781\n",
      "Epoch 591/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17624.9004 - val_loss: 125425.9688\n",
      "Epoch 592/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17624.8770 - val_loss: 125425.2969\n",
      "Epoch 593/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17624.8535 - val_loss: 125424.6562\n",
      "Epoch 594/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17624.8320 - val_loss: 125424.0625\n",
      "Epoch 595/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17624.8086 - val_loss: 125423.3906\n",
      "Epoch 596/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17624.7910 - val_loss: 125422.7266\n",
      "Epoch 597/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 45ms/step - loss: 17624.7637 - val_loss: 125422.1094\n",
      "Epoch 598/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17624.7441 - val_loss: 125421.4375\n",
      "Epoch 599/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17624.7207 - val_loss: 125420.8125\n",
      "Epoch 600/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17624.7012 - val_loss: 125420.1875\n",
      "Epoch 601/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17624.6777 - val_loss: 125419.5078\n",
      "Epoch 602/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17624.6562 - val_loss: 125418.8984\n",
      "Epoch 603/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17624.6348 - val_loss: 125418.2656\n",
      "Epoch 604/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17624.6113 - val_loss: 125417.6094\n",
      "Epoch 605/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17624.5898 - val_loss: 125417.0000\n",
      "Epoch 606/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17624.5684 - val_loss: 125416.3594\n",
      "Epoch 607/2000\n",
      "1/1 [==============================] - 0s 50ms/step - loss: 17624.5488 - val_loss: 125415.7031\n",
      "Epoch 608/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17624.5254 - val_loss: 125415.0938\n",
      "Epoch 609/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17624.5039 - val_loss: 125414.4375\n",
      "Epoch 610/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17624.4824 - val_loss: 125413.7969\n",
      "Epoch 611/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17624.4629 - val_loss: 125413.1719\n",
      "Epoch 612/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17624.4414 - val_loss: 125412.5234\n",
      "Epoch 613/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17624.4199 - val_loss: 125411.9141\n",
      "Epoch 614/2000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 17624.4004 - val_loss: 125411.2812\n",
      "Epoch 615/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17624.3770 - val_loss: 125410.6406\n",
      "Epoch 616/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17624.3535 - val_loss: 125410.0234\n",
      "Epoch 617/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17624.3379 - val_loss: 125409.3906\n",
      "Epoch 618/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17624.3125 - val_loss: 125408.7500\n",
      "Epoch 619/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17624.2930 - val_loss: 125408.1250\n",
      "Epoch 620/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17624.2715 - val_loss: 125407.5000\n",
      "Epoch 621/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17624.2500 - val_loss: 125406.8750\n",
      "Epoch 622/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17624.2285 - val_loss: 125406.2344\n",
      "Epoch 623/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17624.2090 - val_loss: 125405.6094\n",
      "Epoch 624/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17624.1875 - val_loss: 125404.9844\n",
      "Epoch 625/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17624.1660 - val_loss: 125404.3750\n",
      "Epoch 626/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17624.1504 - val_loss: 125403.7344\n",
      "Epoch 627/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17624.1250 - val_loss: 125403.1172\n",
      "Epoch 628/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17624.1055 - val_loss: 125402.4844\n",
      "Epoch 629/2000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 17624.0840 - val_loss: 125401.8516\n",
      "Epoch 630/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17624.0645 - val_loss: 125401.2188\n",
      "Epoch 631/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17624.0449 - val_loss: 125400.6250\n",
      "Epoch 632/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17624.0254 - val_loss: 125400.0000\n",
      "Epoch 633/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17624.0039 - val_loss: 125399.3828\n",
      "Epoch 634/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17623.9805 - val_loss: 125398.7500\n",
      "Epoch 635/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17623.9629 - val_loss: 125398.1172\n",
      "Epoch 636/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17623.9414 - val_loss: 125397.5156\n",
      "Epoch 637/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17623.9219 - val_loss: 125396.8750\n",
      "Epoch 638/2000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 17623.9023 - val_loss: 125396.2656\n",
      "Epoch 639/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17623.8828 - val_loss: 125395.6562\n",
      "Epoch 640/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17623.8613 - val_loss: 125395.0234\n",
      "Epoch 641/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17623.8418 - val_loss: 125394.4062\n",
      "Epoch 642/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17623.8223 - val_loss: 125393.7969\n",
      "Epoch 643/2000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 17623.8027 - val_loss: 125393.1797\n",
      "Epoch 644/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17623.7812 - val_loss: 125392.5547\n",
      "Epoch 645/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17623.7617 - val_loss: 125391.9375\n",
      "Epoch 646/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17623.7441 - val_loss: 125391.3359\n",
      "Epoch 647/2000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 17623.7227 - val_loss: 125390.7188\n",
      "Epoch 648/2000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 17623.7031 - val_loss: 125390.1016\n",
      "Epoch 649/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17623.6836 - val_loss: 125389.4844\n",
      "Epoch 650/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17623.6641 - val_loss: 125388.8750\n",
      "Epoch 651/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17623.6445 - val_loss: 125388.2812\n",
      "Epoch 652/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17623.6250 - val_loss: 125387.6484\n",
      "Epoch 653/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17623.6055 - val_loss: 125387.0469\n",
      "Epoch 654/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17623.5879 - val_loss: 125386.4297\n",
      "Epoch 655/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17623.5664 - val_loss: 125385.8125\n",
      "Epoch 656/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17623.5449 - val_loss: 125385.2109\n",
      "Epoch 657/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17623.5273 - val_loss: 125384.6016\n",
      "Epoch 658/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17623.5098 - val_loss: 125383.9922\n",
      "Epoch 659/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17623.4902 - val_loss: 125383.3906\n",
      "Epoch 660/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17623.4727 - val_loss: 125382.7656\n",
      "Epoch 661/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17623.4512 - val_loss: 125382.1562\n",
      "Epoch 662/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17623.4316 - val_loss: 125381.5625\n",
      "Epoch 663/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17623.4141 - val_loss: 125380.9375\n",
      "Epoch 664/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17623.3965 - val_loss: 125380.3438\n",
      "Epoch 665/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17623.3750 - val_loss: 125379.7344\n",
      "Epoch 666/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17623.3574 - val_loss: 125379.1328\n",
      "Epoch 667/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17623.3379 - val_loss: 125378.5391\n",
      "Epoch 668/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17623.3184 - val_loss: 125377.9297\n",
      "Epoch 669/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17623.3008 - val_loss: 125377.3047\n",
      "Epoch 670/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17623.2812 - val_loss: 125376.7344\n",
      "Epoch 671/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 47ms/step - loss: 17623.2617 - val_loss: 125376.1172\n",
      "Epoch 672/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17623.2441 - val_loss: 125375.5000\n",
      "Epoch 673/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17623.2266 - val_loss: 125374.8906\n",
      "Epoch 674/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17623.2051 - val_loss: 125374.3281\n",
      "Epoch 675/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17623.1875 - val_loss: 125373.7031\n",
      "Epoch 676/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17623.1699 - val_loss: 125373.1172\n",
      "Epoch 677/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17623.1504 - val_loss: 125372.5000\n",
      "Epoch 678/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17623.1328 - val_loss: 125371.9219\n",
      "Epoch 679/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17623.1152 - val_loss: 125371.3047\n",
      "Epoch 680/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17623.0957 - val_loss: 125370.7031\n",
      "Epoch 681/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17623.0781 - val_loss: 125370.1172\n",
      "Epoch 682/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17623.0586 - val_loss: 125369.5000\n",
      "Epoch 683/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17623.0410 - val_loss: 125368.9375\n",
      "Epoch 684/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17623.0254 - val_loss: 125368.3359\n",
      "Epoch 685/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17623.0039 - val_loss: 125367.7266\n",
      "Epoch 686/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17622.9883 - val_loss: 125367.1406\n",
      "Epoch 687/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17622.9688 - val_loss: 125366.5391\n",
      "Epoch 688/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17622.9512 - val_loss: 125365.9531\n",
      "Epoch 689/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17622.9316 - val_loss: 125365.3594\n",
      "Epoch 690/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17622.9160 - val_loss: 125364.7812\n",
      "Epoch 691/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17622.9004 - val_loss: 125364.1719\n",
      "Epoch 692/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17622.8809 - val_loss: 125363.5938\n",
      "Epoch 693/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17622.8633 - val_loss: 125363.0000\n",
      "Epoch 694/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17622.8438 - val_loss: 125362.3906\n",
      "Epoch 695/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17622.8262 - val_loss: 125361.8125\n",
      "Epoch 696/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17622.8086 - val_loss: 125361.2266\n",
      "Epoch 697/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17622.7910 - val_loss: 125360.6484\n",
      "Epoch 698/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17622.7754 - val_loss: 125360.0469\n",
      "Epoch 699/2000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 17622.7578 - val_loss: 125359.4531\n",
      "Epoch 700/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17622.7402 - val_loss: 125358.8750\n",
      "Epoch 701/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17622.7227 - val_loss: 125358.2812\n",
      "Epoch 702/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17622.7051 - val_loss: 125357.6953\n",
      "Epoch 703/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17622.6875 - val_loss: 125357.1406\n",
      "Epoch 704/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17622.6699 - val_loss: 125356.5391\n",
      "Epoch 705/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17622.6523 - val_loss: 125355.9375\n",
      "Epoch 706/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17622.6348 - val_loss: 125355.3594\n",
      "Epoch 707/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17622.6191 - val_loss: 125354.7578\n",
      "Epoch 708/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17622.6016 - val_loss: 125354.2031\n",
      "Epoch 709/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17622.5879 - val_loss: 125353.6250\n",
      "Epoch 710/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17622.5684 - val_loss: 125353.0391\n",
      "Epoch 711/2000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 17622.5527 - val_loss: 125352.4531\n",
      "Epoch 712/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17622.5332 - val_loss: 125351.8672\n",
      "Epoch 713/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17622.5156 - val_loss: 125351.2812\n",
      "Epoch 714/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17622.5000 - val_loss: 125350.7109\n",
      "Epoch 715/2000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 17622.4824 - val_loss: 125350.1328\n",
      "Epoch 716/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17622.4648 - val_loss: 125349.5469\n",
      "Epoch 717/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17622.4473 - val_loss: 125348.9844\n",
      "Epoch 718/2000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 17622.4336 - val_loss: 125348.3906\n",
      "Epoch 719/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17622.4160 - val_loss: 125347.8203\n",
      "Epoch 720/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17622.4004 - val_loss: 125347.2500\n",
      "Epoch 721/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17622.3848 - val_loss: 125346.6641\n",
      "Epoch 722/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17622.3652 - val_loss: 125346.0938\n",
      "Epoch 723/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17622.3516 - val_loss: 125345.5156\n",
      "Epoch 724/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17622.3379 - val_loss: 125344.9375\n",
      "Epoch 725/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17622.3164 - val_loss: 125344.3750\n",
      "Epoch 726/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17622.3008 - val_loss: 125343.7891\n",
      "Epoch 727/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17622.2852 - val_loss: 125343.2188\n",
      "Epoch 728/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17622.2676 - val_loss: 125342.6562\n",
      "Epoch 729/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17622.2539 - val_loss: 125342.0859\n",
      "Epoch 730/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17622.2363 - val_loss: 125341.5156\n",
      "Epoch 731/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17622.2188 - val_loss: 125340.9453\n",
      "Epoch 732/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17622.2051 - val_loss: 125340.3750\n",
      "Epoch 733/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17622.1875 - val_loss: 125339.7969\n",
      "Epoch 734/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17622.1719 - val_loss: 125339.2266\n",
      "Epoch 735/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17622.1543 - val_loss: 125338.6719\n",
      "Epoch 736/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17622.1387 - val_loss: 125338.0938\n",
      "Epoch 737/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17622.1250 - val_loss: 125337.5547\n",
      "Epoch 738/2000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 17622.1074 - val_loss: 125336.9688\n",
      "Epoch 739/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17622.0918 - val_loss: 125336.4062\n",
      "Epoch 740/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17622.0762 - val_loss: 125335.8359\n",
      "Epoch 741/2000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 17622.0625 - val_loss: 125335.2734\n",
      "Epoch 742/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17622.0449 - val_loss: 125334.7188\n",
      "Epoch 743/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17622.0293 - val_loss: 125334.1562\n",
      "Epoch 744/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17622.0137 - val_loss: 125333.5781\n",
      "Epoch 745/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 46ms/step - loss: 17622.0000 - val_loss: 125333.0312\n",
      "Epoch 746/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17621.9824 - val_loss: 125332.4531\n",
      "Epoch 747/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17621.9668 - val_loss: 125331.9062\n",
      "Epoch 748/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17621.9512 - val_loss: 125331.3281\n",
      "Epoch 749/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17621.9336 - val_loss: 125330.7656\n",
      "Epoch 750/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17621.9219 - val_loss: 125330.2188\n",
      "Epoch 751/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17621.9062 - val_loss: 125329.6562\n",
      "Epoch 752/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17621.8926 - val_loss: 125329.0938\n",
      "Epoch 753/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17621.8750 - val_loss: 125328.5312\n",
      "Epoch 754/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17621.8613 - val_loss: 125327.9844\n",
      "Epoch 755/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17621.8457 - val_loss: 125327.4375\n",
      "Epoch 756/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17621.8301 - val_loss: 125326.8906\n",
      "Epoch 757/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17621.8164 - val_loss: 125326.2969\n",
      "Epoch 758/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17621.8027 - val_loss: 125325.7969\n",
      "Epoch 759/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17621.7910 - val_loss: 125325.1719\n",
      "Epoch 760/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17621.7793 - val_loss: 125324.7344\n",
      "Epoch 761/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17621.7754 - val_loss: 125324.0625\n",
      "Epoch 762/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17621.7754 - val_loss: 125323.6797\n",
      "Epoch 763/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17621.7676 - val_loss: 125322.9688\n",
      "Epoch 764/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17621.7461 - val_loss: 125322.5000\n",
      "Epoch 765/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17621.7090 - val_loss: 125321.8906\n",
      "Epoch 766/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17621.6816 - val_loss: 125321.3281\n",
      "Epoch 767/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17621.6777 - val_loss: 125320.8750\n",
      "Epoch 768/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17621.6738 - val_loss: 125320.2188\n",
      "Epoch 769/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17621.6504 - val_loss: 125319.7031\n",
      "Epoch 770/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17621.6250 - val_loss: 125319.1719\n",
      "Epoch 771/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17621.6152 - val_loss: 125318.5625\n",
      "Epoch 772/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17621.6113 - val_loss: 125318.0938\n",
      "Epoch 773/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17621.5879 - val_loss: 125317.5000\n",
      "Epoch 774/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17621.5664 - val_loss: 125316.9531\n",
      "Epoch 775/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17621.5566 - val_loss: 125316.4688\n",
      "Epoch 776/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17621.5449 - val_loss: 125315.8438\n",
      "Epoch 777/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17621.5254 - val_loss: 125315.3125\n",
      "Epoch 778/2000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 17621.5078 - val_loss: 125314.8203\n",
      "Epoch 779/2000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 17621.5000 - val_loss: 125314.2344\n",
      "Epoch 780/2000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 17621.4863 - val_loss: 125313.7266\n",
      "Epoch 781/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17621.4688 - val_loss: 125313.1719\n",
      "Epoch 782/2000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 17621.4512 - val_loss: 125312.5938\n",
      "Epoch 783/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17621.4414 - val_loss: 125312.0781\n",
      "Epoch 784/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17621.4277 - val_loss: 125311.5312\n",
      "Epoch 785/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17621.4102 - val_loss: 125310.9844\n",
      "Epoch 786/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17621.4004 - val_loss: 125310.4688\n",
      "Epoch 787/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17621.3848 - val_loss: 125309.9219\n",
      "Epoch 788/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17621.3691 - val_loss: 125309.3750\n",
      "Epoch 789/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17621.3535 - val_loss: 125308.8516\n",
      "Epoch 790/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17621.3438 - val_loss: 125308.2891\n",
      "Epoch 791/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17621.3281 - val_loss: 125307.7656\n",
      "Epoch 792/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17621.3125 - val_loss: 125307.2500\n",
      "Epoch 793/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17621.2988 - val_loss: 125306.6875\n",
      "Epoch 794/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17621.2891 - val_loss: 125306.1797\n",
      "Epoch 795/2000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 17621.2754 - val_loss: 125305.6406\n",
      "Epoch 796/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17621.2598 - val_loss: 125305.0781\n",
      "Epoch 797/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17621.2461 - val_loss: 125304.5781\n",
      "Epoch 798/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17621.2324 - val_loss: 125304.0234\n",
      "Epoch 799/2000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 17621.2168 - val_loss: 125303.5000\n",
      "Epoch 800/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17621.2051 - val_loss: 125302.9766\n",
      "Epoch 801/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17621.1934 - val_loss: 125302.4375\n",
      "Epoch 802/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17621.1816 - val_loss: 125301.9062\n",
      "Epoch 803/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17621.1660 - val_loss: 125301.3828\n",
      "Epoch 804/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17621.1523 - val_loss: 125300.8438\n",
      "Epoch 805/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17621.1387 - val_loss: 125300.3438\n",
      "Epoch 806/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17621.1270 - val_loss: 125299.7812\n",
      "Epoch 807/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17621.1133 - val_loss: 125299.2500\n",
      "Epoch 808/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17621.1016 - val_loss: 125298.7422\n",
      "Epoch 809/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17621.0879 - val_loss: 125298.2266\n",
      "Epoch 810/2000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 17621.0723 - val_loss: 125297.6875\n",
      "Epoch 811/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17621.0625 - val_loss: 125297.1719\n",
      "Epoch 812/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17621.0488 - val_loss: 125296.6406\n",
      "Epoch 813/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17621.0352 - val_loss: 125296.1250\n",
      "Epoch 814/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17621.0254 - val_loss: 125295.6094\n",
      "Epoch 815/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17621.0098 - val_loss: 125295.0781\n",
      "Epoch 816/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17620.9961 - val_loss: 125294.5781\n",
      "Epoch 817/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17620.9824 - val_loss: 125294.0312\n",
      "Epoch 818/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17620.9727 - val_loss: 125293.5156\n",
      "Epoch 819/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 45ms/step - loss: 17620.9590 - val_loss: 125293.0000\n",
      "Epoch 820/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17620.9473 - val_loss: 125292.4688\n",
      "Epoch 821/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17620.9336 - val_loss: 125291.9688\n",
      "Epoch 822/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17620.9219 - val_loss: 125291.4375\n",
      "Epoch 823/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17620.9082 - val_loss: 125290.9219\n",
      "Epoch 824/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17620.8965 - val_loss: 125290.4062\n",
      "Epoch 825/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17620.8828 - val_loss: 125289.8906\n",
      "Epoch 826/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17620.8711 - val_loss: 125289.3906\n",
      "Epoch 827/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17620.8594 - val_loss: 125288.8672\n",
      "Epoch 828/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17620.8477 - val_loss: 125288.3438\n",
      "Epoch 829/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17620.8379 - val_loss: 125287.8359\n",
      "Epoch 830/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17620.8223 - val_loss: 125287.3047\n",
      "Epoch 831/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17620.8086 - val_loss: 125286.8047\n",
      "Epoch 832/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17620.7988 - val_loss: 125286.2969\n",
      "Epoch 833/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17620.7852 - val_loss: 125285.7891\n",
      "Epoch 834/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17620.7754 - val_loss: 125285.2656\n",
      "Epoch 835/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17620.7617 - val_loss: 125284.7578\n",
      "Epoch 836/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17620.7500 - val_loss: 125284.2500\n",
      "Epoch 837/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17620.7383 - val_loss: 125283.7500\n",
      "Epoch 838/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17620.7266 - val_loss: 125283.2422\n",
      "Epoch 839/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17620.7129 - val_loss: 125282.7422\n",
      "Epoch 840/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17620.7012 - val_loss: 125282.2188\n",
      "Epoch 841/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17620.6895 - val_loss: 125281.7188\n",
      "Epoch 842/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17620.6816 - val_loss: 125281.2031\n",
      "Epoch 843/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17620.6660 - val_loss: 125280.6953\n",
      "Epoch 844/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17620.6562 - val_loss: 125280.2031\n",
      "Epoch 845/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17620.6504 - val_loss: 125279.7031\n",
      "Epoch 846/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17620.6445 - val_loss: 125279.2188\n",
      "Epoch 847/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17620.6445 - val_loss: 125278.7344\n",
      "Epoch 848/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17620.6465 - val_loss: 125278.2500\n",
      "Epoch 849/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17620.6406 - val_loss: 125277.6953\n",
      "Epoch 850/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17620.6152 - val_loss: 125277.1797\n",
      "Epoch 851/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17620.5781 - val_loss: 125276.6875\n",
      "Epoch 852/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17620.5645 - val_loss: 125276.1875\n",
      "Epoch 853/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17620.5703 - val_loss: 125275.7031\n",
      "Epoch 854/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17620.5605 - val_loss: 125275.1953\n",
      "Epoch 855/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17620.5352 - val_loss: 125274.6875\n",
      "Epoch 856/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17620.5176 - val_loss: 125274.1719\n",
      "Epoch 857/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17620.5137 - val_loss: 125273.6953\n",
      "Epoch 858/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17620.5098 - val_loss: 125273.1875\n",
      "Epoch 859/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17620.4883 - val_loss: 125272.6875\n",
      "Epoch 860/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17620.4727 - val_loss: 125272.2188\n",
      "Epoch 861/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17620.4688 - val_loss: 125271.7188\n",
      "Epoch 862/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17620.4590 - val_loss: 125271.2188\n",
      "Epoch 863/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17620.4414 - val_loss: 125270.7188\n",
      "Epoch 864/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17620.4316 - val_loss: 125270.2266\n",
      "Epoch 865/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17620.4258 - val_loss: 125269.7266\n",
      "Epoch 866/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17620.4160 - val_loss: 125269.2812\n",
      "Epoch 867/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17620.4082 - val_loss: 125268.7578\n",
      "Epoch 868/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17620.4082 - val_loss: 125268.3281\n",
      "Epoch 869/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17620.4121 - val_loss: 125267.7891\n",
      "Epoch 870/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17620.4043 - val_loss: 125267.3438\n",
      "Epoch 871/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17620.3828 - val_loss: 125266.7656\n",
      "Epoch 872/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17620.3555 - val_loss: 125266.3750\n",
      "Epoch 873/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17620.3477 - val_loss: 125265.7656\n",
      "Epoch 874/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17620.3574 - val_loss: 125265.4844\n",
      "Epoch 875/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17620.3691 - val_loss: 125264.7969\n",
      "Epoch 876/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17620.3535 - val_loss: 125264.4531\n",
      "Epoch 877/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17620.3125 - val_loss: 125263.8516\n",
      "Epoch 878/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17620.2852 - val_loss: 125263.3750\n",
      "Epoch 879/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17620.2812 - val_loss: 125263.0078\n",
      "Epoch 880/2000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 17620.2812 - val_loss: 125262.3594\n",
      "Epoch 881/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17620.2676 - val_loss: 125262.0000\n",
      "Epoch 882/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17620.2441 - val_loss: 125261.4375\n",
      "Epoch 883/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17620.2363 - val_loss: 125260.9531\n",
      "Epoch 884/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17620.2324 - val_loss: 125260.5469\n",
      "Epoch 885/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17620.2246 - val_loss: 125260.0156\n",
      "Epoch 886/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17620.2148 - val_loss: 125259.5234\n",
      "Epoch 887/2000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 17620.2129 - val_loss: 125259.1641\n",
      "Epoch 888/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17620.2148 - val_loss: 125258.5156\n",
      "Epoch 889/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17620.2031 - val_loss: 125258.1562\n",
      "Epoch 890/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17620.1777 - val_loss: 125257.6172\n",
      "Epoch 891/2000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 17620.1504 - val_loss: 125257.1094\n",
      "Epoch 892/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17620.1387 - val_loss: 125256.7422\n",
      "Epoch 893/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 46ms/step - loss: 17620.1387 - val_loss: 125256.1562\n",
      "Epoch 894/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17620.1328 - val_loss: 125255.7344\n",
      "Epoch 895/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17620.1152 - val_loss: 125255.2500\n",
      "Epoch 896/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17620.0977 - val_loss: 125254.7266\n",
      "Epoch 897/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17620.0879 - val_loss: 125254.3281\n",
      "Epoch 898/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17620.0840 - val_loss: 125253.8203\n",
      "Epoch 899/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17620.0723 - val_loss: 125253.3281\n",
      "Epoch 900/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17620.0586 - val_loss: 125252.8906\n",
      "Epoch 901/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17620.0449 - val_loss: 125252.3750\n",
      "Epoch 902/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17620.0410 - val_loss: 125251.9219\n",
      "Epoch 903/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17620.0312 - val_loss: 125251.4688\n",
      "Epoch 904/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17620.0176 - val_loss: 125250.9766\n",
      "Epoch 905/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17620.0039 - val_loss: 125250.5312\n",
      "Epoch 906/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17619.9980 - val_loss: 125250.0547\n",
      "Epoch 907/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17619.9902 - val_loss: 125249.5781\n",
      "Epoch 908/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17619.9766 - val_loss: 125249.1172\n",
      "Epoch 909/2000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 17619.9648 - val_loss: 125248.6562\n",
      "Epoch 910/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17619.9590 - val_loss: 125248.2031\n",
      "Epoch 911/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17619.9492 - val_loss: 125247.7188\n",
      "Epoch 912/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17619.9375 - val_loss: 125247.2500\n",
      "Epoch 913/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17619.9277 - val_loss: 125246.7812\n",
      "Epoch 914/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17619.9199 - val_loss: 125246.3281\n",
      "Epoch 915/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17619.9102 - val_loss: 125245.8750\n",
      "Epoch 916/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17619.9004 - val_loss: 125245.3906\n",
      "Epoch 917/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17619.8887 - val_loss: 125244.9297\n",
      "Epoch 918/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17619.8828 - val_loss: 125244.4688\n",
      "Epoch 919/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17619.8730 - val_loss: 125244.0000\n",
      "Epoch 920/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17619.8652 - val_loss: 125243.5703\n",
      "Epoch 921/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17619.8535 - val_loss: 125243.1094\n",
      "Epoch 922/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17619.8438 - val_loss: 125242.6484\n",
      "Epoch 923/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17619.8379 - val_loss: 125242.1719\n",
      "Epoch 924/2000\n",
      "1/1 [==============================] - 0s 57ms/step - loss: 17619.8262 - val_loss: 125241.7109\n",
      "Epoch 925/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17619.8164 - val_loss: 125241.2500\n",
      "Epoch 926/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17619.8066 - val_loss: 125240.8125\n",
      "Epoch 927/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17619.7988 - val_loss: 125240.3594\n",
      "Epoch 928/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17619.7891 - val_loss: 125239.9062\n",
      "Epoch 929/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17619.7812 - val_loss: 125239.4531\n",
      "Epoch 930/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17619.7754 - val_loss: 125238.9844\n",
      "Epoch 931/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17619.7637 - val_loss: 125238.5391\n",
      "Epoch 932/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17619.7539 - val_loss: 125238.0781\n",
      "Epoch 933/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17619.7461 - val_loss: 125237.6406\n",
      "Epoch 934/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17619.7363 - val_loss: 125237.1875\n",
      "Epoch 935/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17619.7285 - val_loss: 125236.7266\n",
      "Epoch 936/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17619.7188 - val_loss: 125236.2656\n",
      "Epoch 937/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17619.7090 - val_loss: 125235.8281\n",
      "Epoch 938/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17619.7012 - val_loss: 125235.3672\n",
      "Epoch 939/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17619.6934 - val_loss: 125234.9219\n",
      "Epoch 940/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17619.6836 - val_loss: 125234.4688\n",
      "Epoch 941/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17619.6758 - val_loss: 125234.0312\n",
      "Epoch 942/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17619.6660 - val_loss: 125233.5938\n",
      "Epoch 943/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17619.6582 - val_loss: 125233.1562\n",
      "Epoch 944/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17619.6504 - val_loss: 125232.6875\n",
      "Epoch 945/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17619.6406 - val_loss: 125232.2422\n",
      "Epoch 946/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17619.6328 - val_loss: 125231.7812\n",
      "Epoch 947/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17619.6250 - val_loss: 125231.3594\n",
      "Epoch 948/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17619.6152 - val_loss: 125230.9062\n",
      "Epoch 949/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17619.6074 - val_loss: 125230.4688\n",
      "Epoch 950/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17619.6016 - val_loss: 125230.0312\n",
      "Epoch 951/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17619.5898 - val_loss: 125229.5781\n",
      "Epoch 952/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17619.5840 - val_loss: 125229.1484\n",
      "Epoch 953/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17619.5742 - val_loss: 125228.6875\n",
      "Epoch 954/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17619.5664 - val_loss: 125228.2812\n",
      "Epoch 955/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17619.5566 - val_loss: 125227.8438\n",
      "Epoch 956/2000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 17619.5488 - val_loss: 125227.3906\n",
      "Epoch 957/2000\n",
      "1/1 [==============================] - 0s 53ms/step - loss: 17619.5410 - val_loss: 125226.9609\n",
      "Epoch 958/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17619.5352 - val_loss: 125226.5000\n",
      "Epoch 959/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17619.5254 - val_loss: 125226.0703\n",
      "Epoch 960/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17619.5176 - val_loss: 125225.6484\n",
      "Epoch 961/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17619.5098 - val_loss: 125225.2031\n",
      "Epoch 962/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17619.5020 - val_loss: 125224.7656\n",
      "Epoch 963/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17619.4941 - val_loss: 125224.3438\n",
      "Epoch 964/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17619.4844 - val_loss: 125223.9062\n",
      "Epoch 965/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17619.4766 - val_loss: 125223.4688\n",
      "Epoch 966/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17619.4688 - val_loss: 125223.0234\n",
      "Epoch 967/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 44ms/step - loss: 17619.4629 - val_loss: 125222.6016\n",
      "Epoch 968/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17619.4551 - val_loss: 125222.1719\n",
      "Epoch 969/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17619.4473 - val_loss: 125221.7422\n",
      "Epoch 970/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17619.4434 - val_loss: 125221.3125\n",
      "Epoch 971/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17619.4375 - val_loss: 125220.8750\n",
      "Epoch 972/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17619.4375 - val_loss: 125220.4766\n",
      "Epoch 973/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17619.4414 - val_loss: 125220.0547\n",
      "Epoch 974/2000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 17619.4453 - val_loss: 125219.6250\n",
      "Epoch 975/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17619.4395 - val_loss: 125219.1875\n",
      "Epoch 976/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17619.4160 - val_loss: 125218.7188\n",
      "Epoch 977/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17619.3887 - val_loss: 125218.3125\n",
      "Epoch 978/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17619.3809 - val_loss: 125217.8828\n",
      "Epoch 979/2000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 17619.3848 - val_loss: 125217.5000\n",
      "Epoch 980/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17619.3848 - val_loss: 125217.0000\n",
      "Epoch 981/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17619.3691 - val_loss: 125216.6484\n",
      "Epoch 982/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17619.3535 - val_loss: 125216.1562\n",
      "Epoch 983/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17619.3574 - val_loss: 125215.8438\n",
      "Epoch 984/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17619.3691 - val_loss: 125215.2969\n",
      "Epoch 985/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17619.3613 - val_loss: 125215.0312\n",
      "Epoch 986/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17619.3516 - val_loss: 125214.4531\n",
      "Epoch 987/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17619.3379 - val_loss: 125214.1250\n",
      "Epoch 988/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17619.3164 - val_loss: 125213.6562\n",
      "Epoch 989/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17619.2988 - val_loss: 125213.1875\n",
      "Epoch 990/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17619.2969 - val_loss: 125212.8906\n",
      "Epoch 991/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17619.2988 - val_loss: 125212.3594\n",
      "Epoch 992/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17619.2910 - val_loss: 125212.0156\n",
      "Epoch 993/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17619.2715 - val_loss: 125211.5781\n",
      "Epoch 994/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17619.2598 - val_loss: 125211.1250\n",
      "Epoch 995/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17619.2598 - val_loss: 125210.7812\n",
      "Epoch 996/2000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 17619.2598 - val_loss: 125210.2891\n",
      "Epoch 997/2000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 17619.2441 - val_loss: 125209.8906\n",
      "Epoch 998/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17619.2324 - val_loss: 125209.5156\n",
      "Epoch 999/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17619.2285 - val_loss: 125209.0312\n",
      "Epoch 1000/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17619.2266 - val_loss: 125208.6797\n",
      "Epoch 1001/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17619.2129 - val_loss: 125208.2266\n",
      "Epoch 1002/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17619.2031 - val_loss: 125207.8047\n",
      "Epoch 1003/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17619.1992 - val_loss: 125207.4453\n",
      "Epoch 1004/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17619.1953 - val_loss: 125206.9688\n",
      "Epoch 1005/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17619.1855 - val_loss: 125206.6094\n",
      "Epoch 1006/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17619.1816 - val_loss: 125206.2031\n",
      "Epoch 1007/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17619.1738 - val_loss: 125205.7656\n",
      "Epoch 1008/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17619.1738 - val_loss: 125205.4062\n",
      "Epoch 1009/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17619.1719 - val_loss: 125204.9531\n",
      "Epoch 1010/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17619.1738 - val_loss: 125204.5781\n",
      "Epoch 1011/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17619.1777 - val_loss: 125204.1953\n",
      "Epoch 1012/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17619.1738 - val_loss: 125203.7422\n",
      "Epoch 1013/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17619.1543 - val_loss: 125203.3281\n",
      "Epoch 1014/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17619.1289 - val_loss: 125202.9297\n",
      "Epoch 1015/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17619.1152 - val_loss: 125202.5234\n",
      "Epoch 1016/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17619.1191 - val_loss: 125202.1406\n",
      "Epoch 1017/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17619.1211 - val_loss: 125201.7031\n",
      "Epoch 1018/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17619.1074 - val_loss: 125201.3125\n",
      "Epoch 1019/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17619.0898 - val_loss: 125200.9219\n",
      "Epoch 1020/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17619.0840 - val_loss: 125200.5078\n",
      "Epoch 1021/2000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 17619.0879 - val_loss: 125200.1094\n",
      "Epoch 1022/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17619.0801 - val_loss: 125199.6875\n",
      "Epoch 1023/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17619.0664 - val_loss: 125199.2969\n",
      "Epoch 1024/2000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 17619.0566 - val_loss: 125198.9141\n",
      "Epoch 1025/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17619.0566 - val_loss: 125198.5000\n",
      "Epoch 1026/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17619.0508 - val_loss: 125198.1094\n",
      "Epoch 1027/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17619.0391 - val_loss: 125197.7031\n",
      "Epoch 1028/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17619.0312 - val_loss: 125197.2969\n",
      "Epoch 1029/2000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 17619.0273 - val_loss: 125196.9219\n",
      "Epoch 1030/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17619.0254 - val_loss: 125196.5078\n",
      "Epoch 1031/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17619.0156 - val_loss: 125196.1016\n",
      "Epoch 1032/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17619.0078 - val_loss: 125195.7031\n",
      "Epoch 1033/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17619.0000 - val_loss: 125195.2969\n",
      "Epoch 1034/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17618.9961 - val_loss: 125194.9297\n",
      "Epoch 1035/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17618.9902 - val_loss: 125194.5469\n",
      "Epoch 1036/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.9844 - val_loss: 125194.1641\n",
      "Epoch 1037/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.9824 - val_loss: 125193.7812\n",
      "Epoch 1038/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.9824 - val_loss: 125193.3750\n",
      "Epoch 1039/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17618.9844 - val_loss: 125192.9844\n",
      "Epoch 1040/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.9902 - val_loss: 125192.6562\n",
      "Epoch 1041/2000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 43ms/step - loss: 17618.9941 - val_loss: 125192.2188\n",
      "Epoch 1042/2000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 17618.9785 - val_loss: 125191.8281\n",
      "Epoch 1043/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.9512 - val_loss: 125191.3984\n",
      "Epoch 1044/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17618.9316 - val_loss: 125191.0234\n",
      "Epoch 1045/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.9336 - val_loss: 125190.6250\n",
      "Epoch 1046/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.9414 - val_loss: 125190.2656\n",
      "Epoch 1047/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.9316 - val_loss: 125189.8281\n",
      "Epoch 1048/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17618.9121 - val_loss: 125189.5156\n",
      "Epoch 1049/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17618.9062 - val_loss: 125189.0312\n",
      "Epoch 1050/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.9102 - val_loss: 125188.8047\n",
      "Epoch 1051/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17618.9062 - val_loss: 125188.1875\n",
      "Epoch 1052/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17618.8965 - val_loss: 125188.1016\n",
      "Epoch 1053/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.8965 - val_loss: 125187.3438\n",
      "Epoch 1054/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.9102 - val_loss: 125187.4844\n",
      "Epoch 1055/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17618.9160 - val_loss: 125186.5312\n",
      "Epoch 1056/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17618.9102 - val_loss: 125186.6562\n",
      "Epoch 1057/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.8848 - val_loss: 125185.9141\n",
      "Epoch 1058/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.8633 - val_loss: 125185.5859\n",
      "Epoch 1059/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.8516 - val_loss: 125185.4531\n",
      "Epoch 1060/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.8574 - val_loss: 125184.6797\n",
      "Epoch 1061/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17618.8574 - val_loss: 125184.6562\n",
      "Epoch 1062/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.8438 - val_loss: 125184.1094\n",
      "Epoch 1063/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17618.8262 - val_loss: 125183.6328\n",
      "Epoch 1064/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.8223 - val_loss: 125183.5469\n",
      "Epoch 1065/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.8262 - val_loss: 125182.8750\n",
      "Epoch 1066/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17618.8164 - val_loss: 125182.6094\n",
      "Epoch 1067/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17618.8027 - val_loss: 125182.3125\n",
      "Epoch 1068/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.8027 - val_loss: 125181.7188\n",
      "Epoch 1069/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.8008 - val_loss: 125181.5859\n",
      "Epoch 1070/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.7910 - val_loss: 125181.1094\n",
      "Epoch 1071/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.7812 - val_loss: 125180.6484\n",
      "Epoch 1072/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.7793 - val_loss: 125180.4688\n",
      "Epoch 1073/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.7754 - val_loss: 125179.9219\n",
      "Epoch 1074/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.7676 - val_loss: 125179.5938\n",
      "Epoch 1075/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.7598 - val_loss: 125179.3125\n",
      "Epoch 1076/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.7578 - val_loss: 125178.7812\n",
      "Epoch 1077/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.7520 - val_loss: 125178.5312\n",
      "Epoch 1078/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.7461 - val_loss: 125178.1406\n",
      "Epoch 1079/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17618.7402 - val_loss: 125177.6875\n",
      "Epoch 1080/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17618.7363 - val_loss: 125177.4375\n",
      "Epoch 1081/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.7324 - val_loss: 125177.0234\n",
      "Epoch 1082/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.7266 - val_loss: 125176.6094\n",
      "Epoch 1083/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17618.7266 - val_loss: 125176.3750\n",
      "Epoch 1084/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17618.7285 - val_loss: 125175.8828\n",
      "Epoch 1085/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.7324 - val_loss: 125175.6250\n",
      "Epoch 1086/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.7363 - val_loss: 125175.2344\n",
      "Epoch 1087/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.7285 - val_loss: 125174.8125\n",
      "Epoch 1088/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.7129 - val_loss: 125174.4844\n",
      "Epoch 1089/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.6973 - val_loss: 125174.1094\n",
      "Epoch 1090/2000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 17618.6934 - val_loss: 125173.7344\n",
      "Epoch 1091/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17618.7090 - val_loss: 125173.4219\n",
      "Epoch 1092/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.7227 - val_loss: 125173.0312\n",
      "Epoch 1093/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.7266 - val_loss: 125172.7266\n",
      "Epoch 1094/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.7188 - val_loss: 125172.2656\n",
      "Epoch 1095/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.7090 - val_loss: 125172.1172\n",
      "Epoch 1096/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.6973 - val_loss: 125171.4219\n",
      "Epoch 1097/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.6895 - val_loss: 125171.4219\n",
      "Epoch 1098/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.6738 - val_loss: 125170.7344\n",
      "Epoch 1099/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17618.6660 - val_loss: 125170.5625\n",
      "Epoch 1100/2000\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 17618.6641 - val_loss: 125170.2188\n",
      "Epoch 1101/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.6719 - val_loss: 125169.7344\n",
      "Epoch 1102/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.6816 - val_loss: 125169.5312\n",
      "Epoch 1103/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.6719 - val_loss: 125169.0781\n",
      "Epoch 1104/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.6465 - val_loss: 125168.6172\n",
      "Epoch 1105/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17618.6211 - val_loss: 125168.4688\n",
      "Epoch 1106/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.6191 - val_loss: 125167.9219\n",
      "Epoch 1107/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.6211 - val_loss: 125167.6719\n",
      "Epoch 1108/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.6191 - val_loss: 125167.3438\n",
      "Epoch 1109/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.6055 - val_loss: 125166.8125\n",
      "Epoch 1110/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.5938 - val_loss: 125166.6641\n",
      "Epoch 1111/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.5918 - val_loss: 125166.1875\n",
      "Epoch 1112/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.5898 - val_loss: 125165.8281\n",
      "Epoch 1113/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.5840 - val_loss: 125165.5938\n",
      "Epoch 1114/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.5723 - val_loss: 125165.0703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1115/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.5684 - val_loss: 125164.8438\n",
      "Epoch 1116/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.5664 - val_loss: 125164.4844\n",
      "Epoch 1117/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.5605 - val_loss: 125164.0469\n",
      "Epoch 1118/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.5527 - val_loss: 125163.8438\n",
      "Epoch 1119/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.5488 - val_loss: 125163.3594\n",
      "Epoch 1120/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.5449 - val_loss: 125163.0469\n",
      "Epoch 1121/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.5391 - val_loss: 125162.7500\n",
      "Epoch 1122/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.5352 - val_loss: 125162.3281\n",
      "Epoch 1123/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17618.5273 - val_loss: 125162.0391\n",
      "Epoch 1124/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.5254 - val_loss: 125161.7109\n",
      "Epoch 1125/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.5254 - val_loss: 125161.2656\n",
      "Epoch 1126/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17618.5176 - val_loss: 125161.0781\n",
      "Epoch 1127/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.5156 - val_loss: 125160.5781\n",
      "Epoch 1128/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.5176 - val_loss: 125160.3828\n",
      "Epoch 1129/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17618.5215 - val_loss: 125159.9375\n",
      "Epoch 1130/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.5254 - val_loss: 125159.7031\n",
      "Epoch 1131/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.5254 - val_loss: 125159.2344\n",
      "Epoch 1132/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.5176 - val_loss: 125158.9844\n",
      "Epoch 1133/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17618.4961 - val_loss: 125158.5547\n",
      "Epoch 1134/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.4785 - val_loss: 125158.2422\n",
      "Epoch 1135/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.4785 - val_loss: 125157.9688\n",
      "Epoch 1136/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.4824 - val_loss: 125157.5156\n",
      "Epoch 1137/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.4824 - val_loss: 125157.2969\n",
      "Epoch 1138/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.4707 - val_loss: 125156.8594\n",
      "Epoch 1139/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.4590 - val_loss: 125156.5391\n",
      "Epoch 1140/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.4551 - val_loss: 125156.2500\n",
      "Epoch 1141/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.4590 - val_loss: 125155.8281\n",
      "Epoch 1142/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.4531 - val_loss: 125155.5938\n",
      "Epoch 1143/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.4414 - val_loss: 125155.1875\n",
      "Epoch 1144/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.4355 - val_loss: 125154.8594\n",
      "Epoch 1145/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.4375 - val_loss: 125154.5781\n",
      "Epoch 1146/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.4336 - val_loss: 125154.1484\n",
      "Epoch 1147/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.4277 - val_loss: 125153.8750\n",
      "Epoch 1148/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.4199 - val_loss: 125153.5391\n",
      "Epoch 1149/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.4180 - val_loss: 125153.1562\n",
      "Epoch 1150/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17618.4160 - val_loss: 125152.8750\n",
      "Epoch 1151/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.4121 - val_loss: 125152.5078\n",
      "Epoch 1152/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17618.4062 - val_loss: 125152.1875\n",
      "Epoch 1153/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.4004 - val_loss: 125151.8750\n",
      "Epoch 1154/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.4004 - val_loss: 125151.5000\n",
      "Epoch 1155/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.3945 - val_loss: 125151.2188\n",
      "Epoch 1156/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.3887 - val_loss: 125150.8672\n",
      "Epoch 1157/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.3848 - val_loss: 125150.5156\n",
      "Epoch 1158/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.3828 - val_loss: 125150.2344\n",
      "Epoch 1159/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.3789 - val_loss: 125149.8750\n",
      "Epoch 1160/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.3750 - val_loss: 125149.5625\n",
      "Epoch 1161/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.3691 - val_loss: 125149.2578\n",
      "Epoch 1162/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17618.3691 - val_loss: 125148.8828\n",
      "Epoch 1163/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17618.3633 - val_loss: 125148.5938\n",
      "Epoch 1164/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17618.3574 - val_loss: 125148.2500\n",
      "Epoch 1165/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17618.3535 - val_loss: 125147.9219\n",
      "Epoch 1166/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.3516 - val_loss: 125147.5938\n",
      "Epoch 1167/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17618.3477 - val_loss: 125147.2812\n",
      "Epoch 1168/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.3438 - val_loss: 125146.9375\n",
      "Epoch 1169/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.3418 - val_loss: 125146.6875\n",
      "Epoch 1170/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17618.3398 - val_loss: 125146.2734\n",
      "Epoch 1171/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17618.3398 - val_loss: 125146.0781\n",
      "Epoch 1172/2000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 17618.3438 - val_loss: 125145.6172\n",
      "Epoch 1173/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.3516 - val_loss: 125145.4531\n",
      "Epoch 1174/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17618.3555 - val_loss: 125144.9688\n",
      "Epoch 1175/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.3535 - val_loss: 125144.7812\n",
      "Epoch 1176/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17618.3379 - val_loss: 125144.3594\n",
      "Epoch 1177/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.3145 - val_loss: 125144.0469\n",
      "Epoch 1178/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.3086 - val_loss: 125143.8125\n",
      "Epoch 1179/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.3164 - val_loss: 125143.3750\n",
      "Epoch 1180/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.3203 - val_loss: 125143.1719\n",
      "Epoch 1181/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.3125 - val_loss: 125142.7500\n",
      "Epoch 1182/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17618.3008 - val_loss: 125142.5078\n",
      "Epoch 1183/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.3066 - val_loss: 125142.1875\n",
      "Epoch 1184/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17618.3184 - val_loss: 125141.9062\n",
      "Epoch 1185/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17618.3301 - val_loss: 125141.5625\n",
      "Epoch 1186/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.3281 - val_loss: 125141.2891\n",
      "Epoch 1187/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17618.3164 - val_loss: 125140.8516\n",
      "Epoch 1188/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17618.2949 - val_loss: 125140.6406\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1189/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.2891 - val_loss: 125140.3125\n",
      "Epoch 1190/2000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 17618.3027 - val_loss: 125139.9688\n",
      "Epoch 1191/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.3203 - val_loss: 125139.7500\n",
      "Epoch 1192/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.3184 - val_loss: 125139.3125\n",
      "Epoch 1193/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.2910 - val_loss: 125139.0234\n",
      "Epoch 1194/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.2676 - val_loss: 125138.7500\n",
      "Epoch 1195/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17618.2637 - val_loss: 125138.3750\n",
      "Epoch 1196/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.2754 - val_loss: 125138.1797\n",
      "Epoch 1197/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.2754 - val_loss: 125137.7812\n",
      "Epoch 1198/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17618.2754 - val_loss: 125137.5000\n",
      "Epoch 1199/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.2754 - val_loss: 125137.2344\n",
      "Epoch 1200/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17618.2852 - val_loss: 125136.9062\n",
      "Epoch 1201/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17618.2910 - val_loss: 125136.5859\n",
      "Epoch 1202/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.2754 - val_loss: 125136.2812\n",
      "Epoch 1203/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.2383 - val_loss: 125135.8828\n",
      "Epoch 1204/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.2285 - val_loss: 125135.7266\n",
      "Epoch 1205/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.2480 - val_loss: 125135.2969\n",
      "Epoch 1206/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.2598 - val_loss: 125135.1250\n",
      "Epoch 1207/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.2520 - val_loss: 125134.6641\n",
      "Epoch 1208/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17618.2441 - val_loss: 125134.5312\n",
      "Epoch 1209/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.2500 - val_loss: 125134.0703\n",
      "Epoch 1210/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17618.2461 - val_loss: 125133.8750\n",
      "Epoch 1211/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.2148 - val_loss: 125133.4844\n",
      "Epoch 1212/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.1953 - val_loss: 125133.1719\n",
      "Epoch 1213/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.2090 - val_loss: 125132.9766\n",
      "Epoch 1214/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17618.2207 - val_loss: 125132.5391\n",
      "Epoch 1215/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.2070 - val_loss: 125132.2969\n",
      "Epoch 1216/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.1875 - val_loss: 125132.0000\n",
      "Epoch 1217/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.1914 - val_loss: 125131.6406\n",
      "Epoch 1218/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17618.2012 - val_loss: 125131.5000\n",
      "Epoch 1219/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.2051 - val_loss: 125131.0234\n",
      "Epoch 1220/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.2070 - val_loss: 125130.8984\n",
      "Epoch 1221/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.2168 - val_loss: 125130.5312\n",
      "Epoch 1222/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.2227 - val_loss: 125130.2031\n",
      "Epoch 1223/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.2090 - val_loss: 125130.0000\n",
      "Epoch 1224/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17618.1934 - val_loss: 125129.5234\n",
      "Epoch 1225/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.2031 - val_loss: 125129.5156\n",
      "Epoch 1226/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17618.2324 - val_loss: 125128.9688\n",
      "Epoch 1227/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.2441 - val_loss: 125128.9062\n",
      "Epoch 1228/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17618.2129 - val_loss: 125128.3594\n",
      "Epoch 1229/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.1719 - val_loss: 125128.1250\n",
      "Epoch 1230/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.1523 - val_loss: 125127.8906\n",
      "Epoch 1231/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.1660 - val_loss: 125127.5000\n",
      "Epoch 1232/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17618.1719 - val_loss: 125127.3281\n",
      "Epoch 1233/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17618.1641 - val_loss: 125126.9219\n",
      "Epoch 1234/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17618.1465 - val_loss: 125126.6328\n",
      "Epoch 1235/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.1387 - val_loss: 125126.4219\n",
      "Epoch 1236/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.1387 - val_loss: 125126.0000\n",
      "Epoch 1237/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.1387 - val_loss: 125125.8594\n",
      "Epoch 1238/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.1367 - val_loss: 125125.4219\n",
      "Epoch 1239/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.1328 - val_loss: 125125.2734\n",
      "Epoch 1240/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.1289 - val_loss: 125124.8594\n",
      "Epoch 1241/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.1348 - val_loss: 125124.7500\n",
      "Epoch 1242/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.1465 - val_loss: 125124.2344\n",
      "Epoch 1243/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.1504 - val_loss: 125124.2031\n",
      "Epoch 1244/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17618.1387 - val_loss: 125123.6719\n",
      "Epoch 1245/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.1211 - val_loss: 125123.5078\n",
      "Epoch 1246/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.1074 - val_loss: 125123.2031\n",
      "Epoch 1247/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.1035 - val_loss: 125122.8047\n",
      "Epoch 1248/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.1094 - val_loss: 125122.7656\n",
      "Epoch 1249/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.1152 - val_loss: 125122.2266\n",
      "Epoch 1250/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.1152 - val_loss: 125122.1562\n",
      "Epoch 1251/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17618.1113 - val_loss: 125121.7188\n",
      "Epoch 1252/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17618.1152 - val_loss: 125121.5312\n",
      "Epoch 1253/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.1289 - val_loss: 125121.1562\n",
      "Epoch 1254/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.1250 - val_loss: 125121.0781\n",
      "Epoch 1255/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17618.1113 - val_loss: 125120.3906\n",
      "Epoch 1256/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.1094 - val_loss: 125120.7422\n",
      "Epoch 1257/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.1348 - val_loss: 125119.7344\n",
      "Epoch 1258/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.1543 - val_loss: 125120.2031\n",
      "Epoch 1259/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.1367 - val_loss: 125119.2500\n",
      "Epoch 1260/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17618.0938 - val_loss: 125119.2969\n",
      "Epoch 1261/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17618.0723 - val_loss: 125119.0312\n",
      "Epoch 1262/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.0879 - val_loss: 125118.5469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1263/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.1211 - val_loss: 125118.5469\n",
      "Epoch 1264/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17618.1289 - val_loss: 125118.0938\n",
      "Epoch 1265/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17618.1016 - val_loss: 125117.7109\n",
      "Epoch 1266/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.0664 - val_loss: 125117.6875\n",
      "Epoch 1267/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.0586 - val_loss: 125117.0938\n",
      "Epoch 1268/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.0762 - val_loss: 125117.1562\n",
      "Epoch 1269/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17618.0918 - val_loss: 125116.6641\n",
      "Epoch 1270/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.0879 - val_loss: 125116.4375\n",
      "Epoch 1271/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17618.0723 - val_loss: 125116.2031\n",
      "Epoch 1272/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17618.0684 - val_loss: 125115.8438\n",
      "Epoch 1273/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17618.0605 - val_loss: 125115.6094\n",
      "Epoch 1274/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.0449 - val_loss: 125115.3359\n",
      "Epoch 1275/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.0352 - val_loss: 125115.0000\n",
      "Epoch 1276/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17618.0352 - val_loss: 125114.7812\n",
      "Epoch 1277/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.0430 - val_loss: 125114.5078\n",
      "Epoch 1278/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.0391 - val_loss: 125114.1797\n",
      "Epoch 1279/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.0215 - val_loss: 125113.9844\n",
      "Epoch 1280/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.0117 - val_loss: 125113.6562\n",
      "Epoch 1281/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17618.0176 - val_loss: 125113.3906\n",
      "Epoch 1282/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17618.0254 - val_loss: 125113.1484\n",
      "Epoch 1283/2000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 17618.0117 - val_loss: 125112.7891\n",
      "Epoch 1284/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17618.0000 - val_loss: 125112.5938\n",
      "Epoch 1285/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.0020 - val_loss: 125112.3203\n",
      "Epoch 1286/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17618.0078 - val_loss: 125112.0078\n",
      "Epoch 1287/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17618.0000 - val_loss: 125111.7812\n",
      "Epoch 1288/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.9941 - val_loss: 125111.4609\n",
      "Epoch 1289/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.9902 - val_loss: 125111.2344\n",
      "Epoch 1290/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.9941 - val_loss: 125110.9766\n",
      "Epoch 1291/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.9902 - val_loss: 125110.6719\n",
      "Epoch 1292/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.9824 - val_loss: 125110.4219\n",
      "Epoch 1293/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.9785 - val_loss: 125110.1484\n",
      "Epoch 1294/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.9805 - val_loss: 125109.8828\n",
      "Epoch 1295/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.9785 - val_loss: 125109.6406\n",
      "Epoch 1296/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.9746 - val_loss: 125109.3438\n",
      "Epoch 1297/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.9688 - val_loss: 125109.0938\n",
      "Epoch 1298/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.9688 - val_loss: 125108.8359\n",
      "Epoch 1299/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.9688 - val_loss: 125108.5469\n",
      "Epoch 1300/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.9648 - val_loss: 125108.2812\n",
      "Epoch 1301/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.9629 - val_loss: 125108.0312\n",
      "Epoch 1302/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.9629 - val_loss: 125107.7812\n",
      "Epoch 1303/2000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 17617.9629 - val_loss: 125107.5234\n",
      "Epoch 1304/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.9629 - val_loss: 125107.2656\n",
      "Epoch 1305/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.9648 - val_loss: 125107.0078\n",
      "Epoch 1306/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.9746 - val_loss: 125106.7656\n",
      "Epoch 1307/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.9863 - val_loss: 125106.5156\n",
      "Epoch 1308/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17618.0000 - val_loss: 125106.2656\n",
      "Epoch 1309/2000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 17618.0039 - val_loss: 125106.0000\n",
      "Epoch 1310/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.9941 - val_loss: 125105.7109\n",
      "Epoch 1311/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.9863 - val_loss: 125105.4844\n",
      "Epoch 1312/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.9844 - val_loss: 125105.1875\n",
      "Epoch 1313/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.9766 - val_loss: 125104.9375\n",
      "Epoch 1314/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.9551 - val_loss: 125104.6562\n",
      "Epoch 1315/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.9453 - val_loss: 125104.3906\n",
      "Epoch 1316/2000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 17617.9492 - val_loss: 125104.1562\n",
      "Epoch 1317/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.9590 - val_loss: 125103.8906\n",
      "Epoch 1318/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.9473 - val_loss: 125103.6250\n",
      "Epoch 1319/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.9355 - val_loss: 125103.3750\n",
      "Epoch 1320/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.9375 - val_loss: 125103.1328\n",
      "Epoch 1321/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.9551 - val_loss: 125102.8984\n",
      "Epoch 1322/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.9590 - val_loss: 125102.6172\n",
      "Epoch 1323/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.9551 - val_loss: 125102.3750\n",
      "Epoch 1324/2000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 17617.9492 - val_loss: 125102.0938\n",
      "Epoch 1325/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.9414 - val_loss: 125101.8438\n",
      "Epoch 1326/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.9238 - val_loss: 125101.5469\n",
      "Epoch 1327/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.9121 - val_loss: 125101.3750\n",
      "Epoch 1328/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.9199 - val_loss: 125101.0391\n",
      "Epoch 1329/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.9316 - val_loss: 125100.8984\n",
      "Epoch 1330/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.9277 - val_loss: 125100.5000\n",
      "Epoch 1331/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.9160 - val_loss: 125100.4375\n",
      "Epoch 1332/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.9199 - val_loss: 125100.0000\n",
      "Epoch 1333/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.9375 - val_loss: 125099.9844\n",
      "Epoch 1334/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.9414 - val_loss: 125099.4609\n",
      "Epoch 1335/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.9277 - val_loss: 125099.3906\n",
      "Epoch 1336/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.9043 - val_loss: 125099.0469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1337/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.8945 - val_loss: 125098.7812\n",
      "Epoch 1338/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.9004 - val_loss: 125098.6641\n",
      "Epoch 1339/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.9023 - val_loss: 125098.2266\n",
      "Epoch 1340/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.9004 - val_loss: 125098.1406\n",
      "Epoch 1341/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.8926 - val_loss: 125097.7969\n",
      "Epoch 1342/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.8848 - val_loss: 125097.5391\n",
      "Epoch 1343/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.8828 - val_loss: 125097.4141\n",
      "Epoch 1344/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.8848 - val_loss: 125097.0156\n",
      "Epoch 1345/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.8828 - val_loss: 125096.8594\n",
      "Epoch 1346/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.8750 - val_loss: 125096.5781\n",
      "Epoch 1347/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.8730 - val_loss: 125096.2812\n",
      "Epoch 1348/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.8711 - val_loss: 125096.1484\n",
      "Epoch 1349/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.8730 - val_loss: 125095.7891\n",
      "Epoch 1350/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.8691 - val_loss: 125095.6094\n",
      "Epoch 1351/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.8652 - val_loss: 125095.3750\n",
      "Epoch 1352/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.8633 - val_loss: 125095.0625\n",
      "Epoch 1353/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.8633 - val_loss: 125094.9141\n",
      "Epoch 1354/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.8613 - val_loss: 125094.6094\n",
      "Epoch 1355/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.8574 - val_loss: 125094.3750\n",
      "Epoch 1356/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.8535 - val_loss: 125094.1719\n",
      "Epoch 1357/2000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 17617.8535 - val_loss: 125093.8594\n",
      "Epoch 1358/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.8535 - val_loss: 125093.6719\n",
      "Epoch 1359/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.8516 - val_loss: 125093.4219\n",
      "Epoch 1360/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.8477 - val_loss: 125093.1562\n",
      "Epoch 1361/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.8477 - val_loss: 125092.9609\n",
      "Epoch 1362/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.8438 - val_loss: 125092.6562\n",
      "Epoch 1363/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.8418 - val_loss: 125092.4688\n",
      "Epoch 1364/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.8398 - val_loss: 125092.2188\n",
      "Epoch 1365/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.8379 - val_loss: 125091.9609\n",
      "Epoch 1366/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.8379 - val_loss: 125091.7578\n",
      "Epoch 1367/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.8379 - val_loss: 125091.4688\n",
      "Epoch 1368/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.8379 - val_loss: 125091.2578\n",
      "Epoch 1369/2000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 17617.8320 - val_loss: 125091.0469\n",
      "Epoch 1370/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.8301 - val_loss: 125090.7422\n",
      "Epoch 1371/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.8301 - val_loss: 125090.5781\n",
      "Epoch 1372/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.8301 - val_loss: 125090.2812\n",
      "Epoch 1373/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.8301 - val_loss: 125090.1172\n",
      "Epoch 1374/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.8340 - val_loss: 125089.8281\n",
      "Epoch 1375/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.8398 - val_loss: 125089.6875\n",
      "Epoch 1376/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.8516 - val_loss: 125089.3594\n",
      "Epoch 1377/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.8594 - val_loss: 125089.2422\n",
      "Epoch 1378/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.8652 - val_loss: 125088.8906\n",
      "Epoch 1379/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.8555 - val_loss: 125088.7422\n",
      "Epoch 1380/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.8516 - val_loss: 125088.5000\n",
      "Epoch 1381/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.8652 - val_loss: 125088.2656\n",
      "Epoch 1382/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.8887 - val_loss: 125088.1094\n",
      "Epoch 1383/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.9004 - val_loss: 125087.7656\n",
      "Epoch 1384/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.8691 - val_loss: 125087.5469\n",
      "Epoch 1385/2000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 17617.8203 - val_loss: 125087.2656\n",
      "Epoch 1386/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.8066 - val_loss: 125087.0703\n",
      "Epoch 1387/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.8301 - val_loss: 125086.8516\n",
      "Epoch 1388/2000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 17617.8477 - val_loss: 125086.6328\n",
      "Epoch 1389/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.8340 - val_loss: 125086.2969\n",
      "Epoch 1390/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.8105 - val_loss: 125086.2344\n",
      "Epoch 1391/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.8086 - val_loss: 125085.7812\n",
      "Epoch 1392/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.8301 - val_loss: 125085.8750\n",
      "Epoch 1393/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.8398 - val_loss: 125085.3047\n",
      "Epoch 1394/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.8379 - val_loss: 125085.4062\n",
      "Epoch 1395/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.8281 - val_loss: 125084.8750\n",
      "Epoch 1396/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.8223 - val_loss: 125084.8750\n",
      "Epoch 1397/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.8086 - val_loss: 125084.5000\n",
      "Epoch 1398/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.7949 - val_loss: 125084.3359\n",
      "Epoch 1399/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.7930 - val_loss: 125084.1094\n",
      "Epoch 1400/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.8027 - val_loss: 125083.8984\n",
      "Epoch 1401/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.8086 - val_loss: 125083.5469\n",
      "Epoch 1402/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.7988 - val_loss: 125083.6016\n",
      "Epoch 1403/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.7930 - val_loss: 125082.8828\n",
      "Epoch 1404/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.8066 - val_loss: 125083.4141\n",
      "Epoch 1405/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.8242 - val_loss: 125082.2812\n",
      "Epoch 1406/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.8398 - val_loss: 125083.1406\n",
      "Epoch 1407/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.8535 - val_loss: 125081.8281\n",
      "Epoch 1408/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.8691 - val_loss: 125082.6250\n",
      "Epoch 1409/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.8730 - val_loss: 125081.5938\n",
      "Epoch 1410/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.8652 - val_loss: 125081.8828\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1411/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.8496 - val_loss: 125081.3906\n",
      "Epoch 1412/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.8477 - val_loss: 125081.2812\n",
      "Epoch 1413/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.8770 - val_loss: 125081.0469\n",
      "Epoch 1414/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.9004 - val_loss: 125080.9375\n",
      "Epoch 1415/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.8770 - val_loss: 125080.3906\n",
      "Epoch 1416/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.8242 - val_loss: 125080.5391\n",
      "Epoch 1417/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.7988 - val_loss: 125079.9062\n",
      "Epoch 1418/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.8066 - val_loss: 125079.9922\n",
      "Epoch 1419/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.8184 - val_loss: 125079.6719\n",
      "Epoch 1420/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.8027 - val_loss: 125079.3125\n",
      "Epoch 1421/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.7812 - val_loss: 125079.3438\n",
      "Epoch 1422/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.7832 - val_loss: 125078.9375\n",
      "Epoch 1423/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.7910 - val_loss: 125078.7031\n",
      "Epoch 1424/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.7891 - val_loss: 125078.7656\n",
      "Epoch 1425/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.7793 - val_loss: 125077.9844\n",
      "Epoch 1426/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.7852 - val_loss: 125078.5391\n",
      "Epoch 1427/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.8027 - val_loss: 125077.5859\n",
      "Epoch 1428/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.8066 - val_loss: 125077.9219\n",
      "Epoch 1429/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.7891 - val_loss: 125077.3750\n",
      "Epoch 1430/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.7754 - val_loss: 125077.2188\n",
      "Epoch 1431/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.7773 - val_loss: 125077.1406\n",
      "Epoch 1432/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.7891 - val_loss: 125076.7656\n",
      "Epoch 1433/2000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 17617.8066 - val_loss: 125076.6797\n",
      "Epoch 1434/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.8066 - val_loss: 125076.4219\n",
      "Epoch 1435/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.7852 - val_loss: 125076.1406\n",
      "Epoch 1436/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.7676 - val_loss: 125076.0156\n",
      "Epoch 1437/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.7754 - val_loss: 125075.8125\n",
      "Epoch 1438/2000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 17617.7949 - val_loss: 125075.5078\n",
      "Epoch 1439/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.7910 - val_loss: 125075.3984\n",
      "Epoch 1440/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.7539 - val_loss: 125075.0156\n",
      "Epoch 1441/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.7285 - val_loss: 125074.8906\n",
      "Epoch 1442/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.7441 - val_loss: 125074.7422\n",
      "Epoch 1443/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.7656 - val_loss: 125074.4062\n",
      "Epoch 1444/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.7617 - val_loss: 125074.3281\n",
      "Epoch 1445/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.7441 - val_loss: 125073.9844\n",
      "Epoch 1446/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.7441 - val_loss: 125073.8516\n",
      "Epoch 1447/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.7637 - val_loss: 125073.6719\n",
      "Epoch 1448/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.7754 - val_loss: 125073.3750\n",
      "Epoch 1449/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.7578 - val_loss: 125073.2266\n",
      "Epoch 1450/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.7324 - val_loss: 125072.9531\n",
      "Epoch 1451/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.7227 - val_loss: 125072.7500\n",
      "Epoch 1452/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.7285 - val_loss: 125072.5938\n",
      "Epoch 1453/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.7324 - val_loss: 125072.3281\n",
      "Epoch 1454/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.7285 - val_loss: 125072.1562\n",
      "Epoch 1455/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.7207 - val_loss: 125071.9219\n",
      "Epoch 1456/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.7148 - val_loss: 125071.6875\n",
      "Epoch 1457/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.7148 - val_loss: 125071.5156\n",
      "Epoch 1458/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.7168 - val_loss: 125071.2969\n",
      "Epoch 1459/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.7129 - val_loss: 125071.1094\n",
      "Epoch 1460/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.7129 - val_loss: 125070.8984\n",
      "Epoch 1461/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.7070 - val_loss: 125070.6641\n",
      "Epoch 1462/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.7051 - val_loss: 125070.4844\n",
      "Epoch 1463/2000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 17617.7051 - val_loss: 125070.2500\n",
      "Epoch 1464/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.7031 - val_loss: 125070.0547\n",
      "Epoch 1465/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.6992 - val_loss: 125069.8750\n",
      "Epoch 1466/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.6973 - val_loss: 125069.6328\n",
      "Epoch 1467/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.6973 - val_loss: 125069.4375\n",
      "Epoch 1468/2000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 17617.6973 - val_loss: 125069.2266\n",
      "Epoch 1469/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.6953 - val_loss: 125069.0391\n",
      "Epoch 1470/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.6914 - val_loss: 125068.8359\n",
      "Epoch 1471/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.6934 - val_loss: 125068.6250\n",
      "Epoch 1472/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.6914 - val_loss: 125068.4297\n",
      "Epoch 1473/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.6895 - val_loss: 125068.2188\n",
      "Epoch 1474/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.6875 - val_loss: 125068.0469\n",
      "Epoch 1475/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.6875 - val_loss: 125067.8125\n",
      "Epoch 1476/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.6836 - val_loss: 125067.6250\n",
      "Epoch 1477/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.6836 - val_loss: 125067.4062\n",
      "Epoch 1478/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.6836 - val_loss: 125067.2266\n",
      "Epoch 1479/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.6816 - val_loss: 125066.9922\n",
      "Epoch 1480/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.6816 - val_loss: 125066.8672\n",
      "Epoch 1481/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.6836 - val_loss: 125066.5938\n",
      "Epoch 1482/2000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 17617.6875 - val_loss: 125066.5000\n",
      "Epoch 1483/2000\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 17617.6914 - val_loss: 125066.1562\n",
      "Epoch 1484/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.6992 - val_loss: 125066.1172\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1485/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.7090 - val_loss: 125065.7656\n",
      "Epoch 1486/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.7070 - val_loss: 125065.7031\n",
      "Epoch 1487/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.6934 - val_loss: 125065.3984\n",
      "Epoch 1488/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.6758 - val_loss: 125065.2188\n",
      "Epoch 1489/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.6699 - val_loss: 125065.0703\n",
      "Epoch 1490/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.6758 - val_loss: 125064.7969\n",
      "Epoch 1491/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.6816 - val_loss: 125064.7031\n",
      "Epoch 1492/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.6816 - val_loss: 125064.4219\n",
      "Epoch 1493/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.6699 - val_loss: 125064.2344\n",
      "Epoch 1494/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.6641 - val_loss: 125064.0781\n",
      "Epoch 1495/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.6660 - val_loss: 125063.8203\n",
      "Epoch 1496/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.6699 - val_loss: 125063.6953\n",
      "Epoch 1497/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.6660 - val_loss: 125063.4375\n",
      "Epoch 1498/2000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 17617.6602 - val_loss: 125063.2578\n",
      "Epoch 1499/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.6582 - val_loss: 125063.0938\n",
      "Epoch 1500/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.6641 - val_loss: 125062.8438\n",
      "Epoch 1501/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.6621 - val_loss: 125062.6875\n",
      "Epoch 1502/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.6562 - val_loss: 125062.4922\n",
      "Epoch 1503/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.6543 - val_loss: 125062.2812\n",
      "Epoch 1504/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.6543 - val_loss: 125062.1250\n",
      "Epoch 1505/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.6562 - val_loss: 125061.8594\n",
      "Epoch 1506/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.6543 - val_loss: 125061.7109\n",
      "Epoch 1507/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.6504 - val_loss: 125061.5312\n",
      "Epoch 1508/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.6504 - val_loss: 125061.3125\n",
      "Epoch 1509/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.6504 - val_loss: 125061.1406\n",
      "Epoch 1510/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.6504 - val_loss: 125060.9375\n",
      "Epoch 1511/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.6504 - val_loss: 125060.7344\n",
      "Epoch 1512/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.6504 - val_loss: 125060.6094\n",
      "Epoch 1513/2000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 17617.6562 - val_loss: 125060.3516\n",
      "Epoch 1514/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.6641 - val_loss: 125060.2656\n",
      "Epoch 1515/2000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 17617.6777 - val_loss: 125060.0000\n",
      "Epoch 1516/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.6973 - val_loss: 125059.9375\n",
      "Epoch 1517/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.7188 - val_loss: 125059.6719\n",
      "Epoch 1518/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.7324 - val_loss: 125059.5391\n",
      "Epoch 1519/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.7227 - val_loss: 125059.2500\n",
      "Epoch 1520/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.6895 - val_loss: 125059.0625\n",
      "Epoch 1521/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.6660 - val_loss: 125058.8906\n",
      "Epoch 1522/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.6621 - val_loss: 125058.6875\n",
      "Epoch 1523/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.6699 - val_loss: 125058.5234\n",
      "Epoch 1524/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.6738 - val_loss: 125058.3203\n",
      "Epoch 1525/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.6680 - val_loss: 125058.1250\n",
      "Epoch 1526/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.6660 - val_loss: 125058.0391\n",
      "Epoch 1527/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.6816 - val_loss: 125057.7031\n",
      "Epoch 1528/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.6973 - val_loss: 125057.7422\n",
      "Epoch 1529/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.7031 - val_loss: 125057.2969\n",
      "Epoch 1530/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.6973 - val_loss: 125057.3594\n",
      "Epoch 1531/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.6914 - val_loss: 125056.9688\n",
      "Epoch 1532/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.7012 - val_loss: 125056.9844\n",
      "Epoch 1533/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.7148 - val_loss: 125056.6562\n",
      "Epoch 1534/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.7148 - val_loss: 125056.5625\n",
      "Epoch 1535/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.7051 - val_loss: 125056.2891\n",
      "Epoch 1536/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.6953 - val_loss: 125056.1406\n",
      "Epoch 1537/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.7031 - val_loss: 125056.0000\n",
      "Epoch 1538/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.7266 - val_loss: 125055.7969\n",
      "Epoch 1539/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.7441 - val_loss: 125055.6797\n",
      "Epoch 1540/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.7344 - val_loss: 125055.3750\n",
      "Epoch 1541/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.7227 - val_loss: 125055.3438\n",
      "Epoch 1542/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.7188 - val_loss: 125054.9531\n",
      "Epoch 1543/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.7285 - val_loss: 125055.0625\n",
      "Epoch 1544/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.7324 - val_loss: 125054.5312\n",
      "Epoch 1545/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.7148 - val_loss: 125054.6094\n",
      "Epoch 1546/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.6855 - val_loss: 125054.2188\n",
      "Epoch 1547/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.6660 - val_loss: 125054.0625\n",
      "Epoch 1548/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.6641 - val_loss: 125054.0312\n",
      "Epoch 1549/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.6680 - val_loss: 125053.5938\n",
      "Epoch 1550/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.6699 - val_loss: 125053.7031\n",
      "Epoch 1551/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.6660 - val_loss: 125053.2969\n",
      "Epoch 1552/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.6660 - val_loss: 125053.2422\n",
      "Epoch 1553/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.6758 - val_loss: 125053.0469\n",
      "Epoch 1554/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.6758 - val_loss: 125052.8516\n",
      "Epoch 1555/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.6602 - val_loss: 125052.5938\n",
      "Epoch 1556/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.6445 - val_loss: 125052.6250\n",
      "Epoch 1557/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.6504 - val_loss: 125052.0859\n",
      "Epoch 1558/2000\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 17617.6680 - val_loss: 125052.3750\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1559/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.6719 - val_loss: 125051.7422\n",
      "Epoch 1560/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.6504 - val_loss: 125051.8594\n",
      "Epoch 1561/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.6230 - val_loss: 125051.5312\n",
      "Epoch 1562/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.6191 - val_loss: 125051.3672\n",
      "Epoch 1563/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.6348 - val_loss: 125051.2344\n",
      "Epoch 1564/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.6543 - val_loss: 125051.1719\n",
      "Epoch 1565/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.6602 - val_loss: 125050.6719\n",
      "Epoch 1566/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.6660 - val_loss: 125051.0938\n",
      "Epoch 1567/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.6816 - val_loss: 125050.1562\n",
      "Epoch 1568/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.6895 - val_loss: 125050.8125\n",
      "Epoch 1569/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.6816 - val_loss: 125049.8594\n",
      "Epoch 1570/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.6602 - val_loss: 125050.2109\n",
      "Epoch 1571/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.6562 - val_loss: 125049.7578\n",
      "Epoch 1572/2000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 17617.6660 - val_loss: 125049.6484\n",
      "Epoch 1573/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.6699 - val_loss: 125049.6250\n",
      "Epoch 1574/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.6738 - val_loss: 125049.2344\n",
      "Epoch 1575/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.6895 - val_loss: 125049.3047\n",
      "Epoch 1576/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.7188 - val_loss: 125049.0156\n",
      "Epoch 1577/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.7266 - val_loss: 125048.7734\n",
      "Epoch 1578/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.6836 - val_loss: 125048.6562\n",
      "Epoch 1579/2000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 17617.6211 - val_loss: 125048.3125\n",
      "Epoch 1580/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.5977 - val_loss: 125048.2969\n",
      "Epoch 1581/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.6270 - val_loss: 125048.0938\n",
      "Epoch 1582/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.6523 - val_loss: 125047.8594\n",
      "Epoch 1583/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.6348 - val_loss: 125047.7656\n",
      "Epoch 1584/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.6035 - val_loss: 125047.4688\n",
      "Epoch 1585/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.5938 - val_loss: 125047.3750\n",
      "Epoch 1586/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.6113 - val_loss: 125047.2500\n",
      "Epoch 1587/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.6211 - val_loss: 125046.9531\n",
      "Epoch 1588/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.6113 - val_loss: 125046.9453\n",
      "Epoch 1589/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.6035 - val_loss: 125046.6172\n",
      "Epoch 1590/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.6191 - val_loss: 125046.5781\n",
      "Epoch 1591/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.6465 - val_loss: 125046.3750\n",
      "Epoch 1592/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.6543 - val_loss: 125046.2500\n",
      "Epoch 1593/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.6348 - val_loss: 125045.9219\n",
      "Epoch 1594/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.6191 - val_loss: 125046.0234\n",
      "Epoch 1595/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.6270 - val_loss: 125045.4766\n",
      "Epoch 1596/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.6523 - val_loss: 125045.8281\n",
      "Epoch 1597/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.6602 - val_loss: 125045.1406\n",
      "Epoch 1598/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.6309 - val_loss: 125045.2656\n",
      "Epoch 1599/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.5918 - val_loss: 125044.9688\n",
      "Epoch 1600/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.5840 - val_loss: 125044.7109\n",
      "Epoch 1601/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.5977 - val_loss: 125044.8281\n",
      "Epoch 1602/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.6035 - val_loss: 125044.3359\n",
      "Epoch 1603/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.5918 - val_loss: 125044.3828\n",
      "Epoch 1604/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.5840 - val_loss: 125044.1406\n",
      "Epoch 1605/2000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 17617.5840 - val_loss: 125043.8828\n",
      "Epoch 1606/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.5762 - val_loss: 125043.9062\n",
      "Epoch 1607/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.5742 - val_loss: 125043.5312\n",
      "Epoch 1608/2000\n",
      "1/1 [==============================] - 0s 48ms/step - loss: 17617.5762 - val_loss: 125043.5156\n",
      "Epoch 1609/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.5762 - val_loss: 125043.3125\n",
      "Epoch 1610/2000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 17617.5703 - val_loss: 125043.0625\n",
      "Epoch 1611/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.5625 - val_loss: 125043.0234\n",
      "Epoch 1612/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.5664 - val_loss: 125042.7344\n",
      "Epoch 1613/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.5703 - val_loss: 125042.6406\n",
      "Epoch 1614/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.5664 - val_loss: 125042.4844\n",
      "Epoch 1615/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.5586 - val_loss: 125042.2500\n",
      "Epoch 1616/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.5586 - val_loss: 125042.1953\n",
      "Epoch 1617/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.5605 - val_loss: 125041.9453\n",
      "Epoch 1618/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.5605 - val_loss: 125041.7969\n",
      "Epoch 1619/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.5566 - val_loss: 125041.6484\n",
      "Epoch 1620/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.5566 - val_loss: 125041.4375\n",
      "Epoch 1621/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.5566 - val_loss: 125041.3594\n",
      "Epoch 1622/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.5586 - val_loss: 125041.1562\n",
      "Epoch 1623/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.5586 - val_loss: 125041.0000\n",
      "Epoch 1624/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.5645 - val_loss: 125040.8594\n",
      "Epoch 1625/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.5762 - val_loss: 125040.6953\n",
      "Epoch 1626/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.5898 - val_loss: 125040.5703\n",
      "Epoch 1627/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.6055 - val_loss: 125040.4062\n",
      "Epoch 1628/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.6191 - val_loss: 125040.2188\n",
      "Epoch 1629/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.6035 - val_loss: 125040.0703\n",
      "Epoch 1630/2000\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 17617.5723 - val_loss: 125039.8203\n",
      "Epoch 1631/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.5488 - val_loss: 125039.7500\n",
      "Epoch 1632/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.5566 - val_loss: 125039.4922\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1633/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.5762 - val_loss: 125039.4844\n",
      "Epoch 1634/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.5957 - val_loss: 125039.1797\n",
      "Epoch 1635/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.5996 - val_loss: 125039.2344\n",
      "Epoch 1636/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.5996 - val_loss: 125038.8203\n",
      "Epoch 1637/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.6055 - val_loss: 125038.9453\n",
      "Epoch 1638/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.6113 - val_loss: 125038.4844\n",
      "Epoch 1639/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.5996 - val_loss: 125038.5000\n",
      "Epoch 1640/2000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 17617.5684 - val_loss: 125038.2578\n",
      "Epoch 1641/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.5508 - val_loss: 125037.9922\n",
      "Epoch 1642/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.5566 - val_loss: 125038.1562\n",
      "Epoch 1643/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.5762 - val_loss: 125037.5781\n",
      "Epoch 1644/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.5781 - val_loss: 125037.8594\n",
      "Epoch 1645/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.5625 - val_loss: 125037.2500\n",
      "Epoch 1646/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.5625 - val_loss: 125037.5156\n",
      "Epoch 1647/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.5801 - val_loss: 125037.0781\n",
      "Epoch 1648/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.5879 - val_loss: 125037.1250\n",
      "Epoch 1649/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.5703 - val_loss: 125036.7500\n",
      "Epoch 1650/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.5410 - val_loss: 125036.6562\n",
      "Epoch 1651/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.5352 - val_loss: 125036.5625\n",
      "Epoch 1652/2000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 17617.5488 - val_loss: 125036.2969\n",
      "Epoch 1653/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.5586 - val_loss: 125036.3047\n",
      "Epoch 1654/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.5488 - val_loss: 125035.9688\n",
      "Epoch 1655/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.5352 - val_loss: 125035.9141\n",
      "Epoch 1656/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.5371 - val_loss: 125035.7812\n",
      "Epoch 1657/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.5488 - val_loss: 125035.5312\n",
      "Epoch 1658/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.5605 - val_loss: 125035.5547\n",
      "Epoch 1659/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.5664 - val_loss: 125035.2031\n",
      "Epoch 1660/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.5684 - val_loss: 125035.1719\n",
      "Epoch 1661/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.5605 - val_loss: 125035.0000\n",
      "Epoch 1662/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.5469 - val_loss: 125034.7500\n",
      "Epoch 1663/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.5312 - val_loss: 125034.6953\n",
      "Epoch 1664/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.5312 - val_loss: 125034.5000\n",
      "Epoch 1665/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.5449 - val_loss: 125034.2969\n",
      "Epoch 1666/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.5566 - val_loss: 125034.3203\n",
      "Epoch 1667/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.5566 - val_loss: 125033.9062\n",
      "Epoch 1668/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.5566 - val_loss: 125034.0469\n",
      "Epoch 1669/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.5586 - val_loss: 125033.6719\n",
      "Epoch 1670/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.5605 - val_loss: 125033.6094\n",
      "Epoch 1671/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.5469 - val_loss: 125033.4609\n",
      "Epoch 1672/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.5312 - val_loss: 125033.1875\n",
      "Epoch 1673/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.5391 - val_loss: 125033.2422\n",
      "Epoch 1674/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.5645 - val_loss: 125032.9531\n",
      "Epoch 1675/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.5781 - val_loss: 125032.8672\n",
      "Epoch 1676/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.5625 - val_loss: 125032.6953\n",
      "Epoch 1677/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.5312 - val_loss: 125032.4531\n",
      "Epoch 1678/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.5195 - val_loss: 125032.4062\n",
      "Epoch 1679/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.5312 - val_loss: 125032.2188\n",
      "Epoch 1680/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.5469 - val_loss: 125032.0781\n",
      "Epoch 1681/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.5508 - val_loss: 125031.9688\n",
      "Epoch 1682/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.5449 - val_loss: 125031.7344\n",
      "Epoch 1683/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.5449 - val_loss: 125031.6875\n",
      "Epoch 1684/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.5488 - val_loss: 125031.4609\n",
      "Epoch 1685/2000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 17617.5410 - val_loss: 125031.2656\n",
      "Epoch 1686/2000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 17617.5215 - val_loss: 125031.2031\n",
      "Epoch 1687/2000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 17617.5078 - val_loss: 125030.9219\n",
      "Epoch 1688/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.5156 - val_loss: 125030.9453\n",
      "Epoch 1689/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.5273 - val_loss: 125030.6172\n",
      "Epoch 1690/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.5254 - val_loss: 125030.6250\n",
      "Epoch 1691/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.5137 - val_loss: 125030.2969\n",
      "Epoch 1692/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.5098 - val_loss: 125030.3906\n",
      "Epoch 1693/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.5215 - val_loss: 125029.9297\n",
      "Epoch 1694/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.5352 - val_loss: 125030.2500\n",
      "Epoch 1695/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.5410 - val_loss: 125029.4844\n",
      "Epoch 1696/2000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 17617.5449 - val_loss: 125030.1094\n",
      "Epoch 1697/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.5527 - val_loss: 125029.1250\n",
      "Epoch 1698/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.5586 - val_loss: 125029.8594\n",
      "Epoch 1699/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.5586 - val_loss: 125028.8750\n",
      "Epoch 1700/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.5625 - val_loss: 125029.4844\n",
      "Epoch 1701/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.5703 - val_loss: 125028.7109\n",
      "Epoch 1702/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.5723 - val_loss: 125029.0000\n",
      "Epoch 1703/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.5703 - val_loss: 125028.6094\n",
      "Epoch 1704/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.5566 - val_loss: 125028.4844\n",
      "Epoch 1705/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.5410 - val_loss: 125028.4688\n",
      "Epoch 1706/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.5293 - val_loss: 125028.0625\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1707/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.5254 - val_loss: 125028.1797\n",
      "Epoch 1708/2000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 17617.5215 - val_loss: 125027.8438\n",
      "Epoch 1709/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.5215 - val_loss: 125027.7656\n",
      "Epoch 1710/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.5254 - val_loss: 125027.7031\n",
      "Epoch 1711/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.5215 - val_loss: 125027.3594\n",
      "Epoch 1712/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.5254 - val_loss: 125027.4766\n",
      "Epoch 1713/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.5332 - val_loss: 125027.1250\n",
      "Epoch 1714/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.5469 - val_loss: 125027.1406\n",
      "Epoch 1715/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.5527 - val_loss: 125026.9062\n",
      "Epoch 1716/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.5430 - val_loss: 125026.7812\n",
      "Epoch 1717/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.5332 - val_loss: 125026.6172\n",
      "Epoch 1718/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.5293 - val_loss: 125026.5234\n",
      "Epoch 1719/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.5254 - val_loss: 125026.2656\n",
      "Epoch 1720/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.5176 - val_loss: 125026.2344\n",
      "Epoch 1721/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.5039 - val_loss: 125025.9375\n",
      "Epoch 1722/2000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 17617.5000 - val_loss: 125025.9531\n",
      "Epoch 1723/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.5078 - val_loss: 125025.6953\n",
      "Epoch 1724/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.5137 - val_loss: 125025.6719\n",
      "Epoch 1725/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.5137 - val_loss: 125025.3516\n",
      "Epoch 1726/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.5117 - val_loss: 125025.5000\n",
      "Epoch 1727/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.5254 - val_loss: 125024.9688\n",
      "Epoch 1728/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.5352 - val_loss: 125025.3438\n",
      "Epoch 1729/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.5352 - val_loss: 125024.6172\n",
      "Epoch 1730/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.5215 - val_loss: 125024.9688\n",
      "Epoch 1731/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.5098 - val_loss: 125024.4688\n",
      "Epoch 1732/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.5254 - val_loss: 125024.5859\n",
      "Epoch 1733/2000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 17617.5410 - val_loss: 125024.3203\n",
      "Epoch 1734/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.5312 - val_loss: 125024.1719\n",
      "Epoch 1735/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.4961 - val_loss: 125024.0000\n",
      "Epoch 1736/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.4785 - val_loss: 125023.9375\n",
      "Epoch 1737/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.4941 - val_loss: 125023.6797\n",
      "Epoch 1738/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.5156 - val_loss: 125023.7969\n",
      "Epoch 1739/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.5137 - val_loss: 125023.2656\n",
      "Epoch 1740/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.5000 - val_loss: 125023.5938\n",
      "Epoch 1741/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.5039 - val_loss: 125022.9531\n",
      "Epoch 1742/2000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 17617.5215 - val_loss: 125023.2969\n",
      "Epoch 1743/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.5176 - val_loss: 125022.7656\n",
      "Epoch 1744/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.4902 - val_loss: 125022.7734\n",
      "Epoch 1745/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.4688 - val_loss: 125022.6797\n",
      "Epoch 1746/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.4785 - val_loss: 125022.3750\n",
      "Epoch 1747/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.5000 - val_loss: 125022.4766\n",
      "Epoch 1748/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.5000 - val_loss: 125022.1797\n",
      "Epoch 1749/2000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 17617.4941 - val_loss: 125022.0156\n",
      "Epoch 1750/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.4961 - val_loss: 125022.1250\n",
      "Epoch 1751/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.5176 - val_loss: 125021.6484\n",
      "Epoch 1752/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.5352 - val_loss: 125021.9141\n",
      "Epoch 1753/2000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 17617.5293 - val_loss: 125021.4219\n",
      "Epoch 1754/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.5195 - val_loss: 125021.4609\n",
      "Epoch 1755/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.5215 - val_loss: 125021.3594\n",
      "Epoch 1756/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.5312 - val_loss: 125021.0547\n",
      "Epoch 1757/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.5410 - val_loss: 125021.1875\n",
      "Epoch 1758/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.5449 - val_loss: 125020.7500\n",
      "Epoch 1759/2000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 17617.5391 - val_loss: 125020.8438\n",
      "Epoch 1760/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.5312 - val_loss: 125020.5547\n",
      "Epoch 1761/2000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 17617.5312 - val_loss: 125020.5078\n",
      "Epoch 1762/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.5273 - val_loss: 125020.3203\n",
      "Epoch 1763/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.5312 - val_loss: 125020.2500\n",
      "Epoch 1764/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.5410 - val_loss: 125020.0312\n",
      "Epoch 1765/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.5488 - val_loss: 125019.9922\n",
      "Epoch 1766/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.5410 - val_loss: 125019.7656\n",
      "Epoch 1767/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.5195 - val_loss: 125019.5938\n",
      "Epoch 1768/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.5059 - val_loss: 125019.5938\n",
      "Epoch 1769/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.5098 - val_loss: 125019.2578\n",
      "Epoch 1770/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.5176 - val_loss: 125019.3594\n",
      "Epoch 1771/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.5039 - val_loss: 125018.9688\n",
      "Epoch 1772/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.4785 - val_loss: 125018.9375\n",
      "Epoch 1773/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.4668 - val_loss: 125018.8281\n",
      "Epoch 1774/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.4766 - val_loss: 125018.5781\n",
      "Epoch 1775/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.4863 - val_loss: 125018.6250\n",
      "Epoch 1776/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.4785 - val_loss: 125018.2891\n",
      "Epoch 1777/2000\n",
      "1/1 [==============================] - 0s 54ms/step - loss: 17617.4629 - val_loss: 125018.2578\n",
      "Epoch 1778/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.4570 - val_loss: 125018.1562\n",
      "Epoch 1779/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.4648 - val_loss: 125017.8906\n",
      "Epoch 1780/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.4688 - val_loss: 125017.9297\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1781/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.4629 - val_loss: 125017.6250\n",
      "Epoch 1782/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.4512 - val_loss: 125017.5938\n",
      "Epoch 1783/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.4551 - val_loss: 125017.4531\n",
      "Epoch 1784/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.4590 - val_loss: 125017.2969\n",
      "Epoch 1785/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.4629 - val_loss: 125017.1875\n",
      "Epoch 1786/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.4629 - val_loss: 125017.0781\n",
      "Epoch 1787/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.4668 - val_loss: 125016.8594\n",
      "Epoch 1788/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.4785 - val_loss: 125016.9531\n",
      "Epoch 1789/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.4902 - val_loss: 125016.5469\n",
      "Epoch 1790/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.4941 - val_loss: 125016.6797\n",
      "Epoch 1791/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.4883 - val_loss: 125016.3438\n",
      "Epoch 1792/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.4863 - val_loss: 125016.3125\n",
      "Epoch 1793/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.4941 - val_loss: 125016.2344\n",
      "Epoch 1794/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.5020 - val_loss: 125015.9844\n",
      "Epoch 1795/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.4980 - val_loss: 125015.9375\n",
      "Epoch 1796/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.4863 - val_loss: 125015.7422\n",
      "Epoch 1797/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.4727 - val_loss: 125015.5781\n",
      "Epoch 1798/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.4629 - val_loss: 125015.5156\n",
      "Epoch 1799/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.4570 - val_loss: 125015.2812\n",
      "Epoch 1800/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.4570 - val_loss: 125015.2500\n",
      "Epoch 1801/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.4551 - val_loss: 125015.0781\n",
      "Epoch 1802/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.4531 - val_loss: 125014.9375\n",
      "Epoch 1803/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.4473 - val_loss: 125014.8438\n",
      "Epoch 1804/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.4473 - val_loss: 125014.6484\n",
      "Epoch 1805/2000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 17617.4453 - val_loss: 125014.5469\n",
      "Epoch 1806/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.4453 - val_loss: 125014.4688\n",
      "Epoch 1807/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.4473 - val_loss: 125014.1875\n",
      "Epoch 1808/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.4473 - val_loss: 125014.2656\n",
      "Epoch 1809/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.4492 - val_loss: 125013.9219\n",
      "Epoch 1810/2000\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 17617.4570 - val_loss: 125014.0625\n",
      "Epoch 1811/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.4648 - val_loss: 125013.7031\n",
      "Epoch 1812/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.4688 - val_loss: 125013.7031\n",
      "Epoch 1813/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.4590 - val_loss: 125013.4844\n",
      "Epoch 1814/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.4414 - val_loss: 125013.3594\n",
      "Epoch 1815/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.4336 - val_loss: 125013.2969\n",
      "Epoch 1816/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.4453 - val_loss: 125013.1016\n",
      "Epoch 1817/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.4590 - val_loss: 125013.0938\n",
      "Epoch 1818/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.4688 - val_loss: 125012.8750\n",
      "Epoch 1819/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.4648 - val_loss: 125012.7656\n",
      "Epoch 1820/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.4570 - val_loss: 125012.6719\n",
      "Epoch 1821/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.4453 - val_loss: 125012.4219\n",
      "Epoch 1822/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.4336 - val_loss: 125012.4531\n",
      "Epoch 1823/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.4375 - val_loss: 125012.1094\n",
      "Epoch 1824/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.4453 - val_loss: 125012.2500\n",
      "Epoch 1825/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.4551 - val_loss: 125011.8750\n",
      "Epoch 1826/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.4629 - val_loss: 125012.0469\n",
      "Epoch 1827/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.4727 - val_loss: 125011.5938\n",
      "Epoch 1828/2000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 17617.4785 - val_loss: 125011.8359\n",
      "Epoch 1829/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.4824 - val_loss: 125011.3125\n",
      "Epoch 1830/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.4824 - val_loss: 125011.6250\n",
      "Epoch 1831/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.4902 - val_loss: 125011.1172\n",
      "Epoch 1832/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.5059 - val_loss: 125011.3281\n",
      "Epoch 1833/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.5098 - val_loss: 125010.9219\n",
      "Epoch 1834/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.4941 - val_loss: 125010.9297\n",
      "Epoch 1835/2000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 17617.4746 - val_loss: 125010.7969\n",
      "Epoch 1836/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.4766 - val_loss: 125010.6172\n",
      "Epoch 1837/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.5000 - val_loss: 125010.6094\n",
      "Epoch 1838/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.5098 - val_loss: 125010.3594\n",
      "Epoch 1839/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.4902 - val_loss: 125010.2422\n",
      "Epoch 1840/2000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 17617.4648 - val_loss: 125010.1797\n",
      "Epoch 1841/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.4668 - val_loss: 125009.9766\n",
      "Epoch 1842/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.5000 - val_loss: 125009.9922\n",
      "Epoch 1843/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.5195 - val_loss: 125009.7188\n",
      "Epoch 1844/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.4961 - val_loss: 125009.6250\n",
      "Epoch 1845/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.4629 - val_loss: 125009.5469\n",
      "Epoch 1846/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.4473 - val_loss: 125009.2812\n",
      "Epoch 1847/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.4512 - val_loss: 125009.3125\n",
      "Epoch 1848/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.4512 - val_loss: 125009.0547\n",
      "Epoch 1849/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.4414 - val_loss: 125008.9688\n",
      "Epoch 1850/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.4316 - val_loss: 125008.9219\n",
      "Epoch 1851/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.4336 - val_loss: 125008.6484\n",
      "Epoch 1852/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.4395 - val_loss: 125008.6562\n",
      "Epoch 1853/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.4316 - val_loss: 125008.4375\n",
      "Epoch 1854/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.4316 - val_loss: 125008.3516\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1855/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.4375 - val_loss: 125008.2891\n",
      "Epoch 1856/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.4512 - val_loss: 125008.0938\n",
      "Epoch 1857/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.4551 - val_loss: 125008.0156\n",
      "Epoch 1858/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.4453 - val_loss: 125007.8984\n",
      "Epoch 1859/2000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 17617.4277 - val_loss: 125007.6719\n",
      "Epoch 1860/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.4160 - val_loss: 125007.6875\n",
      "Epoch 1861/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.4180 - val_loss: 125007.4297\n",
      "Epoch 1862/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.4316 - val_loss: 125007.4375\n",
      "Epoch 1863/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.4414 - val_loss: 125007.2500\n",
      "Epoch 1864/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.4570 - val_loss: 125007.2188\n",
      "Epoch 1865/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.4688 - val_loss: 125007.0312\n",
      "Epoch 1866/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.4863 - val_loss: 125007.0234\n",
      "Epoch 1867/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.5020 - val_loss: 125006.7812\n",
      "Epoch 1868/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.5078 - val_loss: 125006.8047\n",
      "Epoch 1869/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.4941 - val_loss: 125006.4688\n",
      "Epoch 1870/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.4688 - val_loss: 125006.5156\n",
      "Epoch 1871/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.4512 - val_loss: 125006.2031\n",
      "Epoch 1872/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.4473 - val_loss: 125006.2969\n",
      "Epoch 1873/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.4512 - val_loss: 125005.9219\n",
      "Epoch 1874/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.4570 - val_loss: 125006.1406\n",
      "Epoch 1875/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.4629 - val_loss: 125005.6016\n",
      "Epoch 1876/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.4668 - val_loss: 125005.9688\n",
      "Epoch 1877/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.4629 - val_loss: 125005.3438\n",
      "Epoch 1878/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.4492 - val_loss: 125005.6484\n",
      "Epoch 1879/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.4453 - val_loss: 125005.2031\n",
      "Epoch 1880/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.4531 - val_loss: 125005.3594\n",
      "Epoch 1881/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.4629 - val_loss: 125005.0312\n",
      "Epoch 1882/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.4648 - val_loss: 125005.1406\n",
      "Epoch 1883/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.4668 - val_loss: 125004.7188\n",
      "Epoch 1884/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.4824 - val_loss: 125005.0938\n",
      "Epoch 1885/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.5156 - val_loss: 125004.3594\n",
      "Epoch 1886/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.5352 - val_loss: 125004.9609\n",
      "Epoch 1887/2000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 17617.5195 - val_loss: 125004.0625\n",
      "Epoch 1888/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.4863 - val_loss: 125004.6094\n",
      "Epoch 1889/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.4863 - val_loss: 125004.0156\n",
      "Epoch 1890/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.5215 - val_loss: 125004.2500\n",
      "Epoch 1891/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.5391 - val_loss: 125003.9297\n",
      "Epoch 1892/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.5039 - val_loss: 125003.7969\n",
      "Epoch 1893/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.4531 - val_loss: 125003.6875\n",
      "Epoch 1894/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.4473 - val_loss: 125003.6016\n",
      "Epoch 1895/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.4941 - val_loss: 125003.5156\n",
      "Epoch 1896/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.5254 - val_loss: 125003.3672\n",
      "Epoch 1897/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.4902 - val_loss: 125003.1719\n",
      "Epoch 1898/2000\n",
      "1/1 [==============================] - 0s 49ms/step - loss: 17617.4238 - val_loss: 125003.0234\n",
      "Epoch 1899/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.3965 - val_loss: 125002.9219\n",
      "Epoch 1900/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.4199 - val_loss: 125002.8750\n",
      "Epoch 1901/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.4414 - val_loss: 125002.6562\n",
      "Epoch 1902/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.4336 - val_loss: 125002.6562\n",
      "Epoch 1903/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.4102 - val_loss: 125002.3516\n",
      "Epoch 1904/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.4004 - val_loss: 125002.4375\n",
      "Epoch 1905/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.4062 - val_loss: 125002.1172\n",
      "Epoch 1906/2000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 17617.4160 - val_loss: 125002.2344\n",
      "Epoch 1907/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.4238 - val_loss: 125001.8516\n",
      "Epoch 1908/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.4219 - val_loss: 125002.0781\n",
      "Epoch 1909/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.4199 - val_loss: 125001.5547\n",
      "Epoch 1910/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.4141 - val_loss: 125001.8125\n",
      "Epoch 1911/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.4043 - val_loss: 125001.4062\n",
      "Epoch 1912/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.3926 - val_loss: 125001.3828\n",
      "Epoch 1913/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.3848 - val_loss: 125001.3828\n",
      "Epoch 1914/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.3848 - val_loss: 125001.0000\n",
      "Epoch 1915/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.3926 - val_loss: 125001.2109\n",
      "Epoch 1916/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.3926 - val_loss: 125000.8125\n",
      "Epoch 1917/2000\n",
      "1/1 [==============================] - 0s 41ms/step - loss: 17617.3828 - val_loss: 125000.7969\n",
      "Epoch 1918/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.3711 - val_loss: 125000.7656\n",
      "Epoch 1919/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.3750 - val_loss: 125000.4375\n",
      "Epoch 1920/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.3828 - val_loss: 125000.5781\n",
      "Epoch 1921/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.3809 - val_loss: 125000.2812\n",
      "Epoch 1922/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.3711 - val_loss: 125000.1875\n",
      "Epoch 1923/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.3691 - val_loss: 125000.1875\n",
      "Epoch 1924/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.3730 - val_loss: 124999.8906\n",
      "Epoch 1925/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.3789 - val_loss: 124999.9531\n",
      "Epoch 1926/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.3828 - val_loss: 124999.7500\n",
      "Epoch 1927/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.3848 - val_loss: 124999.6250\n",
      "Epoch 1928/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.3965 - val_loss: 124999.6562\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1929/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.4121 - val_loss: 124999.3828\n",
      "Epoch 1930/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.4199 - val_loss: 124999.4062\n",
      "Epoch 1931/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.4160 - val_loss: 124999.2188\n",
      "Epoch 1932/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.4043 - val_loss: 124999.0703\n",
      "Epoch 1933/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.3926 - val_loss: 124999.0078\n",
      "Epoch 1934/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.3848 - val_loss: 124998.8438\n",
      "Epoch 1935/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.3789 - val_loss: 124998.7344\n",
      "Epoch 1936/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.3750 - val_loss: 124998.6953\n",
      "Epoch 1937/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.3750 - val_loss: 124998.4062\n",
      "Epoch 1938/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.3770 - val_loss: 124998.5078\n",
      "Epoch 1939/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.3750 - val_loss: 124998.1641\n",
      "Epoch 1940/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.3711 - val_loss: 124998.3047\n",
      "Epoch 1941/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.3711 - val_loss: 124997.9219\n",
      "Epoch 1942/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.3750 - val_loss: 124998.1406\n",
      "Epoch 1943/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.3848 - val_loss: 124997.5938\n",
      "Epoch 1944/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.3965 - val_loss: 124998.1094\n",
      "Epoch 1945/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.4082 - val_loss: 124997.2266\n",
      "Epoch 1946/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.4238 - val_loss: 124998.0625\n",
      "Epoch 1947/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.4375 - val_loss: 124996.9531\n",
      "Epoch 1948/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.4414 - val_loss: 124997.7500\n",
      "Epoch 1949/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.4316 - val_loss: 124996.9531\n",
      "Epoch 1950/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.4180 - val_loss: 124997.1250\n",
      "Epoch 1951/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.4062 - val_loss: 124997.0859\n",
      "Epoch 1952/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.3945 - val_loss: 124996.5625\n",
      "Epoch 1953/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.3848 - val_loss: 124997.0000\n",
      "Epoch 1954/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.3770 - val_loss: 124996.3906\n",
      "Epoch 1955/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.3711 - val_loss: 124996.5625\n",
      "Epoch 1956/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.3750 - val_loss: 124996.4531\n",
      "Epoch 1957/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.3770 - val_loss: 124996.0312\n",
      "Epoch 1958/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.3711 - val_loss: 124996.4531\n",
      "Epoch 1959/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.3652 - val_loss: 124995.7266\n",
      "Epoch 1960/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.3691 - val_loss: 124996.1719\n",
      "Epoch 1961/2000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 17617.3750 - val_loss: 124995.6562\n",
      "Epoch 1962/2000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 17617.3828 - val_loss: 124995.8281\n",
      "Epoch 1963/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.3887 - val_loss: 124995.5625\n",
      "Epoch 1964/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.3867 - val_loss: 124995.5312\n",
      "Epoch 1965/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.3789 - val_loss: 124995.3203\n",
      "Epoch 1966/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.3633 - val_loss: 124995.2734\n",
      "Epoch 1967/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.3516 - val_loss: 124995.1094\n",
      "Epoch 1968/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.3516 - val_loss: 124995.0312\n",
      "Epoch 1969/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.3613 - val_loss: 124994.9453\n",
      "Epoch 1970/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.3691 - val_loss: 124994.7656\n",
      "Epoch 1971/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.3652 - val_loss: 124994.7422\n",
      "Epoch 1972/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.3652 - val_loss: 124994.6250\n",
      "Epoch 1973/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.3789 - val_loss: 124994.5000\n",
      "Epoch 1974/2000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 17617.4102 - val_loss: 124994.4844\n",
      "Epoch 1975/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.4414 - val_loss: 124994.3828\n",
      "Epoch 1976/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.4629 - val_loss: 124994.2344\n",
      "Epoch 1977/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.4629 - val_loss: 124994.2344\n",
      "Epoch 1978/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.4375 - val_loss: 124993.8594\n",
      "Epoch 1979/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.4082 - val_loss: 124993.9375\n",
      "Epoch 1980/2000\n",
      "1/1 [==============================] - 0s 47ms/step - loss: 17617.3848 - val_loss: 124993.7344\n",
      "Epoch 1981/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.3828 - val_loss: 124993.5469\n",
      "Epoch 1982/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.3887 - val_loss: 124993.7031\n",
      "Epoch 1983/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.3926 - val_loss: 124993.1719\n",
      "Epoch 1984/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.3926 - val_loss: 124993.5469\n",
      "Epoch 1985/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.3945 - val_loss: 124993.0000\n",
      "Epoch 1986/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.3965 - val_loss: 124993.2500\n",
      "Epoch 1987/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.3867 - val_loss: 124992.8359\n",
      "Epoch 1988/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.3711 - val_loss: 124992.9922\n",
      "Epoch 1989/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.3691 - val_loss: 124992.5625\n",
      "Epoch 1990/2000\n",
      "1/1 [==============================] - 0s 43ms/step - loss: 17617.3848 - val_loss: 124992.9844\n",
      "Epoch 1991/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.4102 - val_loss: 124992.1875\n",
      "Epoch 1992/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.4160 - val_loss: 124992.8750\n",
      "Epoch 1993/2000\n",
      "1/1 [==============================] - 0s 45ms/step - loss: 17617.4004 - val_loss: 124991.9219\n",
      "Epoch 1994/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.3906 - val_loss: 124992.5703\n",
      "Epoch 1995/2000\n",
      "1/1 [==============================] - 0s 44ms/step - loss: 17617.4023 - val_loss: 124991.9453\n",
      "Epoch 1996/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.4160 - val_loss: 124992.1094\n",
      "Epoch 1997/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.4141 - val_loss: 124991.8906\n",
      "Epoch 1998/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.3945 - val_loss: 124991.7656\n",
      "Epoch 1999/2000\n",
      "1/1 [==============================] - 0s 42ms/step - loss: 17617.3750 - val_loss: 124991.6719\n",
      "Epoch 2000/2000\n",
      "1/1 [==============================] - 0s 46ms/step - loss: 17617.3730 - val_loss: 124991.5781\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x29399efa0>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_optimizer = 'adam'\n",
    "loss_fun = 'mse'\n",
    "\n",
    "\n",
    "np.random.seed(1337)\n",
    "full_model = tf.keras.models.Sequential()\n",
    "full_model.add(tf.keras.layers.LSTM(units = 30, \n",
    "                                       activation=\"relu\", \n",
    "                                       return_sequences=False))\n",
    "        \n",
    "full_model.add(tf.keras.layers.Dense(label_width * num_cols))\n",
    "full_model.add(tf.keras.layers.Reshape([label_width,num_cols]))\n",
    "compile_and_fit(full_model, num_epochs, input_optimizer=model_optimizer, input_loss=loss_fun)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " lstm_1 (LSTM)               (None, 30)                1407720   \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 11700)             362700    \n",
      "                                                                 \n",
      " reshape_1 (Reshape)         (None, 1, 11700)          0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,770,420\n",
      "Trainable params: 1,770,420\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "full_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Store full model prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#1991-2001\n",
    "\n",
    "full_train_inputs = next(iter(train_ds))[0] #from 1991-2000\n",
    "full_train_labels = next(iter(train_ds))[1] #from 1992-2001\n",
    "full_train_predictions = full_model(full_train_inputs) #from 1992-2001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(9, 2, 11700), dtype=float32, numpy=\n",
       "array([[[2400., 2347., 2277., ...,  180.,   98.,   50.],\n",
       "        [2392., 2344., 2194., ...,  200.,  104.,   55.]],\n",
       "\n",
       "       [[2392., 2344., 2194., ...,  200.,  104.,   55.],\n",
       "        [2382., 2354., 2126., ...,  209.,  105.,   59.]],\n",
       "\n",
       "       [[2382., 2354., 2126., ...,  209.,  105.,   59.],\n",
       "        [2357., 2351., 2084., ...,  212.,  112.,   65.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[2242., 2327., 2054., ...,  316.,  123.,   79.],\n",
       "        [2171., 2338., 2051., ...,  358.,  142.,   88.]],\n",
       "\n",
       "       [[2171., 2338., 2051., ...,  358.,  142.,   88.],\n",
       "        [2132., 2348., 2039., ...,  389.,  163.,   95.]],\n",
       "\n",
       "       [[2132., 2348., 2039., ...,  389.,  163.,   95.],\n",
       "        [2107., 2335., 2029., ...,  439.,  190.,  103.]]], dtype=float32)>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_train_inputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(9, 1, 11700), dtype=float32, numpy=\n",
       "array([[[2382., 2354., 2126., ...,  209.,  105.,   59.]],\n",
       "\n",
       "       [[2357., 2351., 2084., ...,  212.,  112.,   65.]],\n",
       "\n",
       "       [[2318., 2357., 2055., ...,  247.,  116.,   69.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[2132., 2348., 2039., ...,  389.,  163.,   95.]],\n",
       "\n",
       "       [[2107., 2335., 2029., ...,  439.,  190.,  103.]],\n",
       "\n",
       "       [[2077., 2259., 2051., ...,  457.,  221.,  119.]]], dtype=float32)>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_train_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(9, 1, 11700), dtype=float32, numpy=\n",
       "array([[[2159.7605  , 2265.0527  , 1997.8512  , ...,  314.4326  ,\n",
       "          140.2392  ,   81.23108 ]],\n",
       "\n",
       "       [[2176.8596  , 2282.9858  , 2013.6686  , ...,  316.9225  ,\n",
       "          141.34967 ,   81.87417 ]],\n",
       "\n",
       "       [[2193.8914  , 2300.848   , 2029.4236  , ...,  319.40256 ,\n",
       "          142.45573 ,   82.51471 ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[2257.8867  , 2367.9636  , 2088.6218  , ...,  328.72125 ,\n",
       "          146.61174 ,   84.921524]],\n",
       "\n",
       "       [[2272.971   , 2383.7832  , 2102.5752  , ...,  330.91772 ,\n",
       "          147.59134 ,   85.48883 ]],\n",
       "\n",
       "       [[2290.4912  , 2402.1575  , 2118.782   , ...,  333.4689  ,\n",
       "          148.72913 ,   86.14774 ]]], dtype=float32)>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_train_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "#2002-2005\n",
    "\n",
    "full_val_inputs = next(iter(val_ds))[0] #pairs 2002,2003 and 2003,2004\n",
    "full_val_labels = next(iter(val_ds))[1] #2004 and 2005\n",
    "full_val_predictions = full_model(val_inputs) #2004 and 2005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_test_inputs = next(iter(test_ds))[0] #pairs from 2006-2010\n",
    "full_test_labels = next(iter(test_ds))[1] #2008-2011\n",
    "full_test_predictions = full_model(test_inputs) #2008-2011"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10101 f0.4</th>\n",
       "      <th>10101 f10.14</th>\n",
       "      <th>10101 f15.19</th>\n",
       "      <th>10101 f20.24</th>\n",
       "      <th>10101 f25.29</th>\n",
       "      <th>10101 f30.34</th>\n",
       "      <th>10101 f35.39</th>\n",
       "      <th>10101 f40.44</th>\n",
       "      <th>10101 f45.49</th>\n",
       "      <th>10101 f5.9</th>\n",
       "      <th>...</th>\n",
       "      <th>80109 m45.49</th>\n",
       "      <th>80109 m5.9</th>\n",
       "      <th>80109 m50.54</th>\n",
       "      <th>80109 m55.59</th>\n",
       "      <th>80109 m60.64</th>\n",
       "      <th>80109 m65.69</th>\n",
       "      <th>80109 m70.74</th>\n",
       "      <th>80109 m75.79</th>\n",
       "      <th>80109 m80.84</th>\n",
       "      <th>80109 m85.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2400</td>\n",
       "      <td>2347</td>\n",
       "      <td>2277</td>\n",
       "      <td>1820</td>\n",
       "      <td>2152</td>\n",
       "      <td>2315</td>\n",
       "      <td>2185</td>\n",
       "      <td>2146</td>\n",
       "      <td>1859</td>\n",
       "      <td>2495</td>\n",
       "      <td>...</td>\n",
       "      <td>1219</td>\n",
       "      <td>1002</td>\n",
       "      <td>1226</td>\n",
       "      <td>920</td>\n",
       "      <td>784</td>\n",
       "      <td>606</td>\n",
       "      <td>328</td>\n",
       "      <td>180</td>\n",
       "      <td>98</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2392</td>\n",
       "      <td>2344</td>\n",
       "      <td>2194</td>\n",
       "      <td>1830</td>\n",
       "      <td>2067</td>\n",
       "      <td>2339</td>\n",
       "      <td>2204</td>\n",
       "      <td>2139</td>\n",
       "      <td>1954</td>\n",
       "      <td>2480</td>\n",
       "      <td>...</td>\n",
       "      <td>1257</td>\n",
       "      <td>1009</td>\n",
       "      <td>1209</td>\n",
       "      <td>931</td>\n",
       "      <td>794</td>\n",
       "      <td>617</td>\n",
       "      <td>376</td>\n",
       "      <td>200</td>\n",
       "      <td>104</td>\n",
       "      <td>55</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2382</td>\n",
       "      <td>2354</td>\n",
       "      <td>2126</td>\n",
       "      <td>1813</td>\n",
       "      <td>1977</td>\n",
       "      <td>2329</td>\n",
       "      <td>2225</td>\n",
       "      <td>2139</td>\n",
       "      <td>2037</td>\n",
       "      <td>2451</td>\n",
       "      <td>...</td>\n",
       "      <td>1285</td>\n",
       "      <td>993</td>\n",
       "      <td>1154</td>\n",
       "      <td>963</td>\n",
       "      <td>792</td>\n",
       "      <td>634</td>\n",
       "      <td>413</td>\n",
       "      <td>209</td>\n",
       "      <td>105</td>\n",
       "      <td>59</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2357</td>\n",
       "      <td>2351</td>\n",
       "      <td>2084</td>\n",
       "      <td>1777</td>\n",
       "      <td>1913</td>\n",
       "      <td>2316</td>\n",
       "      <td>2232</td>\n",
       "      <td>2158</td>\n",
       "      <td>2079</td>\n",
       "      <td>2418</td>\n",
       "      <td>...</td>\n",
       "      <td>1284</td>\n",
       "      <td>988</td>\n",
       "      <td>1123</td>\n",
       "      <td>998</td>\n",
       "      <td>778</td>\n",
       "      <td>649</td>\n",
       "      <td>459</td>\n",
       "      <td>212</td>\n",
       "      <td>112</td>\n",
       "      <td>65</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2318</td>\n",
       "      <td>2357</td>\n",
       "      <td>2055</td>\n",
       "      <td>1734</td>\n",
       "      <td>1869</td>\n",
       "      <td>2272</td>\n",
       "      <td>2256</td>\n",
       "      <td>2165</td>\n",
       "      <td>2111</td>\n",
       "      <td>2403</td>\n",
       "      <td>...</td>\n",
       "      <td>1277</td>\n",
       "      <td>998</td>\n",
       "      <td>1105</td>\n",
       "      <td>1020</td>\n",
       "      <td>819</td>\n",
       "      <td>648</td>\n",
       "      <td>486</td>\n",
       "      <td>247</td>\n",
       "      <td>116</td>\n",
       "      <td>69</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>2280</td>\n",
       "      <td>2360</td>\n",
       "      <td>2060</td>\n",
       "      <td>1656</td>\n",
       "      <td>1869</td>\n",
       "      <td>2218</td>\n",
       "      <td>2290</td>\n",
       "      <td>2186</td>\n",
       "      <td>2150</td>\n",
       "      <td>2400</td>\n",
       "      <td>...</td>\n",
       "      <td>1244</td>\n",
       "      <td>1015</td>\n",
       "      <td>1112</td>\n",
       "      <td>1051</td>\n",
       "      <td>833</td>\n",
       "      <td>672</td>\n",
       "      <td>513</td>\n",
       "      <td>277</td>\n",
       "      <td>124</td>\n",
       "      <td>74</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2242</td>\n",
       "      <td>2327</td>\n",
       "      <td>2054</td>\n",
       "      <td>1555</td>\n",
       "      <td>1905</td>\n",
       "      <td>2104</td>\n",
       "      <td>2340</td>\n",
       "      <td>2208</td>\n",
       "      <td>2153</td>\n",
       "      <td>2393</td>\n",
       "      <td>...</td>\n",
       "      <td>1188</td>\n",
       "      <td>985</td>\n",
       "      <td>1117</td>\n",
       "      <td>1086</td>\n",
       "      <td>835</td>\n",
       "      <td>676</td>\n",
       "      <td>527</td>\n",
       "      <td>316</td>\n",
       "      <td>123</td>\n",
       "      <td>79</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2171</td>\n",
       "      <td>2338</td>\n",
       "      <td>2051</td>\n",
       "      <td>1480</td>\n",
       "      <td>1912</td>\n",
       "      <td>2054</td>\n",
       "      <td>2403</td>\n",
       "      <td>2171</td>\n",
       "      <td>2153</td>\n",
       "      <td>2400</td>\n",
       "      <td>...</td>\n",
       "      <td>1157</td>\n",
       "      <td>975</td>\n",
       "      <td>1127</td>\n",
       "      <td>1083</td>\n",
       "      <td>847</td>\n",
       "      <td>690</td>\n",
       "      <td>572</td>\n",
       "      <td>358</td>\n",
       "      <td>142</td>\n",
       "      <td>88</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2132</td>\n",
       "      <td>2348</td>\n",
       "      <td>2039</td>\n",
       "      <td>1450</td>\n",
       "      <td>1892</td>\n",
       "      <td>2005</td>\n",
       "      <td>2395</td>\n",
       "      <td>2217</td>\n",
       "      <td>2195</td>\n",
       "      <td>2400</td>\n",
       "      <td>...</td>\n",
       "      <td>1149</td>\n",
       "      <td>975</td>\n",
       "      <td>1123</td>\n",
       "      <td>1051</td>\n",
       "      <td>851</td>\n",
       "      <td>694</td>\n",
       "      <td>598</td>\n",
       "      <td>389</td>\n",
       "      <td>163</td>\n",
       "      <td>95</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>2107</td>\n",
       "      <td>2335</td>\n",
       "      <td>2029</td>\n",
       "      <td>1452</td>\n",
       "      <td>1848</td>\n",
       "      <td>1991</td>\n",
       "      <td>2363</td>\n",
       "      <td>2251</td>\n",
       "      <td>2201</td>\n",
       "      <td>2393</td>\n",
       "      <td>...</td>\n",
       "      <td>1122</td>\n",
       "      <td>967</td>\n",
       "      <td>1163</td>\n",
       "      <td>997</td>\n",
       "      <td>892</td>\n",
       "      <td>717</td>\n",
       "      <td>577</td>\n",
       "      <td>439</td>\n",
       "      <td>190</td>\n",
       "      <td>103</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>2077</td>\n",
       "      <td>2259</td>\n",
       "      <td>2051</td>\n",
       "      <td>1467</td>\n",
       "      <td>1777</td>\n",
       "      <td>1997</td>\n",
       "      <td>2263</td>\n",
       "      <td>2282</td>\n",
       "      <td>2203</td>\n",
       "      <td>2302</td>\n",
       "      <td>...</td>\n",
       "      <td>1100</td>\n",
       "      <td>945</td>\n",
       "      <td>1200</td>\n",
       "      <td>996</td>\n",
       "      <td>898</td>\n",
       "      <td>676</td>\n",
       "      <td>596</td>\n",
       "      <td>457</td>\n",
       "      <td>221</td>\n",
       "      <td>119</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11 rows × 11700 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    10101 f0.4  10101 f10.14  10101 f15.19  10101 f20.24  10101 f25.29  \\\n",
       "0         2400          2347          2277          1820          2152   \n",
       "1         2392          2344          2194          1830          2067   \n",
       "2         2382          2354          2126          1813          1977   \n",
       "3         2357          2351          2084          1777          1913   \n",
       "4         2318          2357          2055          1734          1869   \n",
       "5         2280          2360          2060          1656          1869   \n",
       "6         2242          2327          2054          1555          1905   \n",
       "7         2171          2338          2051          1480          1912   \n",
       "8         2132          2348          2039          1450          1892   \n",
       "9         2107          2335          2029          1452          1848   \n",
       "10        2077          2259          2051          1467          1777   \n",
       "\n",
       "    10101 f30.34  10101 f35.39  10101 f40.44  10101 f45.49  10101 f5.9  ...  \\\n",
       "0           2315          2185          2146          1859        2495  ...   \n",
       "1           2339          2204          2139          1954        2480  ...   \n",
       "2           2329          2225          2139          2037        2451  ...   \n",
       "3           2316          2232          2158          2079        2418  ...   \n",
       "4           2272          2256          2165          2111        2403  ...   \n",
       "5           2218          2290          2186          2150        2400  ...   \n",
       "6           2104          2340          2208          2153        2393  ...   \n",
       "7           2054          2403          2171          2153        2400  ...   \n",
       "8           2005          2395          2217          2195        2400  ...   \n",
       "9           1991          2363          2251          2201        2393  ...   \n",
       "10          1997          2263          2282          2203        2302  ...   \n",
       "\n",
       "    80109 m45.49  80109 m5.9  80109 m50.54  80109 m55.59  80109 m60.64  \\\n",
       "0           1219        1002          1226           920           784   \n",
       "1           1257        1009          1209           931           794   \n",
       "2           1285         993          1154           963           792   \n",
       "3           1284         988          1123           998           778   \n",
       "4           1277         998          1105          1020           819   \n",
       "5           1244        1015          1112          1051           833   \n",
       "6           1188         985          1117          1086           835   \n",
       "7           1157         975          1127          1083           847   \n",
       "8           1149         975          1123          1051           851   \n",
       "9           1122         967          1163           997           892   \n",
       "10          1100         945          1200           996           898   \n",
       "\n",
       "    80109 m65.69  80109 m70.74  80109 m75.79  80109 m80.84  80109 m85.  \n",
       "0            606           328           180            98          50  \n",
       "1            617           376           200           104          55  \n",
       "2            634           413           209           105          59  \n",
       "3            649           459           212           112          65  \n",
       "4            648           486           247           116          69  \n",
       "5            672           513           277           124          74  \n",
       "6            676           527           316           123          79  \n",
       "7            690           572           358           142          88  \n",
       "8            694           598           389           163          95  \n",
       "9            717           577           439           190         103  \n",
       "10           676           596           457           221         119  \n",
       "\n",
       "[11 rows x 11700 columns]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(9, 2, 11700), dtype=float32, numpy=\n",
       "array([[[2400., 2347., 2277., ...,  180.,   98.,   50.],\n",
       "        [2392., 2344., 2194., ...,  200.,  104.,   55.]],\n",
       "\n",
       "       [[2392., 2344., 2194., ...,  200.,  104.,   55.],\n",
       "        [2382., 2354., 2126., ...,  209.,  105.,   59.]],\n",
       "\n",
       "       [[2382., 2354., 2126., ...,  209.,  105.,   59.],\n",
       "        [2357., 2351., 2084., ...,  212.,  112.,   65.]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[2242., 2327., 2054., ...,  316.,  123.,   79.],\n",
       "        [2171., 2338., 2051., ...,  358.,  142.,   88.]],\n",
       "\n",
       "       [[2171., 2338., 2051., ...,  358.,  142.,   88.],\n",
       "        [2132., 2348., 2039., ...,  389.,  163.,   95.]],\n",
       "\n",
       "       [[2132., 2348., 2039., ...,  389.,  163.,   95.],\n",
       "        [2107., 2335., 2029., ...,  439.,  190.,  103.]]], dtype=float32)>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_train_inputs #pairs from 1991-2000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2, 11700), dtype=float32, numpy=\n",
       "array([[[2107., 2335., 2029., ...,  439.,  190.,  103.],\n",
       "        [2077., 2259., 2051., ...,  457.,  221.,  119.]],\n",
       "\n",
       "       [[2077., 2259., 2051., ...,  457.,  221.,  119.],\n",
       "        [2068., 2272., 2072., ...,  452.,  253.,  128.]]], dtype=float32)>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2000-2001 input for predicting 2002\n",
    "input_2002 = tf.stack([full_train_labels[7,0,:], full_train_labels[8,0,:]],0)\n",
    "#2001-2002 input for predicting 2003\n",
    "input_2003 = tf.stack([full_train_labels[8,0,:], full_val_inputs[0,0,:]],0)\n",
    "#2000-2001 and 2001-2002 inputs as tensor\n",
    "input_2002_2003 = tf.stack([input_2002,input_2003],0)\n",
    "\n",
    "input_2002_2003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10101 f0.4</th>\n",
       "      <th>10101 f10.14</th>\n",
       "      <th>10101 f15.19</th>\n",
       "      <th>10101 f20.24</th>\n",
       "      <th>10101 f25.29</th>\n",
       "      <th>10101 f30.34</th>\n",
       "      <th>10101 f35.39</th>\n",
       "      <th>10101 f40.44</th>\n",
       "      <th>10101 f45.49</th>\n",
       "      <th>10101 f5.9</th>\n",
       "      <th>...</th>\n",
       "      <th>80109 m45.49</th>\n",
       "      <th>80109 m5.9</th>\n",
       "      <th>80109 m50.54</th>\n",
       "      <th>80109 m55.59</th>\n",
       "      <th>80109 m60.64</th>\n",
       "      <th>80109 m65.69</th>\n",
       "      <th>80109 m70.74</th>\n",
       "      <th>80109 m75.79</th>\n",
       "      <th>80109 m80.84</th>\n",
       "      <th>80109 m85.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>2068</td>\n",
       "      <td>2272</td>\n",
       "      <td>2072</td>\n",
       "      <td>1450</td>\n",
       "      <td>1682</td>\n",
       "      <td>2080</td>\n",
       "      <td>2189</td>\n",
       "      <td>2323</td>\n",
       "      <td>2239</td>\n",
       "      <td>2319</td>\n",
       "      <td>...</td>\n",
       "      <td>1114</td>\n",
       "      <td>984</td>\n",
       "      <td>1132</td>\n",
       "      <td>986</td>\n",
       "      <td>960</td>\n",
       "      <td>692</td>\n",
       "      <td>590</td>\n",
       "      <td>452</td>\n",
       "      <td>253</td>\n",
       "      <td>128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>2033</td>\n",
       "      <td>2317</td>\n",
       "      <td>2078</td>\n",
       "      <td>1466</td>\n",
       "      <td>1622</td>\n",
       "      <td>2116</td>\n",
       "      <td>2172</td>\n",
       "      <td>2394</td>\n",
       "      <td>2217</td>\n",
       "      <td>2299</td>\n",
       "      <td>...</td>\n",
       "      <td>1115</td>\n",
       "      <td>990</td>\n",
       "      <td>1078</td>\n",
       "      <td>1003</td>\n",
       "      <td>920</td>\n",
       "      <td>713</td>\n",
       "      <td>601</td>\n",
       "      <td>454</td>\n",
       "      <td>285</td>\n",
       "      <td>127</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>2035</td>\n",
       "      <td>2372</td>\n",
       "      <td>2058</td>\n",
       "      <td>1481</td>\n",
       "      <td>1585</td>\n",
       "      <td>2089</td>\n",
       "      <td>2152</td>\n",
       "      <td>2419</td>\n",
       "      <td>2220</td>\n",
       "      <td>2256</td>\n",
       "      <td>...</td>\n",
       "      <td>1136</td>\n",
       "      <td>978</td>\n",
       "      <td>1065</td>\n",
       "      <td>993</td>\n",
       "      <td>887</td>\n",
       "      <td>728</td>\n",
       "      <td>584</td>\n",
       "      <td>476</td>\n",
       "      <td>298</td>\n",
       "      <td>129</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>2038</td>\n",
       "      <td>2406</td>\n",
       "      <td>2052</td>\n",
       "      <td>1520</td>\n",
       "      <td>1571</td>\n",
       "      <td>2025</td>\n",
       "      <td>2171</td>\n",
       "      <td>2403</td>\n",
       "      <td>2240</td>\n",
       "      <td>2216</td>\n",
       "      <td>...</td>\n",
       "      <td>1192</td>\n",
       "      <td>979</td>\n",
       "      <td>1027</td>\n",
       "      <td>1030</td>\n",
       "      <td>828</td>\n",
       "      <td>754</td>\n",
       "      <td>599</td>\n",
       "      <td>471</td>\n",
       "      <td>313</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>4 rows × 11700 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    10101 f0.4  10101 f10.14  10101 f15.19  10101 f20.24  10101 f25.29  \\\n",
       "11        2068          2272          2072          1450          1682   \n",
       "12        2033          2317          2078          1466          1622   \n",
       "13        2035          2372          2058          1481          1585   \n",
       "14        2038          2406          2052          1520          1571   \n",
       "\n",
       "    10101 f30.34  10101 f35.39  10101 f40.44  10101 f45.49  10101 f5.9  ...  \\\n",
       "11          2080          2189          2323          2239        2319  ...   \n",
       "12          2116          2172          2394          2217        2299  ...   \n",
       "13          2089          2152          2419          2220        2256  ...   \n",
       "14          2025          2171          2403          2240        2216  ...   \n",
       "\n",
       "    80109 m45.49  80109 m5.9  80109 m50.54  80109 m55.59  80109 m60.64  \\\n",
       "11          1114         984          1132           986           960   \n",
       "12          1115         990          1078          1003           920   \n",
       "13          1136         978          1065           993           887   \n",
       "14          1192         979          1027          1030           828   \n",
       "\n",
       "    80109 m65.69  80109 m70.74  80109 m75.79  80109 m80.84  80109 m85.  \n",
       "11           692           590           452           253         128  \n",
       "12           713           601           454           285         127  \n",
       "13           728           584           476           298         129  \n",
       "14           754           599           471           313         156  \n",
       "\n",
       "[4 rows x 11700 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2, 11700), dtype=float32, numpy=\n",
       "array([[[2068., 2272., 2072., ...,  452.,  253.,  128.],\n",
       "        [2033., 2317., 2078., ...,  454.,  285.,  127.]],\n",
       "\n",
       "       [[2033., 2317., 2078., ...,  454.,  285.,  127.],\n",
       "        [2035., 2372., 2058., ...,  476.,  298.,  129.]]], dtype=float32)>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_val_inputs #pairs from 2002-2004"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(2, 2, 11700), dtype=float32, numpy=\n",
       "array([[[2035., 2372., 2058., ...,  476.,  298.,  129.],\n",
       "        [2038., 2406., 2052., ...,  471.,  313.,  156.]],\n",
       "\n",
       "       [[2038., 2406., 2052., ...,  471.,  313.,  156.],\n",
       "        [2005., 2428., 2034., ...,  499.,  325.,  195.]]], dtype=float32)>"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#2004-2005 input for predicting 2006\n",
    "input_2006 = tf.stack([full_val_labels[0,0,:],full_val_labels[1,0,:]],0)\n",
    "#2005-2006 input for predicting 2007\n",
    "input_2007 = tf.stack([full_val_labels[1,0,:], full_test_inputs[0,0,:]],0)\n",
    "#2004-2005 and 2005-2006 inputs as tensor\n",
    "input_2006_2007 = tf.stack([input_2006,input_2007],0)\n",
    "\n",
    "input_2006_2007"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(4, 2, 11700), dtype=float32, numpy=\n",
       "array([[[2005., 2428., 2034., ...,  499.,  325.,  195.],\n",
       "        [2011., 2375., 2112., ...,  480.,  356.,  214.]],\n",
       "\n",
       "       [[2011., 2375., 2112., ...,  480.,  356.,  214.],\n",
       "        [2044., 2343., 2145., ...,  484.,  361.,  234.]],\n",
       "\n",
       "       [[2044., 2343., 2145., ...,  484.,  361.,  234.],\n",
       "        [2109., 2330., 2182., ...,  481.,  363.,  250.]],\n",
       "\n",
       "       [[2109., 2330., 2182., ...,  481.,  363.,  250.],\n",
       "        [2170., 2336., 2230., ...,  492.,  360.,  264.]]], dtype=float32)>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_test_inputs #pairs from 2006-2010"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>10101 f0.4</th>\n",
       "      <th>10101 f10.14</th>\n",
       "      <th>10101 f15.19</th>\n",
       "      <th>10101 f20.24</th>\n",
       "      <th>10101 f25.29</th>\n",
       "      <th>10101 f30.34</th>\n",
       "      <th>10101 f35.39</th>\n",
       "      <th>10101 f40.44</th>\n",
       "      <th>10101 f45.49</th>\n",
       "      <th>10101 f5.9</th>\n",
       "      <th>...</th>\n",
       "      <th>80109 m45.49</th>\n",
       "      <th>80109 m5.9</th>\n",
       "      <th>80109 m50.54</th>\n",
       "      <th>80109 m55.59</th>\n",
       "      <th>80109 m60.64</th>\n",
       "      <th>80109 m65.69</th>\n",
       "      <th>80109 m70.74</th>\n",
       "      <th>80109 m75.79</th>\n",
       "      <th>80109 m80.84</th>\n",
       "      <th>80109 m85.</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>2005</td>\n",
       "      <td>2428</td>\n",
       "      <td>2034</td>\n",
       "      <td>1570</td>\n",
       "      <td>1566</td>\n",
       "      <td>1936</td>\n",
       "      <td>2246</td>\n",
       "      <td>2337</td>\n",
       "      <td>2292</td>\n",
       "      <td>2232</td>\n",
       "      <td>...</td>\n",
       "      <td>1248</td>\n",
       "      <td>992</td>\n",
       "      <td>1030</td>\n",
       "      <td>1031</td>\n",
       "      <td>795</td>\n",
       "      <td>771</td>\n",
       "      <td>589</td>\n",
       "      <td>499</td>\n",
       "      <td>325</td>\n",
       "      <td>195</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>2011</td>\n",
       "      <td>2375</td>\n",
       "      <td>2112</td>\n",
       "      <td>1542</td>\n",
       "      <td>1583</td>\n",
       "      <td>1875</td>\n",
       "      <td>2288</td>\n",
       "      <td>2247</td>\n",
       "      <td>2352</td>\n",
       "      <td>2210</td>\n",
       "      <td>...</td>\n",
       "      <td>1279</td>\n",
       "      <td>962</td>\n",
       "      <td>1042</td>\n",
       "      <td>988</td>\n",
       "      <td>798</td>\n",
       "      <td>818</td>\n",
       "      <td>599</td>\n",
       "      <td>480</td>\n",
       "      <td>356</td>\n",
       "      <td>214</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>2044</td>\n",
       "      <td>2343</td>\n",
       "      <td>2145</td>\n",
       "      <td>1594</td>\n",
       "      <td>1640</td>\n",
       "      <td>1840</td>\n",
       "      <td>2314</td>\n",
       "      <td>2246</td>\n",
       "      <td>2418</td>\n",
       "      <td>2210</td>\n",
       "      <td>...</td>\n",
       "      <td>1283</td>\n",
       "      <td>954</td>\n",
       "      <td>1068</td>\n",
       "      <td>971</td>\n",
       "      <td>826</td>\n",
       "      <td>783</td>\n",
       "      <td>627</td>\n",
       "      <td>484</td>\n",
       "      <td>361</td>\n",
       "      <td>234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>2109</td>\n",
       "      <td>2330</td>\n",
       "      <td>2182</td>\n",
       "      <td>1670</td>\n",
       "      <td>1662</td>\n",
       "      <td>1857</td>\n",
       "      <td>2312</td>\n",
       "      <td>2256</td>\n",
       "      <td>2463</td>\n",
       "      <td>2213</td>\n",
       "      <td>...</td>\n",
       "      <td>1296</td>\n",
       "      <td>968</td>\n",
       "      <td>1082</td>\n",
       "      <td>975</td>\n",
       "      <td>852</td>\n",
       "      <td>768</td>\n",
       "      <td>643</td>\n",
       "      <td>481</td>\n",
       "      <td>363</td>\n",
       "      <td>250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>2170</td>\n",
       "      <td>2336</td>\n",
       "      <td>2230</td>\n",
       "      <td>1722</td>\n",
       "      <td>1691</td>\n",
       "      <td>1883</td>\n",
       "      <td>2274</td>\n",
       "      <td>2328</td>\n",
       "      <td>2474</td>\n",
       "      <td>2217</td>\n",
       "      <td>...</td>\n",
       "      <td>1262</td>\n",
       "      <td>983</td>\n",
       "      <td>1129</td>\n",
       "      <td>962</td>\n",
       "      <td>893</td>\n",
       "      <td>741</td>\n",
       "      <td>674</td>\n",
       "      <td>492</td>\n",
       "      <td>360</td>\n",
       "      <td>264</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>2225</td>\n",
       "      <td>2381</td>\n",
       "      <td>2178</td>\n",
       "      <td>1698</td>\n",
       "      <td>1681</td>\n",
       "      <td>1864</td>\n",
       "      <td>2252</td>\n",
       "      <td>2426</td>\n",
       "      <td>2430</td>\n",
       "      <td>2217</td>\n",
       "      <td>...</td>\n",
       "      <td>1201</td>\n",
       "      <td>974</td>\n",
       "      <td>1194</td>\n",
       "      <td>927</td>\n",
       "      <td>867</td>\n",
       "      <td>756</td>\n",
       "      <td>716</td>\n",
       "      <td>505</td>\n",
       "      <td>414</td>\n",
       "      <td>282</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 11700 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "    10101 f0.4  10101 f10.14  10101 f15.19  10101 f20.24  10101 f25.29  \\\n",
       "15        2005          2428          2034          1570          1566   \n",
       "16        2011          2375          2112          1542          1583   \n",
       "17        2044          2343          2145          1594          1640   \n",
       "18        2109          2330          2182          1670          1662   \n",
       "19        2170          2336          2230          1722          1691   \n",
       "20        2225          2381          2178          1698          1681   \n",
       "\n",
       "    10101 f30.34  10101 f35.39  10101 f40.44  10101 f45.49  10101 f5.9  ...  \\\n",
       "15          1936          2246          2337          2292        2232  ...   \n",
       "16          1875          2288          2247          2352        2210  ...   \n",
       "17          1840          2314          2246          2418        2210  ...   \n",
       "18          1857          2312          2256          2463        2213  ...   \n",
       "19          1883          2274          2328          2474        2217  ...   \n",
       "20          1864          2252          2426          2430        2217  ...   \n",
       "\n",
       "    80109 m45.49  80109 m5.9  80109 m50.54  80109 m55.59  80109 m60.64  \\\n",
       "15          1248         992          1030          1031           795   \n",
       "16          1279         962          1042           988           798   \n",
       "17          1283         954          1068           971           826   \n",
       "18          1296         968          1082           975           852   \n",
       "19          1262         983          1129           962           893   \n",
       "20          1201         974          1194           927           867   \n",
       "\n",
       "    80109 m65.69  80109 m70.74  80109 m75.79  80109 m80.84  80109 m85.  \n",
       "15           771           589           499           325         195  \n",
       "16           818           599           480           356         214  \n",
       "17           783           627           484           361         234  \n",
       "18           768           643           481           363         250  \n",
       "19           741           674           492           360         264  \n",
       "20           756           716           505           414         282  \n",
       "\n",
       "[6 rows x 11700 columns]"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Update the training data for next projection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1, 11700), dtype=float32, numpy=\n",
       "array([[[2312.7427 , 2425.4941 , 2139.3657 , ...,  336.70908,\n",
       "          150.1742 ,   86.98461]]], dtype=float32)>"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_result = []\n",
    "result_2002 = full_model(tf.concat([tf.stack([input_2002],0)],0))\n",
    "result_2002"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(1, 1, 11700), dtype=float32, numpy=\n",
       "array([[[2330.4192  , 2444.0325  , 2155.7173  , ...,  339.28305 ,\n",
       "          151.32214 ,   87.649414]]], dtype=float32)>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_2003 =  tf.stack([full_train_labels[8,0,:], result_2002[0,0,:]],0)\n",
    "result_2003 = full_model(tf.concat([tf.stack([input_2003],0)],0))\n",
    "result_2003"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([[2312.7427 , 2425.4941 , 2139.3657 , ...,  336.70908,  150.1742 ,\n",
       "           86.98461]], dtype=float32)]"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_2002 = result_2002.numpy()\n",
    "final_result.append(result_2002[0])\n",
    "final_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all-in-one input\n",
    "\n",
    "all_input = tf.concat([full_train_inputs,input_2002_2003,full_val_inputs,input_2006_2007,full_test_inputs],0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(19, 1, 11700), dtype=float32, numpy=\n",
       "array([[[2159.7637  , 2265.0562  , 1997.8541  , ...,  314.43307 ,\n",
       "          140.23941 ,   81.2312  ]],\n",
       "\n",
       "       [[2176.8542  , 2282.9802  , 2013.6637  , ...,  316.92172 ,\n",
       "          141.34932 ,   81.87397 ]],\n",
       "\n",
       "       [[2193.8901  , 2300.8464  , 2029.4224  , ...,  319.40237 ,\n",
       "          142.45566 ,   82.51466 ]],\n",
       "\n",
       "       ...,\n",
       "\n",
       "       [[2526.2898  , 2649.453   , 2336.905   , ...,  367.80466 ,\n",
       "          164.04237 ,   95.01593 ]],\n",
       "\n",
       "       [[2576.7805  , 2702.4053  , 2383.6108  , ...,  375.15686 ,\n",
       "          167.32135 ,   96.91483 ]],\n",
       "\n",
       "       [[2615.2136  , 2742.7124  , 2419.163   , ...,  380.7533  ,\n",
       "          169.81728 ,   98.360275]]], dtype=float32)>"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#predictions for years 1993-2011\n",
    "\n",
    "result = full_model(all_input) #1993-2011\n",
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['10101 f0.4', '10101 f10.14', '10101 f15.19', '10101 f20.24',\n",
       "       '10101 f25.29', '10101 f30.34', '10101 f35.39', '10101 f40.44',\n",
       "       '10101 f45.49', '10101 f5.9',\n",
       "       ...\n",
       "       '80109 m45.49', '80109 m5.9', '80109 m50.54', '80109 m55.59',\n",
       "       '80109 m60.64', '80109 m65.69', '80109 m70.74', '80109 m75.79',\n",
       "       '80109 m80.84', '80109 m85.'],\n",
       "      dtype='object', length=11700)"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "Code = []\n",
    "Sex = []\n",
    "Age = []\n",
    "for sets in test_df.columns:\n",
    "    code = sets.split()[0]\n",
    "    sex = sets.split()[1][0]\n",
    "    age = sets.split()[1][1:]\n",
    "    Code.append(code)\n",
    "    Sex.append(sex)\n",
    "    Age.append(age)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2312.7402  2425.4915  2139.3633  ...  336.7087   150.17403   86.98451]]\n",
      "[[2335.17    2449.0146  2160.1116  ...  339.9748   151.63066   87.82807]]\n",
      "[[2358.85     2473.8494   2182.0166   ...  343.42297   153.16849\n",
      "    88.718666]]\n",
      "[[2381.3325  2497.428   2202.8137  ...  346.69675  154.62856   89.56421]]\n",
      "[[2407.5566  2524.9307  2227.0723  ...  350.51538  156.3316    90.55048]]\n",
      "[[2436.2122  2554.9834  2253.5798  ...  354.68805  158.19257   91.62819]]\n",
      "[[2477.855    2598.6565   2292.1008   ...  360.75183   160.89693\n",
      "    93.194336]]\n",
      "[[2526.2898  2649.453   2336.905   ...  367.80466  164.04237   95.01593]]\n",
      "[[2576.7805  2702.4053  2383.6108  ...  375.15686  167.32135   96.91483]]\n",
      "[[2615.2136   2742.7124   2419.163    ...  380.7533    169.81728\n",
      "    98.360275]]\n"
     ]
    }
   ],
   "source": [
    "year2002_2011 = result[-10:]\n",
    "year2002_2011 = year2002_2011.numpy()\n",
    "final_result = []\n",
    "final_result.append(Code)\n",
    "final_result.append(Sex)\n",
    "final_result.append(Age)\n",
    "for year in year2002_2011:\n",
    "    print(year)\n",
    "    final_result.append(year[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Code</th>\n",
       "      <th>Sex</th>\n",
       "      <th>Age</th>\n",
       "      <th>2002</th>\n",
       "      <th>2003</th>\n",
       "      <th>2004</th>\n",
       "      <th>2005</th>\n",
       "      <th>2006</th>\n",
       "      <th>2007</th>\n",
       "      <th>2008</th>\n",
       "      <th>2009</th>\n",
       "      <th>2010</th>\n",
       "      <th>2011</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>10101</td>\n",
       "      <td>f</td>\n",
       "      <td>0.4</td>\n",
       "      <td>2312.740234</td>\n",
       "      <td>2335.169922</td>\n",
       "      <td>2358.850098</td>\n",
       "      <td>2381.33252</td>\n",
       "      <td>2407.556641</td>\n",
       "      <td>2436.212158</td>\n",
       "      <td>2477.85498</td>\n",
       "      <td>2526.289795</td>\n",
       "      <td>2576.780518</td>\n",
       "      <td>2615.213623</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>10101</td>\n",
       "      <td>f</td>\n",
       "      <td>10.14</td>\n",
       "      <td>2425.491455</td>\n",
       "      <td>2449.014648</td>\n",
       "      <td>2473.849365</td>\n",
       "      <td>2497.427979</td>\n",
       "      <td>2524.930664</td>\n",
       "      <td>2554.983398</td>\n",
       "      <td>2598.656494</td>\n",
       "      <td>2649.452881</td>\n",
       "      <td>2702.405273</td>\n",
       "      <td>2742.712402</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10101</td>\n",
       "      <td>f</td>\n",
       "      <td>15.19</td>\n",
       "      <td>2139.363281</td>\n",
       "      <td>2160.111572</td>\n",
       "      <td>2182.016602</td>\n",
       "      <td>2202.813721</td>\n",
       "      <td>2227.072266</td>\n",
       "      <td>2253.579834</td>\n",
       "      <td>2292.10083</td>\n",
       "      <td>2336.905029</td>\n",
       "      <td>2383.61084</td>\n",
       "      <td>2419.163086</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>10101</td>\n",
       "      <td>f</td>\n",
       "      <td>20.24</td>\n",
       "      <td>1656.695435</td>\n",
       "      <td>1672.762329</td>\n",
       "      <td>1689.724854</td>\n",
       "      <td>1705.82959</td>\n",
       "      <td>1724.614624</td>\n",
       "      <td>1745.141235</td>\n",
       "      <td>1774.970825</td>\n",
       "      <td>1809.665894</td>\n",
       "      <td>1845.833496</td>\n",
       "      <td>1873.364136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>10101</td>\n",
       "      <td>f</td>\n",
       "      <td>25.29</td>\n",
       "      <td>1955.963257</td>\n",
       "      <td>1974.932861</td>\n",
       "      <td>1994.959961</td>\n",
       "      <td>2013.974121</td>\n",
       "      <td>2036.152832</td>\n",
       "      <td>2060.387939</td>\n",
       "      <td>2095.606445</td>\n",
       "      <td>2136.56958</td>\n",
       "      <td>2179.27124</td>\n",
       "      <td>2211.775635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11695</th>\n",
       "      <td>80109</td>\n",
       "      <td>m</td>\n",
       "      <td>65.69</td>\n",
       "      <td>699.020447</td>\n",
       "      <td>705.799744</td>\n",
       "      <td>712.95697</td>\n",
       "      <td>719.752258</td>\n",
       "      <td>727.678406</td>\n",
       "      <td>736.339478</td>\n",
       "      <td>748.925842</td>\n",
       "      <td>763.565125</td>\n",
       "      <td>778.825745</td>\n",
       "      <td>790.442017</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11696</th>\n",
       "      <td>80109</td>\n",
       "      <td>m</td>\n",
       "      <td>70.74</td>\n",
       "      <td>548.074097</td>\n",
       "      <td>553.389771</td>\n",
       "      <td>559.001831</td>\n",
       "      <td>564.330017</td>\n",
       "      <td>570.544922</td>\n",
       "      <td>577.336121</td>\n",
       "      <td>587.205139</td>\n",
       "      <td>598.683838</td>\n",
       "      <td>610.649719</td>\n",
       "      <td>619.758118</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11697</th>\n",
       "      <td>80109</td>\n",
       "      <td>m</td>\n",
       "      <td>75.79</td>\n",
       "      <td>336.70871</td>\n",
       "      <td>339.974792</td>\n",
       "      <td>343.422974</td>\n",
       "      <td>346.696747</td>\n",
       "      <td>350.515381</td>\n",
       "      <td>354.688049</td>\n",
       "      <td>360.751831</td>\n",
       "      <td>367.804657</td>\n",
       "      <td>375.15686</td>\n",
       "      <td>380.753296</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11698</th>\n",
       "      <td>80109</td>\n",
       "      <td>m</td>\n",
       "      <td>80.84</td>\n",
       "      <td>150.174026</td>\n",
       "      <td>151.630661</td>\n",
       "      <td>153.168488</td>\n",
       "      <td>154.628555</td>\n",
       "      <td>156.331604</td>\n",
       "      <td>158.192566</td>\n",
       "      <td>160.896927</td>\n",
       "      <td>164.042374</td>\n",
       "      <td>167.32135</td>\n",
       "      <td>169.817276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11699</th>\n",
       "      <td>80109</td>\n",
       "      <td>m</td>\n",
       "      <td>85.</td>\n",
       "      <td>86.984512</td>\n",
       "      <td>87.828072</td>\n",
       "      <td>88.718666</td>\n",
       "      <td>89.564209</td>\n",
       "      <td>90.550484</td>\n",
       "      <td>91.628189</td>\n",
       "      <td>93.194336</td>\n",
       "      <td>95.01593</td>\n",
       "      <td>96.914833</td>\n",
       "      <td>98.360275</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11700 rows × 13 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        Code Sex    Age         2002         2003         2004         2005  \\\n",
       "0      10101   f    0.4  2312.740234  2335.169922  2358.850098   2381.33252   \n",
       "1      10101   f  10.14  2425.491455  2449.014648  2473.849365  2497.427979   \n",
       "2      10101   f  15.19  2139.363281  2160.111572  2182.016602  2202.813721   \n",
       "3      10101   f  20.24  1656.695435  1672.762329  1689.724854   1705.82959   \n",
       "4      10101   f  25.29  1955.963257  1974.932861  1994.959961  2013.974121   \n",
       "...      ...  ..    ...          ...          ...          ...          ...   \n",
       "11695  80109   m  65.69   699.020447   705.799744    712.95697   719.752258   \n",
       "11696  80109   m  70.74   548.074097   553.389771   559.001831   564.330017   \n",
       "11697  80109   m  75.79    336.70871   339.974792   343.422974   346.696747   \n",
       "11698  80109   m  80.84   150.174026   151.630661   153.168488   154.628555   \n",
       "11699  80109   m    85.    86.984512    87.828072    88.718666    89.564209   \n",
       "\n",
       "              2006         2007         2008         2009         2010  \\\n",
       "0      2407.556641  2436.212158   2477.85498  2526.289795  2576.780518   \n",
       "1      2524.930664  2554.983398  2598.656494  2649.452881  2702.405273   \n",
       "2      2227.072266  2253.579834   2292.10083  2336.905029   2383.61084   \n",
       "3      1724.614624  1745.141235  1774.970825  1809.665894  1845.833496   \n",
       "4      2036.152832  2060.387939  2095.606445   2136.56958   2179.27124   \n",
       "...            ...          ...          ...          ...          ...   \n",
       "11695   727.678406   736.339478   748.925842   763.565125   778.825745   \n",
       "11696   570.544922   577.336121   587.205139   598.683838   610.649719   \n",
       "11697   350.515381   354.688049   360.751831   367.804657    375.15686   \n",
       "11698   156.331604   158.192566   160.896927   164.042374    167.32135   \n",
       "11699    90.550484    91.628189    93.194336     95.01593    96.914833   \n",
       "\n",
       "              2011  \n",
       "0      2615.213623  \n",
       "1      2742.712402  \n",
       "2      2419.163086  \n",
       "3      1873.364136  \n",
       "4      2211.775635  \n",
       "...            ...  \n",
       "11695   790.442017  \n",
       "11696   619.758118  \n",
       "11697   380.753296  \n",
       "11698   169.817276  \n",
       "11699    98.360275  \n",
       "\n",
       "[11700 rows x 13 columns]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "column_name = ['Code','Sex','Age', '2002','2003','2004','2005','2006','2007','2008','2009','2010','2011']\n",
    "final_df = pd.DataFrame(final_result).T\n",
    "final_df.columns = column_name\n",
    "final_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.9 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
